{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"kube/","text":"Community We are here to help Lorem ipsum","title":"Marx and Spenser"},{"location":"kube/#community","text":"","title":"Community"},{"location":"kube/#we-are-here-to-help","text":"Lorem ipsum","title":"We are here to help"},{"location":"kube/k8s-next-conclusion/","text":"Status Date APPROVED 29th of April 2021 Introduction This document compiles all the outcomes of the different design and PoC documents that have been created for our Kubernetes Next Deployment. Each section might contain one or multiple milestones. All sections of the same milestone should be considered as a new \"release\" of the Kubernetes Next strategy. Design Summary Helm as the primary deployment method We want to leverage Helm as our primary way of deployment. Helm will exactly define what resources are being created during the deployment. Resources in our deployment will ideally only be created or deleted with Helm. Update and Rollback tasks would be issued using Helm as well. The Helm chart will contain suitable default settings for a default deployment and also allow customers to customize as much of the deployment as possible for their needs. All the configuration will be determined by values.yaml - as per the Helm standard. DXCTL would still be an option for customers who don't want to use Helm. The deployment procedure that DXCTL will use will be directly and programmatically derived from our Helm chart, thus enabling the Helm chart to be our single point of truth regarding deployment structure. Milestone 1 Summary Providing a simple, fresh deployment via Helm. Limitations - no official support for other ambassadors, OS-routes,... to reduce test effort - dxctl as a deployment option will follow in a later milestone Runtime Controller with focus on operation/controller tasks Based on What is a Kube Operator we are not building an operator but a runtime controller. The wording needs to be precise here since the term operator implies certain functionalities which we are explicitly avoiding with a runtime controller. Instead of the focus on deploying and reconciling resources with a CRD, the runtime controller will focus on operational topics. Part of this is that it should not have to take care of application logic, that ideally should be handled by our applications themselves. The runtime controller should not act as a mitigation strategy for lack of application capabilities, but rather help with application-specific tasks and behaviors. This would include e.g. automatic rollout of configuration changes from ConfigMaps, verification of Kubernetes resource changes, overall health metrics, support during backup and restore procedures. Milestone 1 Summary - Create a new runtime controller with - DAM persistence RW/RO failover - Autorollout config changes via Pod-Recycle Configuration changes via Helm Enable (disable) services like DAM Service configurations (DAM file size limits, CORS,..) Scaling Settings Horizontal Pod Autoscaler SSL certificate management for Ambassador and services DX Admin credential management (Secrets) Limitations - Deployment cannot run old operator and new runtime controller at the same time - SSL certificate management will only cover shipping the current certificate Milestone 3 Summary - auto renewal of interservice SSL certificates ConfigMaps as configuration backbone Since CRDs are cluster-wide resources that require special care (e.g. versioning), we will go with ConfigMaps as our single way of passing configuration values. CRDs also cannot be updated using Helm, which already poses issues for our SoFy deployments. Validation of ConfigMaps could be achieved by using VerificationHooks that call the runtime controller for sign-off on changes. Milestone 1 Summary Changes in ConfigMaps will be rolled out automatically by the runtime controller since our Containers are consuming ConfigMap values only on startup. Limitations - No verification Hooks Containers need to be more self-sustaining Our applications, especially the stateful ones, need to be improved to behave more naturally in a Kubernetes / Containerized Environment. This is especially true for DX Core. Currently, the operator mainly defines how DX Core starts, behaves and updates. Some of this will have to go into the DX Core container and away from the newly planned runtime controller. Milestone 1 Summary DX-Core will become more self-aware. By definition of kube, containers have the naming scheme <pod-name>-<incrementing integer> like dx-core-0 to dx-core-(n-1) where n is the number of Pods. dx-core-0 will be the first core pod created and therefore: - run initialization of the profile - run db transfer for multi-pod environments All other pods will not start until: - init is done - db transfer has happened - This logic can apply in kube and docker-compose environments if we apply the same name schema to the pods/containers. After initialization, pods will be notified (by Helm or the Runtime Controller) of config changes via a checksum environment variable. When any pod starts it will compare this value to the config checksum value stored on the shared profile persistent volume. If the value has changed, then the pod will apply the configuration changes and afterwards update the value in the profile. Because of the order in which kube recycles pods in a stateful set when the spec changes, the pod making the changes should in practice always be dx-core-(n-1) . Limitations - providing a docker-compose file is a stretch goal Vanilla as possible, Custom as needed To keep the support for multiple Kubernetes platforms as easy as possible and the deployment maintainable for us, we want to use as few Kubernetes flavour-specific resources as possible. Furthermore we want to leverage as much as possible of the OOB functionality that Kubernetes provides for our applications regarding deployments, updates, scaling etc. This would also mean the we try to get away from seemingly different deployment behaviors on different flavours, like Openshift vs GKE. Milestone 1 Summary Dropping the support of OS routes for Kubernetes Next deployments. Still relying on shared volumes where provided, NFS-Containers as a workaround where needed. Limitations - tbd Enable logging using Kubernetes logs We will use the sidecar container concept to expose as many logs from DX and its applications as possible, so that customers can directly use Kubernetes standard tools to retrieve them. In the future we could provide a fully fledged logging stack, that uses this logging concept and provides customers with proper logfile management. Milestone 1 Not considered in this milestone. Establish a metrics/monitoring stack We will expose application-specific metrics in our containers that can easily be consumed by software like Prometheus and allows customers to establish proper monitoring. In the future we could also provide our own monitoring stack allowing customers to easily see the current status of their deployment. Milestone 1 Not considered in this milestone. Rolling update We will provide a rolling upgrade capability so customers can easily update their deployment. A Rolling update does not imply a deployment with zero downtime for all features. However zero downtime is possible for rendering. Milestone 1 Not considered in this milestone since there is nothing to update from. Milestone 2 Summary By design kube always updates Statefulsets by picking the pod with the highest number first. In all other scenarios kube picks a random pod. Pods are always upgraded one at a time. Once this pod is ready, the next pod will be upgraded. DAM is self-aware of its own updates. If a database change is needed, DAM will take care of it based of the release-to-release migration. 1. Kube recycles pod with new image - if no DB change is necessary proceed with step 4. 2. The new pod is initializing release-to-release migration. 3. The new pod is performing the release-to-release migration. 4. Once the migration is done it marks itself ready via ready-probe. 5. Kube updates the next pod with the new container image. 6. Kube updates the next pod until there are none on the old image version. Core image will be modified to support rolling updates. This will not change the core build itself but only the wrapping container. 1. Kube recycles pod with new image 2. Pod will create a new, cloned wp_profile directory on the shared volume 3. Pod will switch its own symlink from wp_profile to the cloned, versioned wp_profile 4. Pod will perform the actual upgrade and marks itself ready via ready-probe 5. Kube updates the next pod. 6. Pod checks if the new wp_profile directory exists and adjusts own symlink prior to startup 7. repeat steps 5 and 6 until there are no further pods on the old image The following flowcharts give more detail: Limitations - helm only upgrade - no rollback possible - stick with shared volume - writes for DAM will have a downtime - wp_profile changes from old core pods might be lost during upgrade Undeploy Milestone 1 Summary Use helm uninstall to remove the deployment. PVs will not be removed. Limitations - we do not clean up parts from the current Kubernetes strategy Operator/Runtime Controller technology stack As mentioned earlier within this document we are not building a fully fledged operator. Therefore we do not need to rely on an operator framework to manage CRDs etc. Based on the skillset within the DX development team we decided that the official kuberenetes client for Java is capable of everything we need and are therefore building a Java based runtime controller. Milestone 1 Summary Creating a first version of the runtime controller including proper build, lint and test CI/CD. We also need a proper development setup based on maven and make files.","title":"Conclusion"},{"location":"kube/k8s-next-conclusion/#introduction","text":"This document compiles all the outcomes of the different design and PoC documents that have been created for our Kubernetes Next Deployment. Each section might contain one or multiple milestones. All sections of the same milestone should be considered as a new \"release\" of the Kubernetes Next strategy.","title":"Introduction"},{"location":"kube/k8s-next-conclusion/#design-summary","text":"","title":"Design Summary"},{"location":"kube/k8s-next-conclusion/#helm-as-the-primary-deployment-method","text":"We want to leverage Helm as our primary way of deployment. Helm will exactly define what resources are being created during the deployment. Resources in our deployment will ideally only be created or deleted with Helm. Update and Rollback tasks would be issued using Helm as well. The Helm chart will contain suitable default settings for a default deployment and also allow customers to customize as much of the deployment as possible for their needs. All the configuration will be determined by values.yaml - as per the Helm standard. DXCTL would still be an option for customers who don't want to use Helm. The deployment procedure that DXCTL will use will be directly and programmatically derived from our Helm chart, thus enabling the Helm chart to be our single point of truth regarding deployment structure.","title":"Helm as the primary deployment method"},{"location":"kube/k8s-next-conclusion/#milestone-1","text":"Summary Providing a simple, fresh deployment via Helm. Limitations - no official support for other ambassadors, OS-routes,... to reduce test effort - dxctl as a deployment option will follow in a later milestone","title":"Milestone 1"},{"location":"kube/k8s-next-conclusion/#runtime-controller-with-focus-on-operationcontroller-tasks","text":"Based on What is a Kube Operator we are not building an operator but a runtime controller. The wording needs to be precise here since the term operator implies certain functionalities which we are explicitly avoiding with a runtime controller. Instead of the focus on deploying and reconciling resources with a CRD, the runtime controller will focus on operational topics. Part of this is that it should not have to take care of application logic, that ideally should be handled by our applications themselves. The runtime controller should not act as a mitigation strategy for lack of application capabilities, but rather help with application-specific tasks and behaviors. This would include e.g. automatic rollout of configuration changes from ConfigMaps, verification of Kubernetes resource changes, overall health metrics, support during backup and restore procedures.","title":"Runtime Controller with focus on operation/controller tasks"},{"location":"kube/k8s-next-conclusion/#milestone-1_1","text":"Summary - Create a new runtime controller with - DAM persistence RW/RO failover - Autorollout config changes via Pod-Recycle Configuration changes via Helm Enable (disable) services like DAM Service configurations (DAM file size limits, CORS,..) Scaling Settings Horizontal Pod Autoscaler SSL certificate management for Ambassador and services DX Admin credential management (Secrets) Limitations - Deployment cannot run old operator and new runtime controller at the same time - SSL certificate management will only cover shipping the current certificate","title":"Milestone 1"},{"location":"kube/k8s-next-conclusion/#milestone-3","text":"Summary - auto renewal of interservice SSL certificates","title":"Milestone 3"},{"location":"kube/k8s-next-conclusion/#configmaps-as-configuration-backbone","text":"Since CRDs are cluster-wide resources that require special care (e.g. versioning), we will go with ConfigMaps as our single way of passing configuration values. CRDs also cannot be updated using Helm, which already poses issues for our SoFy deployments. Validation of ConfigMaps could be achieved by using VerificationHooks that call the runtime controller for sign-off on changes.","title":"ConfigMaps as configuration backbone"},{"location":"kube/k8s-next-conclusion/#milestone-1_2","text":"Summary Changes in ConfigMaps will be rolled out automatically by the runtime controller since our Containers are consuming ConfigMap values only on startup. Limitations - No verification Hooks","title":"Milestone 1"},{"location":"kube/k8s-next-conclusion/#containers-need-to-be-more-self-sustaining","text":"Our applications, especially the stateful ones, need to be improved to behave more naturally in a Kubernetes / Containerized Environment. This is especially true for DX Core. Currently, the operator mainly defines how DX Core starts, behaves and updates. Some of this will have to go into the DX Core container and away from the newly planned runtime controller.","title":"Containers need to be more self-sustaining"},{"location":"kube/k8s-next-conclusion/#milestone-1_3","text":"Summary DX-Core will become more self-aware. By definition of kube, containers have the naming scheme <pod-name>-<incrementing integer> like dx-core-0 to dx-core-(n-1) where n is the number of Pods. dx-core-0 will be the first core pod created and therefore: - run initialization of the profile - run db transfer for multi-pod environments All other pods will not start until: - init is done - db transfer has happened - This logic can apply in kube and docker-compose environments if we apply the same name schema to the pods/containers. After initialization, pods will be notified (by Helm or the Runtime Controller) of config changes via a checksum environment variable. When any pod starts it will compare this value to the config checksum value stored on the shared profile persistent volume. If the value has changed, then the pod will apply the configuration changes and afterwards update the value in the profile. Because of the order in which kube recycles pods in a stateful set when the spec changes, the pod making the changes should in practice always be dx-core-(n-1) . Limitations - providing a docker-compose file is a stretch goal","title":"Milestone 1"},{"location":"kube/k8s-next-conclusion/#vanilla-as-possible-custom-as-needed","text":"To keep the support for multiple Kubernetes platforms as easy as possible and the deployment maintainable for us, we want to use as few Kubernetes flavour-specific resources as possible. Furthermore we want to leverage as much as possible of the OOB functionality that Kubernetes provides for our applications regarding deployments, updates, scaling etc. This would also mean the we try to get away from seemingly different deployment behaviors on different flavours, like Openshift vs GKE.","title":"Vanilla as possible, Custom as needed"},{"location":"kube/k8s-next-conclusion/#milestone-1_4","text":"Summary Dropping the support of OS routes for Kubernetes Next deployments. Still relying on shared volumes where provided, NFS-Containers as a workaround where needed. Limitations - tbd","title":"Milestone 1"},{"location":"kube/k8s-next-conclusion/#enable-logging-using-kubernetes-logs","text":"We will use the sidecar container concept to expose as many logs from DX and its applications as possible, so that customers can directly use Kubernetes standard tools to retrieve them. In the future we could provide a fully fledged logging stack, that uses this logging concept and provides customers with proper logfile management.","title":"Enable logging using Kubernetes logs"},{"location":"kube/k8s-next-conclusion/#milestone-1_5","text":"Not considered in this milestone.","title":"Milestone 1"},{"location":"kube/k8s-next-conclusion/#establish-a-metricsmonitoring-stack","text":"We will expose application-specific metrics in our containers that can easily be consumed by software like Prometheus and allows customers to establish proper monitoring. In the future we could also provide our own monitoring stack allowing customers to easily see the current status of their deployment.","title":"Establish a metrics/monitoring stack"},{"location":"kube/k8s-next-conclusion/#milestone-1_6","text":"Not considered in this milestone.","title":"Milestone 1"},{"location":"kube/k8s-next-conclusion/#rolling-update","text":"We will provide a rolling upgrade capability so customers can easily update their deployment. A Rolling update does not imply a deployment with zero downtime for all features. However zero downtime is possible for rendering.","title":"Rolling update"},{"location":"kube/k8s-next-conclusion/#milestone-1_7","text":"Not considered in this milestone since there is nothing to update from.","title":"Milestone 1"},{"location":"kube/k8s-next-conclusion/#milestone-2","text":"Summary By design kube always updates Statefulsets by picking the pod with the highest number first. In all other scenarios kube picks a random pod. Pods are always upgraded one at a time. Once this pod is ready, the next pod will be upgraded. DAM is self-aware of its own updates. If a database change is needed, DAM will take care of it based of the release-to-release migration. 1. Kube recycles pod with new image - if no DB change is necessary proceed with step 4. 2. The new pod is initializing release-to-release migration. 3. The new pod is performing the release-to-release migration. 4. Once the migration is done it marks itself ready via ready-probe. 5. Kube updates the next pod with the new container image. 6. Kube updates the next pod until there are none on the old image version. Core image will be modified to support rolling updates. This will not change the core build itself but only the wrapping container. 1. Kube recycles pod with new image 2. Pod will create a new, cloned wp_profile directory on the shared volume 3. Pod will switch its own symlink from wp_profile to the cloned, versioned wp_profile 4. Pod will perform the actual upgrade and marks itself ready via ready-probe 5. Kube updates the next pod. 6. Pod checks if the new wp_profile directory exists and adjusts own symlink prior to startup 7. repeat steps 5 and 6 until there are no further pods on the old image The following flowcharts give more detail: Limitations - helm only upgrade - no rollback possible - stick with shared volume - writes for DAM will have a downtime - wp_profile changes from old core pods might be lost during upgrade","title":"Milestone 2"},{"location":"kube/k8s-next-conclusion/#undeploy","text":"","title":"Undeploy"},{"location":"kube/k8s-next-conclusion/#milestone-1_8","text":"Summary Use helm uninstall to remove the deployment. PVs will not be removed. Limitations - we do not clean up parts from the current Kubernetes strategy","title":"Milestone 1"},{"location":"kube/k8s-next-conclusion/#operatorruntime-controller-technology-stack","text":"As mentioned earlier within this document we are not building a fully fledged operator. Therefore we do not need to rely on an operator framework to manage CRDs etc. Based on the skillset within the DX development team we decided that the official kuberenetes client for Java is capable of everything we need and are therefore building a Java based runtime controller.","title":"Operator/Runtime Controller technology stack"},{"location":"kube/k8s-next-conclusion/#milestone-1_9","text":"Summary Creating a first version of the runtime controller including proper build, lint and test CI/CD. We also need a proper development setup based on maven and make files.","title":"Milestone 1"},{"location":"kube/k8s-next-deploy/","text":"Status Date APPROVED 22nd of April 2021 All the stuff which is documented here, it is was only created under a deployment perspective. Operations, undeploy and version updates are not part of this architecture concept. Requirements Transparency of resources and the whole deployment Try to meet the Kube standards in terms of different flavours Ongoing support for new versions of Kube API's Single point of configuration truth Transparency of resources and the whole deployment The complexity of our current deployment is to large. Customers have no real chance to determine how many resources will be consumed before they actually deploy it. The new deployment strategy need a clear summary of: min/max number of pods per service requested and limit of CPU&RAM scaling triggers required disk space resulting minimal footprint resulting maximum footprint Also is a deployment via a operator extremely non-transparent. A customer has no changes to get information about: What is going via the deployment What will be deployed What will be changed automatically ... Try to meet the Kube standards in terms of different flavours. The different kuberenetes flavours offer additional features to distinguish themselves in the market. While this is nice for individual application development, DX supports multiple vendors. Therefore it is very important that we are only following the kubernetes standard and not supporting some special functionality of other kubernetes flavour. Example of a special flavour functionality: Kubernetes is using Ingress to provide load balancing, SSL termination and name-based virtual hosting. OpenShift additionally offers Routes which provides a bit more functionality than Ingress. Since OpenShift is kubernetes compatible Ingress is also available. https://www.openshift.com/blog/kubernetes-ingress-vs-openshift-route To minimize our maintenance and support cost, it is crucial that we are not using or relying on special functionalities outside the kube standard. Ongoing support for new versions of Kube API's. Kubernetes and its internal API are rapidly changing. Therefore relying on the kubectl cmd is not an ideal way. We have no option to ensure that all deployments are working correctly with newer versions of kube. For that it would be ideal to have a framework as an abstraction layer which is our single interface to communicate with kube. There are multiple kube client libraries available . Some of theme are officially-supported and other are maintained by an open source community. We should only go with a officially-supported library. Officially-supported language are: Golang Python Java dotnet Javascript/Typescript Haskel To reduce the number of different languages we need to maintain within our development organization we should limit ourselves to one of these: Java Javascript/Typescript Single point of configuration truth From an easy maintaining and supporting perspective it is really important to have a single point of configuration which will be used for both deployments ways Helm-based and DXCTL-based. Future design Met some proposals for a new deployment approach For the new future design we have met the following proposals to align the future design with the requirements. Using Helm charts as our single point of configuration Supporting DXCTL as a alternative deployment way DXCTL should be implemented new with a other language then golang Don't use an operator Automatically roll deployment - provide for each application/deployment a separated ConfigMap and provide a Global ConfigMap for generic configurations Using a master consensus logic to executing changes on a multi pod deployment (like the execution of a ConfigTask on DX) Deployment designs HELM-based deployment OLD DIAGRAM DXCTL-based deployment OLD DIAGRAM Detail decisions informations Single configuration point for HELM and DXCTL deployment HELM Chart structure - Charts.yaml - values.yaml - templates/service.yaml - templates/deployments.yaml - ... The values.yaml file provided a way to collect all required values on a central place. The properties form values.yaml can be used in each template. Exports YAML files With the following CMD it is possible to export all templates. During the export process all used values.yaml properties will be replaced with the right value. helm template <CHART-NAME> A example output of this CMD is a text based output and should be looks like that one: --- # Source: mychart/templates/serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata: name: RELEASE-NAME-mychart labels: helm.sh/chart: mychart-0.1.0 app.kubernetes.io/name: mychart app.kubernetes.io/instance: RELEASE-NAME app.kubernetes.io/version: \"1.16.0\" app.kubernetes.io/managed-by: Helm --- # Source: mychart/templates/service.yaml apiVersion: v1 kind: Service ... This output can be stored as an YAML file and can be apply to a cluster via the kubectl tool. The DXCTL tool needs a new functionality to apply a bunch of YAML files to a kube cluster. New implementation of the DXCTL In our DX organization we have extremely less experience in GoLang. The most common skills are present in Java and Javascript/Typescript. Therefore it is from an maintaining and supporting perspective important to write the DXCTL tool in one of this language. Besides of that is the actual code quality not so good. Missing linting Missing tests Missing use of global constants A non-transparent proprietary mapping concept Bad code structure Contained unused code Code only copied from the operator All in all it makes more sense to write the DXCTL tool new. Don't use an operator for the deployment What are the key features at the moment of our DX CLOUD OPERATOR ? Scaling Triggering of some task (like a DX ConfigEngine task) Decided to use Routes (OpenShift) and the Ambassador for the rest Installation of our apps (RingAPI, CC, DAM) All this task are also possible to do that without an operator. The full status quo of the operator you can find here . What are the main reason why we should don't use an operator for the deployment? The work of an operator is to non-transparent. Customers don't like it if they don't know what's happened during the deployment. Automatically roll deployment - provide for each application/deployment a separated ConfigMap and provide a Global ConfigMap for all together For a automatically roll deployment it would be good to have the configuration attributes are separated by deployments. Therefore it is possible to check the special ConfigMap if the values have changed and the special pods must be deployed again. Helm can helps us here with a checksum. The checksum of a special ConfigMap must be a part of the deployment metadata annotation. https://helm.sh/docs/howto/charts_tips_and_tricks/#automatically-roll-deployments Using a leader consensus logic to executing changes on a multi pod deployment (like the execution of a ConfigTask on DX) The leader consensus logic we need to find which is the leader from the multi pods. Only of the leader it is possible to run a ConfigEngin task. This task will change the wp-profile which is used by all core pods. Kubernetes self used etcd as a consensus system. etcd is using the Raft consensus algorithm. Here is a really nice article and demo of Raft.","title":"Deployment"},{"location":"kube/k8s-next-deploy/#requirements","text":"Transparency of resources and the whole deployment Try to meet the Kube standards in terms of different flavours Ongoing support for new versions of Kube API's Single point of configuration truth","title":"Requirements"},{"location":"kube/k8s-next-deploy/#transparency-of-resources-and-the-whole-deployment","text":"The complexity of our current deployment is to large. Customers have no real chance to determine how many resources will be consumed before they actually deploy it. The new deployment strategy need a clear summary of: min/max number of pods per service requested and limit of CPU&RAM scaling triggers required disk space resulting minimal footprint resulting maximum footprint Also is a deployment via a operator extremely non-transparent. A customer has no changes to get information about: What is going via the deployment What will be deployed What will be changed automatically ...","title":"Transparency of resources and the whole deployment"},{"location":"kube/k8s-next-deploy/#try-to-meet-the-kube-standards-in-terms-of-different-flavours","text":"The different kuberenetes flavours offer additional features to distinguish themselves in the market. While this is nice for individual application development, DX supports multiple vendors. Therefore it is very important that we are only following the kubernetes standard and not supporting some special functionality of other kubernetes flavour. Example of a special flavour functionality: Kubernetes is using Ingress to provide load balancing, SSL termination and name-based virtual hosting. OpenShift additionally offers Routes which provides a bit more functionality than Ingress. Since OpenShift is kubernetes compatible Ingress is also available. https://www.openshift.com/blog/kubernetes-ingress-vs-openshift-route To minimize our maintenance and support cost, it is crucial that we are not using or relying on special functionalities outside the kube standard.","title":"Try to meet the Kube standards in terms of different flavours."},{"location":"kube/k8s-next-deploy/#ongoing-support-for-new-versions-of-kube-apis","text":"Kubernetes and its internal API are rapidly changing. Therefore relying on the kubectl cmd is not an ideal way. We have no option to ensure that all deployments are working correctly with newer versions of kube. For that it would be ideal to have a framework as an abstraction layer which is our single interface to communicate with kube. There are multiple kube client libraries available . Some of theme are officially-supported and other are maintained by an open source community. We should only go with a officially-supported library. Officially-supported language are: Golang Python Java dotnet Javascript/Typescript Haskel To reduce the number of different languages we need to maintain within our development organization we should limit ourselves to one of these: Java Javascript/Typescript","title":"Ongoing support for new versions of Kube API's."},{"location":"kube/k8s-next-deploy/#single-point-of-configuration-truth","text":"From an easy maintaining and supporting perspective it is really important to have a single point of configuration which will be used for both deployments ways Helm-based and DXCTL-based.","title":"Single point of configuration truth"},{"location":"kube/k8s-next-deploy/#future-design","text":"","title":"Future design"},{"location":"kube/k8s-next-deploy/#met-some-proposals-for-a-new-deployment-approach","text":"For the new future design we have met the following proposals to align the future design with the requirements. Using Helm charts as our single point of configuration Supporting DXCTL as a alternative deployment way DXCTL should be implemented new with a other language then golang Don't use an operator Automatically roll deployment - provide for each application/deployment a separated ConfigMap and provide a Global ConfigMap for generic configurations Using a master consensus logic to executing changes on a multi pod deployment (like the execution of a ConfigTask on DX)","title":"Met some proposals for a new deployment approach"},{"location":"kube/k8s-next-deploy/#deployment-designs","text":"","title":"Deployment designs"},{"location":"kube/k8s-next-deploy/#helm-based-deployment","text":"OLD DIAGRAM","title":"HELM-based deployment"},{"location":"kube/k8s-next-deploy/#dxctl-based-deployment","text":"OLD DIAGRAM","title":"DXCTL-based deployment"},{"location":"kube/k8s-next-deploy/#detail-decisions-informations","text":"","title":"Detail decisions informations"},{"location":"kube/k8s-next-deploy/#single-configuration-point-for-helm-and-dxctl-deployment","text":"","title":"Single configuration point for HELM and DXCTL deployment"},{"location":"kube/k8s-next-deploy/#helm-chart-structure","text":"- Charts.yaml - values.yaml - templates/service.yaml - templates/deployments.yaml - ... The values.yaml file provided a way to collect all required values on a central place. The properties form values.yaml can be used in each template.","title":"HELM Chart structure"},{"location":"kube/k8s-next-deploy/#exports-yaml-files","text":"With the following CMD it is possible to export all templates. During the export process all used values.yaml properties will be replaced with the right value. helm template <CHART-NAME> A example output of this CMD is a text based output and should be looks like that one: --- # Source: mychart/templates/serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata: name: RELEASE-NAME-mychart labels: helm.sh/chart: mychart-0.1.0 app.kubernetes.io/name: mychart app.kubernetes.io/instance: RELEASE-NAME app.kubernetes.io/version: \"1.16.0\" app.kubernetes.io/managed-by: Helm --- # Source: mychart/templates/service.yaml apiVersion: v1 kind: Service ... This output can be stored as an YAML file and can be apply to a cluster via the kubectl tool. The DXCTL tool needs a new functionality to apply a bunch of YAML files to a kube cluster.","title":"Exports YAML files"},{"location":"kube/k8s-next-deploy/#new-implementation-of-the-dxctl","text":"In our DX organization we have extremely less experience in GoLang. The most common skills are present in Java and Javascript/Typescript. Therefore it is from an maintaining and supporting perspective important to write the DXCTL tool in one of this language. Besides of that is the actual code quality not so good. Missing linting Missing tests Missing use of global constants A non-transparent proprietary mapping concept Bad code structure Contained unused code Code only copied from the operator All in all it makes more sense to write the DXCTL tool new.","title":"New implementation of the DXCTL"},{"location":"kube/k8s-next-deploy/#dont-use-an-operator-for-the-deployment","text":"What are the key features at the moment of our DX CLOUD OPERATOR ? Scaling Triggering of some task (like a DX ConfigEngine task) Decided to use Routes (OpenShift) and the Ambassador for the rest Installation of our apps (RingAPI, CC, DAM) All this task are also possible to do that without an operator. The full status quo of the operator you can find here . What are the main reason why we should don't use an operator for the deployment? The work of an operator is to non-transparent. Customers don't like it if they don't know what's happened during the deployment.","title":"Don't use an operator for the deployment"},{"location":"kube/k8s-next-deploy/#automatically-roll-deployment-provide-for-each-applicationdeployment-a-separated-configmap-and-provide-a-global-configmap-for-all-together","text":"For a automatically roll deployment it would be good to have the configuration attributes are separated by deployments. Therefore it is possible to check the special ConfigMap if the values have changed and the special pods must be deployed again. Helm can helps us here with a checksum. The checksum of a special ConfigMap must be a part of the deployment metadata annotation. https://helm.sh/docs/howto/charts_tips_and_tricks/#automatically-roll-deployments","title":"Automatically roll deployment - provide for each application/deployment a separated ConfigMap and provide a Global ConfigMap for all together"},{"location":"kube/k8s-next-deploy/#using-a-leader-consensus-logic-to-executing-changes-on-a-multi-pod-deployment-like-the-execution-of-a-configtask-on-dx","text":"The leader consensus logic we need to find which is the leader from the multi pods. Only of the leader it is possible to run a ConfigEngin task. This task will change the wp-profile which is used by all core pods. Kubernetes self used etcd as a consensus system. etcd is using the Raft consensus algorithm. Here is a really nice article and demo of Raft.","title":"Using a leader consensus logic to executing changes on a multi pod deployment (like the execution of a ConfigTask on DX)"},{"location":"kube/k8s-next-helm-incubator-values/","text":"Introduction It turns out to be a very common case, that we can not get certain features done within just one release. Hence implementation spans multiple releases and some of the new implementation will be available in the release that we are shipping before the new feature was finalized. We need to ensure a way to enable development to use these new features in development deployments and for testing and at the same time minimize efforts during endgame to make these unfinished features \"invisible\" for a release and we need to prevent documentation efforts for features that might be somewhat visible to customers, but which should actually not be used yet. Example case \"central logging configuration\" We have been working on our central logging configuration for quite some time. At the time of writing, we are right before the endgame sprint of CF199 and we are in the situation that we now have to remove logging configuration options just in the release branch, so that customers don't see the future capabilities and we also don't have to document anything we would not even want to be documented yet. Solution proposal To circumvent these kinds of situations, the proposal is to establish a so called \"incubator\" section inside our helm values.yaml file. We'll add appropriate documentation so that it is obvious to our customers that configuration options within this section are solely for internal use yet. As soon as we then finalize a feature, its configuration will move out of the incubator section, which will sort of represent the release of the feature, which would also then include appropriate end user documentation, too. A nice side effect of the approach is that, we finally would have a good place to hold feature flag configuration e.g. for some beta features that we would only want to allow to use for certain customers. This example values.yaml extract provides a proposal on how the configuration extension would look like: # Image related configuration images: ... # Resource allocation settings, definition per pod # Use number + unit, e.g. 1500m for CPU or 1500M for Memory resources: ... # This is the incubator that contains internal and/or not yet published configuration pieces. # Please only touch any of the incubator configurations in case our support teams ask you to. # Feel free to have a look at new configuration items that might be released in future and make sure to ask any question you might have! incubator: # Logging configuration logging: # Notice: log level settings are currently under development # Core specific logging configuration core: ...","title":"Incubator configurations"},{"location":"kube/k8s-next-helm-incubator-values/#introduction","text":"It turns out to be a very common case, that we can not get certain features done within just one release. Hence implementation spans multiple releases and some of the new implementation will be available in the release that we are shipping before the new feature was finalized. We need to ensure a way to enable development to use these new features in development deployments and for testing and at the same time minimize efforts during endgame to make these unfinished features \"invisible\" for a release and we need to prevent documentation efforts for features that might be somewhat visible to customers, but which should actually not be used yet.","title":"Introduction"},{"location":"kube/k8s-next-helm-incubator-values/#example-case-central-logging-configuration","text":"We have been working on our central logging configuration for quite some time. At the time of writing, we are right before the endgame sprint of CF199 and we are in the situation that we now have to remove logging configuration options just in the release branch, so that customers don't see the future capabilities and we also don't have to document anything we would not even want to be documented yet.","title":"Example case \"central logging configuration\""},{"location":"kube/k8s-next-helm-incubator-values/#solution-proposal","text":"To circumvent these kinds of situations, the proposal is to establish a so called \"incubator\" section inside our helm values.yaml file. We'll add appropriate documentation so that it is obvious to our customers that configuration options within this section are solely for internal use yet. As soon as we then finalize a feature, its configuration will move out of the incubator section, which will sort of represent the release of the feature, which would also then include appropriate end user documentation, too. A nice side effect of the approach is that, we finally would have a good place to hold feature flag configuration e.g. for some beta features that we would only want to allow to use for certain customers. This example values.yaml extract provides a proposal on how the configuration extension would look like: # Image related configuration images: ... # Resource allocation settings, definition per pod # Use number + unit, e.g. 1500m for CPU or 1500M for Memory resources: ... # This is the incubator that contains internal and/or not yet published configuration pieces. # Please only touch any of the incubator configurations in case our support teams ask you to. # Feel free to have a look at new configuration items that might be released in future and make sure to ask any question you might have! incubator: # Logging configuration logging: # Notice: log level settings are currently under development # Core specific logging configuration core: ...","title":"Solution proposal"},{"location":"kube/k8s-next-operation/","text":"Introduction We want to provide a easy to maintain, reliable and transparent Kubernetes deployment. From a operation perspective this means that customers should have the ability to know exactly in what state their deployment is, what resources are maintained, how to create backups and do restores etc. This document provides a proposal on how this deployment could look like. Deployment structure Overview Application Overview Operator influence Deployed applications DX Core Kubernetes Type: StatefulSet with two persistent volumes (1x RWM, 1x RWO) The DX Core application is currently our application with the highest complexity in the Kubernetes deployment. This is mainly driven by the fact that DX Core, unlike our new applications, was not designed for a containerized deployment. The original deployment of DX Core in a clustered fashion usually relied on having multiple Machines / Nodes that have WAS ND deployed. WAS ND would then consist of deployment manager (DMGR) and multiple nodes - This is called a Cell. The primary configuration of all applications would be maintained on the DMGR and then synced throughout the Cell to all nodes. The applications, in our case DX, would be running on multiple nodes and be therefore highly available. In a containerized deployment, the goal is usually to get away from such stateful units. Especially the DMGR would be difficult. To get around that issue and still have a highly available deployment (and scalability) we leverage the \"Farming\" approach for deployment. In that case, each node - or in terms of Kubernetes: Pod - will access a shared persistent storage containing the DX profile, there is no DMGR that defines the profile and pushes it to the node. All nodes see the same profile at all time. The farming deployment brings certain issues with it, that should be mentioned: ConfigEngine Tasks can and must only be run on one DX instance at a time. Other instances in the farm are not allowed to run those tasks. All Nodes / Pods require a shared persistent file storage, which will then usually be something like NFS. To ensure that only one DX Pod is performing the initial setup in Kubernetes and runs ConfigEngine tasks, the current implementation leverages an operator which ensures that before tasks are run / init is done, only one DX Pod is running. That has the following implications: Startup behavior of DX Core is completely defined by the operator, the container image itself remains relatively \"dumb\" During configurations DX Core is not highly available and no load balancing between multiple Pods is possibly, since only one Pod is running There is a distinct relationship between DX Core and the operator. Versions of both must be matched or a successful operation is not guaranteed/supported There are multiple ways to go: Keep an operator that ensures that only DX Core Pod is running to prevent multiple execution of ConfigEngine Tasks / Writing conflicts Enhance the DX Core image + Kubernetes deployment to allow DX Core to have a consensus based configuration The preferred way would be having a consensus based deployment of multiple Pods containing DX Core. This means, that there would also be one elected leader Pod, which executed ConfigEngine tasks, while there are one or multiple follower Pods, which just read the profile. Also it could be thought about, having a persistence sync from leader to followers which is not using a RWM Volume. This would ease deployments, since not all cloud providers allow for RWM volumes. Using a consesus based deployment could dramatically reduce the dependency to an operator and could possibly allow for maintaining HA during configuration changes. RingAPI Kubernetes Type: Deployment/ReplicaSet - stateless The RingAPI is a simple part of our deployment. It basically performs API wrapping actions for DX Core. It does not have any persistence, neither via a DB connection nor via a persistent volume. Thus scaling, initialization and running requires no application specific specialties. Content Composer Kubernetes Type: Deployment/ReplicaSet - stateless The Content Composer is a web application that consumes APIs provided by the RingAPI. It mainly consists of a web server that hosts static JS files used for the web application. It does not have any persistence, neither via a DB connection nor via a persistent volume. Thus scaling, initialization and running requires no application specific specialties. To use content composer integrated in DX, it is necessary to run a ConfigEngine task. DAM Kubernetes Type: StatefulSet with one persistent volume (1x RWM) The Digital Asset Management is a web application that consists of a UI and an API server. The API server uses a persistent volume and a database to provide functionality. DAM is designed to have no writing conflicts between multiple Pods. The application itself will ensure that all configuration tasks are only performed by one Pod. There is not special startup or initialization that needs to be performed by an operator or similar. To use DAM integrated inside DX, it is necessary to run a ConfigEngine task. DAM Persistence Kubernetes Type: Two StatefulSet with two persistent volumes (1x RWO, 1x RWO) The DAM Persistence consists of a RW leader and a RO follower, both having their own persistent volume. Replication is maintained by application logic (PostgreSQL). Since our current implementation of PostgreSQL has no real HA or loadbalancing, we currently use an operator to automatically switch over to RO if the RW Pod dies. This dependency on an operator must be removed for multiple reasons: Switchover from RW to RO is not zero-downtime No load balancing between multiple PostgreSQL Pods No scaling for load surges, we are basically limited to vertical scaling (e.g. allow for much CPU and Memory on a single DB instance) Operator switchover adds complexity to the deployment Operator switchover forms a possibly single point of failure The solution to that is to have a properly scalable Persistence Layer, e.g. HA PostgreSQL. With such a setup, the manually build switchover can be removed and the necessity for a DAM operator vanishes. Image Processor Kubernetes Type: Deployment/ReplicaSet - stateless The image processor is a very simple API based computing application that performs image manipulation. The Pod is completely stateless, therefore scaling, initialization and running requires no application specific specialties. Conclusion Our current implementation of operations using operators is mainly based on certain issues with our applications themselves. Instead of building solutions to handle the symptoms, we should focus on fixing them to make such constructs obsolete and reduce complexity. The goal in a containerized deployment should be to make applications ready for it and not force applications not made for it into a specific deployment structure. Side note: Having operators carrying much logic also has implications for customers that may not want to use Kubernetes, but rather just docker because they are just trying out things or want to quickly spin up development environments. Since operators rely on Kubernetes for their actions, all functionality they provide will not be directly available outside Kubernetes. Having our container images taking more responsibility for themselves allows customers and developers to get a faster start with DX. Monitoring / Metrics We need to enable monitoring capabilities for our customers. This topic can be split into two parts: Health/Monitoring data exposure Data Aggregation and Visualization It is worth having a look at the current implementation of the SoFy team, which at least provides monitoring out of the box for Kubernetes seemingly using Prometheus and Grafana. Health/Monitoring data exposure All our application Pods should expose a metrics endpoint that can be scraped by a metrics data aggregation tooling. The endpoint should ideally be available at the same port for all Pods, thus making the configuration easier and more straight forward. The format should orient on what is best practice, e.g. the Prometheus format. Kubernetes Resource Data Kubernetes itself provides metrics using the Kube state metrics . This contains metrics regarding Kubernetes resources like nodes and Pods. The data provided by the metrics are in a Prometheus consumable format. Java Application Data For Java we can leverage the JMX Exporter . It provides metrics about the state of a JVM in a Prometheus consumable format. NodeJS Application Data For NodeJS we can use the Prom-Client . It is a npm package that provides various metrics regarding the NodeJS eventloop, heap usage etc. It also allows to configure custom metrics that can be exposed. The data format is Prometheus consumable. DB / PostgreSQL Data For our current persistence stack we can use PostgreSQL Exporter . It exposes various DB related metrics and stats directly coming from the database itself. The output format is Prometheus consumable. Data Aggregation All metrics data exposure is worthless if we don't gather that data somewhere central. For that purpose applications like Prometheus should be used. Prometheus is a scraping + persistence engine that will automatically scrape configured endpoints for exported metrics in a defined interval. It allows for specific querying to view data. There are already existing prometheus helm charts that could be leveraged as a dependency or inspiration in our helm charts. For visualization Grafana may present a good choice. It is widely adopted and provides out of the box compatibility with Prometheus. It also comes with a default set of helm charts . Logging It is important that our customers are able to easily access logs during runtime. This is necessary for them to configure e.g. notifications based on certain log messages, as well as for debugging issues. Using Sidecar Containers for log exposure The Kubernetes stack already provides a logging mechanism which exposes logs of containers / Pods. The logs maintained here are based of the stdout and stderr streams of the containers and their respective running application. This principle has a disadvantage: Whenever your application uses something different for logging, e.g. typically files, you may only be able to tail the output of one file during runtime, thus only expose one log file in the Pod. Kubernetes provides the concept of sidecar containers for exactly that case. Each sidecar container can expose a file as a log stream inside Kubernetes, allowing Kubernetes to log multiple log files of a single application. Since sidecar containers share the same volumes as the run containers, they can access and tail the log files, allowing for an easy implementation. Log output gathering Since Kubernetes already gathers all log output of Pods and their containers, the usual approach is to gather the logs via Kubernetes itself. There are two often used ways to do logging in Kubernetes: EKF / ELK (Elasticsearch for data aggregation, Kibana for visualization and Fluentd for log data collection) Loki + Grafana (Loki for aggregation, Grafana for visualization) The basic concept behind both is to take the logs of each cluster Node (which include the logs of the Pods running on that cluster), aggregate them and them add a visual interface to access them for monitoring, search and debugging. Changing configurations Adjusting resource allocations Changing resource allocations like CPU/Memory requests or limits would be applied directly on the definition of Deployments/ReplicaSets. When the values have changed, Kubernetes will internally trigger its own reconciliation and roll out the update to the resources. Adjusting enabled/disabling applications If a customer wants to enable/disable particular parts of the deployment, this should be done via the Helm upgrade process. Since we deploy the necessary Resources incl. Routes, StatefulSets etc. directly during deployment and not via an operator, enabling the application in the values.yaml of helm would then create or destroy the resources based on the enablement condition. It would also adjust the configuration of the DX Core Pods, thus leading to the leader of those Pods to configure the application, e.g. DAM. Change application configuration Since we would like to store our configuration in configMaps, we'd like to know when they have changed and rollout the updated configuration. Therefore we need a mechanism to trigger the rollout of that update via StatefulSets/Deployments. An easy way to do so, is to add a checksum annotation to the Pods of our StatefulSets/Deployments for each configMap they care about. As soon as a configMap is changed, the operator will update the checksum of the affected resources and Kubernetes will start to roll out updated Pods or restart them, performing a rolling update. Change validation Our configuration lives within configMaps and Kubernetes resources. Those are created by Helm or with DXCTL using Helm created yaml files. There are multiple ways to validate changes to the resources we are managing: Using Helm Helm supports the usage of schema files for its values.yaml . Since all values we use in our deployment are derived from that file during install or upgrade, we can leverage that check. Using Kubernetes validation webhooks Kubernetes provides the principle of validation webhooks . In combination with an operator, that provides validation endpoints, we would be able to reject wrong/undesired changes to configMaps directly in Kubernetes during runtime. We can also provide fitting error messages that will the customers why the changes they planned to do will not work. This would not only be applicable to configMaps, but also to other Kubernetes Resources e.g. our Deployments or StatefulSets. Application lifecycle Initial startup During the initial startup we need to ensure that DX Core will only run the initialization once and only in one running Pod. Since the wp_profile is shared between all Pods, the initialized profile will be available to all Pods after initialization. The initialization flow could look the following way: The desired set of DX Core Pods is created by the StatefulSet The Pods negotiate a leader Pod, all others will act as followers. The leading Pod will start the initialization process, all other Pods will not start DX Core until the initialization has been completed After the initialization has been performed by the leader, all Pods (incl. followers) will start their DX Core process The application is initialized and ready to serve This requires communication between the DX Core Pods, e.g. on the shared volume. The current state of initialization could be persistent in a lock/timestamp file inside wp_profile. Other applications like DAM for example do already have initialization logic that prevents multiple Pods from colliding. Scaling We can use HorizontalPodAutoscalers that are included in Kubernetes to automatically scale services up and down based on CPU and memory usage. In case we want to leverage custom metrics, e.g. DAM operations pressure, there are also ways to establish custom metrics in Kubernetes and use them in autoScalers. Backup & Restore Backup and Restore tasks could be run as Jobs, which are one-off Pods in Kubernetes that fulfill a specific task and return a completion status. Those jobs could be created by the operator, since they need to be created during the deployments lifecycle. Creation of those backup tasks could either be triggered manually, or run automatically in defined intervals. Backups could include copying specific files to backup volumes, dumping deployment information and state or even trigger functions like EXIM in DAM and put the output to a desired spot. Restoring of before used backups should also be managed by the operator, as it would be able to bring the applications into their desired state before re-importing the data into them. It could also do validate if a restore was successful, e.g. by checking specific health-checks of individual applications. Used Kubernetes structures Operators/Controllers There are certain actions inside our deployment that would require manual intervention to get to the desired goal. The basic principle of watching a type of resource and acting accordingly to changes is considered a controller in the Kubernetes ecosystem. An operator on the other hand can consist of multiple controllers for different resources and implements applications specific knowledge/actions. A basic example would be a reaction on configuration changes. When we change the configMap of one our applications, we want to make that application aware of the introduced changes. This is true for stateful and stateless applications. A simple, but yet effective way to force the rollout of those changes is to have a annotation at the Pods level, which just contains a checksum of the applications config map. If that checksum changes, the application will automatically cycled through, performing a rollout of the new configuration to all Pods. Helm could provide the checksum during execution. If the value changes happen in Helm and the customer uses Helm upgrade to apply those changes, we could easily update the annotations during that automatically and the changes will be applied. Since that builds up a complexity in the Helm charts and e.g. would not work if customers use the plain yaml output with DXCTL, unless we port that functionality to DXCTL. If the customer applies changes directly in Kubernetes on configMap level, we would not get any automated rollout. To cover all different aspects of changes to configuration, having an operator that watches configurations and modifies our Pod definitions accordingly, would ease the process for customers a lot and reduce implementation overhead in both Helm and DXCTL. Other cases where the operator could act with application specific logic: Operator/Controller use cases Leading Pod only Routes There may be the necessity to have routes that should only go to the leading DX Core Pod. Certain actions, like config changes should only happen in one of our DX Core Pods. The operator could use a leader lookup routine to configure certain routes to point to exactly that Pod, instead of pointing to a Service consisting of multiple Pods. This would require the operator to check who is the leading DX Core Pod in a regular fashion and update the Endpoint for the corresponding route. This could be a repeatable pattern, where we could have multiple pre defined Endpoints that share a common label, allowing the operator to apply the change easily to multiple routes at the same time. Automated ConfigMap application As mentioned in the introduction before, configuration changes on ConfigMap level do not induce a automatic rollout on dependent resources in Kubernetes. This logic needs to be established by us. The operator could watch all relevant ConfigMaps and adjust the checksum annotation of all dependent resources, e.g. StatefulSets and Deployments. The built in Kubernetes reconciliation would then automatically roll out those changes. DAM persistence RO fallback As long as we do not have a true loadbalanced HA persistence solution for DAM, we will rely on a ReadWrite Primary and ReadOnly Secondary PostgreSQL instance, which do automatically replicate. In order to have a fallback in the case of an outage of the primary DB, the operator would change the routing to the DB for DAM automatically to the secondary. This would be restored to use the primary as soon as the primary DB is up and ready again. DX Core configuration Proxy If we are running multiple instances of DX Core, we want customers to execute configuration changes only on the leading Pod. The operator could perform a leader lookup and perform as a proxy for such executions. Instead of directly executing kubectl exec commands in one of the DX Core Pods, they would be pointed to the operator Pod and could either be executed through a direct SSH tunnel to the Core Pod or initiating the Kubernetes exec command on the DX Core Pod. Customers would not need to know which of the Cores is the current leading one. Leading Pod identification For configuration and debugging purpose it may be useful to know, which DX Core Pod is currently the leading one. A simple command that can be triggered in the operator Pod that return the Pod Name could already be sufficient. For other use cases the operator would already need to know which Pod is leading. Configuration validation The operator could provide a validation endpoint that can be used to validate the changes to Resources during runtime. This could include type checks and even specific value verification. DX Deployment Specific Metrics We can use the operator to expose metrics endpoints that aggregate DX deployment relevant health and status information. Since the operator can possibly know all members of the deployment in its namespace, it can act as a point of entry for quick health checks. Custom Resources Definition vs. configMaps A classical operator design usually incorporates the use of a Custom Resource Definition or short CRD. A CRD is an extension to the Kubernetes API and basically allows for creation of custom resource definitions that reflect whatever is necessary. In our case, we used a custom resource (CR) called dxDeployment in the past. What content can be inside that CR is defined by the CRD. You could think of the CRD as a schema or object definition, from which you can create Objects inside Kubernetes. This definition can also contain validation of values. The operator would use this CR as its configuration backbone. ConfigMaps on the other hand are vanilla Kubernetes resources that contain a list of key-value-pairs. The values of those pairs could either be a single value or even be a whole file structure. Still in the end the principle is always key-value-pair. This is a simple data structure that allows for easy editing, but not so much flexibility in regards of structure. ConfigMaps, unlike CRs, can be directly consumed in some Kubernetes resources. Pods can either mount configMaps as a volume inside the Containers or extract values from them and set them as environment variables inside Containers. There is no necessary conversion logic that needs to be implemented. If we perform changes to our configuration, e.g. add a field, we would require the CRD to be updated prior to updating the CR. Also our operator would need the be update, even if it would only pass the configuration field through to another Kubernetes resource. Example: We have a CRD that defines certain parts of Pod specification. Unfortunately, we did not incorporate labels in the past. By our implementation logic, we take add the field to the CRD, then enhance the operator to be able to consume that field and pass it on through the Pod definition e.g. inside a StatefulSet. This apparently small change to just to enable a field for a Pod specification requires changes on CRD definition, changes to operator code, rolling out a new CRD and operator. Alternatively the label could just have directly been added to the StatefulSet in the first place, without being passed through an operator. Also, there has been a lot of discussion the the Helm community regarding the lifecycle of CRDs. Therefore Helm does currently not support updating CRDs , which will make the use of CRDs with SoFy harder for us and our customers, since SoFy relies on Helm. Because CRDs define a content/config structure and are cluster wide resources, special attention has to go into version updates of CRDs, updating them and testing use/edge cases like having two versions of a CRD in the cluster because two different versions of DX are deployed. (E.g. CF191 and CF193 on SoFy sandbox clusters.) Since an operator could also watch ConfigMaps as a source of configuration, we would like to use those. Glossary DMGR: Deployment Manager from WAS ND, maintains deployment cells containing multiple WAS nodes. WAS ND: Websphere Application Server Network Deployment, Java application server used by DX Core. Cell: In WAS Terms consists of a DMGR and multiple nodes. RWO: ReadWriteOnce Volume - can only be accessed by one Pod, one to one relationship between volume and Pod. RWM: ReadWriteMany Volume - can be accessed by multiple Pods, one to many relationship between volume and Pods.","title":"Operations"},{"location":"kube/k8s-next-operation/#introduction","text":"We want to provide a easy to maintain, reliable and transparent Kubernetes deployment. From a operation perspective this means that customers should have the ability to know exactly in what state their deployment is, what resources are maintained, how to create backups and do restores etc. This document provides a proposal on how this deployment could look like.","title":"Introduction"},{"location":"kube/k8s-next-operation/#deployment-structure","text":"","title":"Deployment structure"},{"location":"kube/k8s-next-operation/#overview","text":"","title":"Overview"},{"location":"kube/k8s-next-operation/#application-overview","text":"","title":"Application Overview"},{"location":"kube/k8s-next-operation/#operator-influence","text":"","title":"Operator influence"},{"location":"kube/k8s-next-operation/#deployed-applications","text":"","title":"Deployed applications"},{"location":"kube/k8s-next-operation/#dx-core","text":"Kubernetes Type: StatefulSet with two persistent volumes (1x RWM, 1x RWO) The DX Core application is currently our application with the highest complexity in the Kubernetes deployment. This is mainly driven by the fact that DX Core, unlike our new applications, was not designed for a containerized deployment. The original deployment of DX Core in a clustered fashion usually relied on having multiple Machines / Nodes that have WAS ND deployed. WAS ND would then consist of deployment manager (DMGR) and multiple nodes - This is called a Cell. The primary configuration of all applications would be maintained on the DMGR and then synced throughout the Cell to all nodes. The applications, in our case DX, would be running on multiple nodes and be therefore highly available. In a containerized deployment, the goal is usually to get away from such stateful units. Especially the DMGR would be difficult. To get around that issue and still have a highly available deployment (and scalability) we leverage the \"Farming\" approach for deployment. In that case, each node - or in terms of Kubernetes: Pod - will access a shared persistent storage containing the DX profile, there is no DMGR that defines the profile and pushes it to the node. All nodes see the same profile at all time. The farming deployment brings certain issues with it, that should be mentioned: ConfigEngine Tasks can and must only be run on one DX instance at a time. Other instances in the farm are not allowed to run those tasks. All Nodes / Pods require a shared persistent file storage, which will then usually be something like NFS. To ensure that only one DX Pod is performing the initial setup in Kubernetes and runs ConfigEngine tasks, the current implementation leverages an operator which ensures that before tasks are run / init is done, only one DX Pod is running. That has the following implications: Startup behavior of DX Core is completely defined by the operator, the container image itself remains relatively \"dumb\" During configurations DX Core is not highly available and no load balancing between multiple Pods is possibly, since only one Pod is running There is a distinct relationship between DX Core and the operator. Versions of both must be matched or a successful operation is not guaranteed/supported There are multiple ways to go: Keep an operator that ensures that only DX Core Pod is running to prevent multiple execution of ConfigEngine Tasks / Writing conflicts Enhance the DX Core image + Kubernetes deployment to allow DX Core to have a consensus based configuration The preferred way would be having a consensus based deployment of multiple Pods containing DX Core. This means, that there would also be one elected leader Pod, which executed ConfigEngine tasks, while there are one or multiple follower Pods, which just read the profile. Also it could be thought about, having a persistence sync from leader to followers which is not using a RWM Volume. This would ease deployments, since not all cloud providers allow for RWM volumes. Using a consesus based deployment could dramatically reduce the dependency to an operator and could possibly allow for maintaining HA during configuration changes.","title":"DX Core"},{"location":"kube/k8s-next-operation/#ringapi","text":"Kubernetes Type: Deployment/ReplicaSet - stateless The RingAPI is a simple part of our deployment. It basically performs API wrapping actions for DX Core. It does not have any persistence, neither via a DB connection nor via a persistent volume. Thus scaling, initialization and running requires no application specific specialties.","title":"RingAPI"},{"location":"kube/k8s-next-operation/#content-composer","text":"Kubernetes Type: Deployment/ReplicaSet - stateless The Content Composer is a web application that consumes APIs provided by the RingAPI. It mainly consists of a web server that hosts static JS files used for the web application. It does not have any persistence, neither via a DB connection nor via a persistent volume. Thus scaling, initialization and running requires no application specific specialties. To use content composer integrated in DX, it is necessary to run a ConfigEngine task.","title":"Content Composer"},{"location":"kube/k8s-next-operation/#dam","text":"Kubernetes Type: StatefulSet with one persistent volume (1x RWM) The Digital Asset Management is a web application that consists of a UI and an API server. The API server uses a persistent volume and a database to provide functionality. DAM is designed to have no writing conflicts between multiple Pods. The application itself will ensure that all configuration tasks are only performed by one Pod. There is not special startup or initialization that needs to be performed by an operator or similar. To use DAM integrated inside DX, it is necessary to run a ConfigEngine task.","title":"DAM"},{"location":"kube/k8s-next-operation/#dam-persistence","text":"Kubernetes Type: Two StatefulSet with two persistent volumes (1x RWO, 1x RWO) The DAM Persistence consists of a RW leader and a RO follower, both having their own persistent volume. Replication is maintained by application logic (PostgreSQL). Since our current implementation of PostgreSQL has no real HA or loadbalancing, we currently use an operator to automatically switch over to RO if the RW Pod dies. This dependency on an operator must be removed for multiple reasons: Switchover from RW to RO is not zero-downtime No load balancing between multiple PostgreSQL Pods No scaling for load surges, we are basically limited to vertical scaling (e.g. allow for much CPU and Memory on a single DB instance) Operator switchover adds complexity to the deployment Operator switchover forms a possibly single point of failure The solution to that is to have a properly scalable Persistence Layer, e.g. HA PostgreSQL. With such a setup, the manually build switchover can be removed and the necessity for a DAM operator vanishes.","title":"DAM Persistence"},{"location":"kube/k8s-next-operation/#image-processor","text":"Kubernetes Type: Deployment/ReplicaSet - stateless The image processor is a very simple API based computing application that performs image manipulation. The Pod is completely stateless, therefore scaling, initialization and running requires no application specific specialties.","title":"Image Processor"},{"location":"kube/k8s-next-operation/#conclusion","text":"Our current implementation of operations using operators is mainly based on certain issues with our applications themselves. Instead of building solutions to handle the symptoms, we should focus on fixing them to make such constructs obsolete and reduce complexity. The goal in a containerized deployment should be to make applications ready for it and not force applications not made for it into a specific deployment structure. Side note: Having operators carrying much logic also has implications for customers that may not want to use Kubernetes, but rather just docker because they are just trying out things or want to quickly spin up development environments. Since operators rely on Kubernetes for their actions, all functionality they provide will not be directly available outside Kubernetes. Having our container images taking more responsibility for themselves allows customers and developers to get a faster start with DX.","title":"Conclusion"},{"location":"kube/k8s-next-operation/#monitoring-metrics","text":"We need to enable monitoring capabilities for our customers. This topic can be split into two parts: Health/Monitoring data exposure Data Aggregation and Visualization It is worth having a look at the current implementation of the SoFy team, which at least provides monitoring out of the box for Kubernetes seemingly using Prometheus and Grafana.","title":"Monitoring / Metrics"},{"location":"kube/k8s-next-operation/#healthmonitoring-data-exposure","text":"All our application Pods should expose a metrics endpoint that can be scraped by a metrics data aggregation tooling. The endpoint should ideally be available at the same port for all Pods, thus making the configuration easier and more straight forward. The format should orient on what is best practice, e.g. the Prometheus format.","title":"Health/Monitoring data exposure"},{"location":"kube/k8s-next-operation/#kubernetes-resource-data","text":"Kubernetes itself provides metrics using the Kube state metrics . This contains metrics regarding Kubernetes resources like nodes and Pods. The data provided by the metrics are in a Prometheus consumable format.","title":"Kubernetes Resource Data"},{"location":"kube/k8s-next-operation/#java-application-data","text":"For Java we can leverage the JMX Exporter . It provides metrics about the state of a JVM in a Prometheus consumable format.","title":"Java Application Data"},{"location":"kube/k8s-next-operation/#nodejs-application-data","text":"For NodeJS we can use the Prom-Client . It is a npm package that provides various metrics regarding the NodeJS eventloop, heap usage etc. It also allows to configure custom metrics that can be exposed. The data format is Prometheus consumable.","title":"NodeJS Application Data"},{"location":"kube/k8s-next-operation/#db-postgresql-data","text":"For our current persistence stack we can use PostgreSQL Exporter . It exposes various DB related metrics and stats directly coming from the database itself. The output format is Prometheus consumable.","title":"DB / PostgreSQL Data"},{"location":"kube/k8s-next-operation/#data-aggregation","text":"All metrics data exposure is worthless if we don't gather that data somewhere central. For that purpose applications like Prometheus should be used. Prometheus is a scraping + persistence engine that will automatically scrape configured endpoints for exported metrics in a defined interval. It allows for specific querying to view data. There are already existing prometheus helm charts that could be leveraged as a dependency or inspiration in our helm charts. For visualization Grafana may present a good choice. It is widely adopted and provides out of the box compatibility with Prometheus. It also comes with a default set of helm charts .","title":"Data Aggregation"},{"location":"kube/k8s-next-operation/#logging","text":"It is important that our customers are able to easily access logs during runtime. This is necessary for them to configure e.g. notifications based on certain log messages, as well as for debugging issues.","title":"Logging"},{"location":"kube/k8s-next-operation/#using-sidecar-containers-for-log-exposure","text":"The Kubernetes stack already provides a logging mechanism which exposes logs of containers / Pods. The logs maintained here are based of the stdout and stderr streams of the containers and their respective running application. This principle has a disadvantage: Whenever your application uses something different for logging, e.g. typically files, you may only be able to tail the output of one file during runtime, thus only expose one log file in the Pod. Kubernetes provides the concept of sidecar containers for exactly that case. Each sidecar container can expose a file as a log stream inside Kubernetes, allowing Kubernetes to log multiple log files of a single application. Since sidecar containers share the same volumes as the run containers, they can access and tail the log files, allowing for an easy implementation.","title":"Using Sidecar Containers for log exposure"},{"location":"kube/k8s-next-operation/#log-output-gathering","text":"Since Kubernetes already gathers all log output of Pods and their containers, the usual approach is to gather the logs via Kubernetes itself. There are two often used ways to do logging in Kubernetes: EKF / ELK (Elasticsearch for data aggregation, Kibana for visualization and Fluentd for log data collection) Loki + Grafana (Loki for aggregation, Grafana for visualization) The basic concept behind both is to take the logs of each cluster Node (which include the logs of the Pods running on that cluster), aggregate them and them add a visual interface to access them for monitoring, search and debugging.","title":"Log output gathering"},{"location":"kube/k8s-next-operation/#changing-configurations","text":"","title":"Changing configurations"},{"location":"kube/k8s-next-operation/#adjusting-resource-allocations","text":"Changing resource allocations like CPU/Memory requests or limits would be applied directly on the definition of Deployments/ReplicaSets. When the values have changed, Kubernetes will internally trigger its own reconciliation and roll out the update to the resources.","title":"Adjusting resource allocations"},{"location":"kube/k8s-next-operation/#adjusting-enableddisabling-applications","text":"If a customer wants to enable/disable particular parts of the deployment, this should be done via the Helm upgrade process. Since we deploy the necessary Resources incl. Routes, StatefulSets etc. directly during deployment and not via an operator, enabling the application in the values.yaml of helm would then create or destroy the resources based on the enablement condition. It would also adjust the configuration of the DX Core Pods, thus leading to the leader of those Pods to configure the application, e.g. DAM.","title":"Adjusting enabled/disabling applications"},{"location":"kube/k8s-next-operation/#change-application-configuration","text":"Since we would like to store our configuration in configMaps, we'd like to know when they have changed and rollout the updated configuration. Therefore we need a mechanism to trigger the rollout of that update via StatefulSets/Deployments. An easy way to do so, is to add a checksum annotation to the Pods of our StatefulSets/Deployments for each configMap they care about. As soon as a configMap is changed, the operator will update the checksum of the affected resources and Kubernetes will start to roll out updated Pods or restart them, performing a rolling update.","title":"Change application configuration"},{"location":"kube/k8s-next-operation/#change-validation","text":"Our configuration lives within configMaps and Kubernetes resources. Those are created by Helm or with DXCTL using Helm created yaml files. There are multiple ways to validate changes to the resources we are managing:","title":"Change validation"},{"location":"kube/k8s-next-operation/#using-helm","text":"Helm supports the usage of schema files for its values.yaml . Since all values we use in our deployment are derived from that file during install or upgrade, we can leverage that check.","title":"Using Helm"},{"location":"kube/k8s-next-operation/#using-kubernetes-validation-webhooks","text":"Kubernetes provides the principle of validation webhooks . In combination with an operator, that provides validation endpoints, we would be able to reject wrong/undesired changes to configMaps directly in Kubernetes during runtime. We can also provide fitting error messages that will the customers why the changes they planned to do will not work. This would not only be applicable to configMaps, but also to other Kubernetes Resources e.g. our Deployments or StatefulSets.","title":"Using Kubernetes validation webhooks"},{"location":"kube/k8s-next-operation/#application-lifecycle","text":"","title":"Application lifecycle"},{"location":"kube/k8s-next-operation/#initial-startup","text":"During the initial startup we need to ensure that DX Core will only run the initialization once and only in one running Pod. Since the wp_profile is shared between all Pods, the initialized profile will be available to all Pods after initialization. The initialization flow could look the following way: The desired set of DX Core Pods is created by the StatefulSet The Pods negotiate a leader Pod, all others will act as followers. The leading Pod will start the initialization process, all other Pods will not start DX Core until the initialization has been completed After the initialization has been performed by the leader, all Pods (incl. followers) will start their DX Core process The application is initialized and ready to serve This requires communication between the DX Core Pods, e.g. on the shared volume. The current state of initialization could be persistent in a lock/timestamp file inside wp_profile. Other applications like DAM for example do already have initialization logic that prevents multiple Pods from colliding.","title":"Initial startup"},{"location":"kube/k8s-next-operation/#scaling","text":"We can use HorizontalPodAutoscalers that are included in Kubernetes to automatically scale services up and down based on CPU and memory usage. In case we want to leverage custom metrics, e.g. DAM operations pressure, there are also ways to establish custom metrics in Kubernetes and use them in autoScalers.","title":"Scaling"},{"location":"kube/k8s-next-operation/#backup-restore","text":"Backup and Restore tasks could be run as Jobs, which are one-off Pods in Kubernetes that fulfill a specific task and return a completion status. Those jobs could be created by the operator, since they need to be created during the deployments lifecycle. Creation of those backup tasks could either be triggered manually, or run automatically in defined intervals. Backups could include copying specific files to backup volumes, dumping deployment information and state or even trigger functions like EXIM in DAM and put the output to a desired spot. Restoring of before used backups should also be managed by the operator, as it would be able to bring the applications into their desired state before re-importing the data into them. It could also do validate if a restore was successful, e.g. by checking specific health-checks of individual applications.","title":"Backup &amp; Restore"},{"location":"kube/k8s-next-operation/#used-kubernetes-structures","text":"","title":"Used Kubernetes structures"},{"location":"kube/k8s-next-operation/#operatorscontrollers","text":"There are certain actions inside our deployment that would require manual intervention to get to the desired goal. The basic principle of watching a type of resource and acting accordingly to changes is considered a controller in the Kubernetes ecosystem. An operator on the other hand can consist of multiple controllers for different resources and implements applications specific knowledge/actions. A basic example would be a reaction on configuration changes. When we change the configMap of one our applications, we want to make that application aware of the introduced changes. This is true for stateful and stateless applications. A simple, but yet effective way to force the rollout of those changes is to have a annotation at the Pods level, which just contains a checksum of the applications config map. If that checksum changes, the application will automatically cycled through, performing a rollout of the new configuration to all Pods. Helm could provide the checksum during execution. If the value changes happen in Helm and the customer uses Helm upgrade to apply those changes, we could easily update the annotations during that automatically and the changes will be applied. Since that builds up a complexity in the Helm charts and e.g. would not work if customers use the plain yaml output with DXCTL, unless we port that functionality to DXCTL. If the customer applies changes directly in Kubernetes on configMap level, we would not get any automated rollout. To cover all different aspects of changes to configuration, having an operator that watches configurations and modifies our Pod definitions accordingly, would ease the process for customers a lot and reduce implementation overhead in both Helm and DXCTL. Other cases where the operator could act with application specific logic:","title":"Operators/Controllers"},{"location":"kube/k8s-next-operation/#operatorcontroller-use-cases","text":"","title":"Operator/Controller use cases"},{"location":"kube/k8s-next-operation/#leading-pod-only-routes","text":"There may be the necessity to have routes that should only go to the leading DX Core Pod. Certain actions, like config changes should only happen in one of our DX Core Pods. The operator could use a leader lookup routine to configure certain routes to point to exactly that Pod, instead of pointing to a Service consisting of multiple Pods. This would require the operator to check who is the leading DX Core Pod in a regular fashion and update the Endpoint for the corresponding route. This could be a repeatable pattern, where we could have multiple pre defined Endpoints that share a common label, allowing the operator to apply the change easily to multiple routes at the same time.","title":"Leading Pod only Routes"},{"location":"kube/k8s-next-operation/#automated-configmap-application","text":"As mentioned in the introduction before, configuration changes on ConfigMap level do not induce a automatic rollout on dependent resources in Kubernetes. This logic needs to be established by us. The operator could watch all relevant ConfigMaps and adjust the checksum annotation of all dependent resources, e.g. StatefulSets and Deployments. The built in Kubernetes reconciliation would then automatically roll out those changes.","title":"Automated ConfigMap application"},{"location":"kube/k8s-next-operation/#dam-persistence-ro-fallback","text":"As long as we do not have a true loadbalanced HA persistence solution for DAM, we will rely on a ReadWrite Primary and ReadOnly Secondary PostgreSQL instance, which do automatically replicate. In order to have a fallback in the case of an outage of the primary DB, the operator would change the routing to the DB for DAM automatically to the secondary. This would be restored to use the primary as soon as the primary DB is up and ready again.","title":"DAM persistence RO fallback"},{"location":"kube/k8s-next-operation/#dx-core-configuration-proxy","text":"If we are running multiple instances of DX Core, we want customers to execute configuration changes only on the leading Pod. The operator could perform a leader lookup and perform as a proxy for such executions. Instead of directly executing kubectl exec commands in one of the DX Core Pods, they would be pointed to the operator Pod and could either be executed through a direct SSH tunnel to the Core Pod or initiating the Kubernetes exec command on the DX Core Pod. Customers would not need to know which of the Cores is the current leading one.","title":"DX Core configuration Proxy"},{"location":"kube/k8s-next-operation/#leading-pod-identification","text":"For configuration and debugging purpose it may be useful to know, which DX Core Pod is currently the leading one. A simple command that can be triggered in the operator Pod that return the Pod Name could already be sufficient. For other use cases the operator would already need to know which Pod is leading.","title":"Leading Pod identification"},{"location":"kube/k8s-next-operation/#configuration-validation","text":"The operator could provide a validation endpoint that can be used to validate the changes to Resources during runtime. This could include type checks and even specific value verification.","title":"Configuration validation"},{"location":"kube/k8s-next-operation/#dx-deployment-specific-metrics","text":"We can use the operator to expose metrics endpoints that aggregate DX deployment relevant health and status information. Since the operator can possibly know all members of the deployment in its namespace, it can act as a point of entry for quick health checks.","title":"DX Deployment Specific Metrics"},{"location":"kube/k8s-next-operation/#custom-resources-definition-vs-configmaps","text":"A classical operator design usually incorporates the use of a Custom Resource Definition or short CRD. A CRD is an extension to the Kubernetes API and basically allows for creation of custom resource definitions that reflect whatever is necessary. In our case, we used a custom resource (CR) called dxDeployment in the past. What content can be inside that CR is defined by the CRD. You could think of the CRD as a schema or object definition, from which you can create Objects inside Kubernetes. This definition can also contain validation of values. The operator would use this CR as its configuration backbone. ConfigMaps on the other hand are vanilla Kubernetes resources that contain a list of key-value-pairs. The values of those pairs could either be a single value or even be a whole file structure. Still in the end the principle is always key-value-pair. This is a simple data structure that allows for easy editing, but not so much flexibility in regards of structure. ConfigMaps, unlike CRs, can be directly consumed in some Kubernetes resources. Pods can either mount configMaps as a volume inside the Containers or extract values from them and set them as environment variables inside Containers. There is no necessary conversion logic that needs to be implemented. If we perform changes to our configuration, e.g. add a field, we would require the CRD to be updated prior to updating the CR. Also our operator would need the be update, even if it would only pass the configuration field through to another Kubernetes resource. Example: We have a CRD that defines certain parts of Pod specification. Unfortunately, we did not incorporate labels in the past. By our implementation logic, we take add the field to the CRD, then enhance the operator to be able to consume that field and pass it on through the Pod definition e.g. inside a StatefulSet. This apparently small change to just to enable a field for a Pod specification requires changes on CRD definition, changes to operator code, rolling out a new CRD and operator. Alternatively the label could just have directly been added to the StatefulSet in the first place, without being passed through an operator. Also, there has been a lot of discussion the the Helm community regarding the lifecycle of CRDs. Therefore Helm does currently not support updating CRDs , which will make the use of CRDs with SoFy harder for us and our customers, since SoFy relies on Helm. Because CRDs define a content/config structure and are cluster wide resources, special attention has to go into version updates of CRDs, updating them and testing use/edge cases like having two versions of a CRD in the cluster because two different versions of DX are deployed. (E.g. CF191 and CF193 on SoFy sandbox clusters.) Since an operator could also watch ConfigMaps as a source of configuration, we would like to use those.","title":"Custom Resources Definition vs. configMaps"},{"location":"kube/k8s-next-operation/#glossary","text":"DMGR: Deployment Manager from WAS ND, maintains deployment cells containing multiple WAS nodes. WAS ND: Websphere Application Server Network Deployment, Java application server used by DX Core. Cell: In WAS Terms consists of a DMGR and multiple nodes. RWO: ReadWriteOnce Volume - can only be accessed by one Pod, one to one relationship between volume and Pod. RWM: ReadWriteMany Volume - can be accessed by multiple Pods, one to many relationship between volume and Pods.","title":"Glossary"},{"location":"kube/k8s-next-runtime-log-levels-dbha/","text":"Postgres The log levels for Postgres are defined by the log_min_messages parameter . Supported values are: DEBUG5 , DEBUG4 , DEBUG3 , DEBUG2 , DEBUG1 , INFO , NOTICE , WARNING , ERROR , LOG , FATAL , PANIC Log levels for Postgres can be adjusted in different ways. Update postgresql.conf In the current build of the DBHA Postgres image, the postgresql.conf is located in /var/lib/pgsql/11/data/dx/ . It includes the log_min_messages which is commented out by default. After it was changed, the configuration can be reloaded without restarting Postgres with repmgr node service --action=reload Update in database Another way of configuring the log level is to alter the entry in the database directly. For that we can connect to the database inside the container using psql . To change and apply the level change, we use ALTER SYSTEM SET log_min_messages = 'DEBUG5'; SELECT pg_reload_conf(); Repmgr The log levels for the Repmgr are defined by the log_level parameter . Supported values are: DEBUG , INFO , NOTICE , WARNING , ERROR , ALERT , CRIT , EMERG Log levels for Postgres can be adjusted the following way. Update repmgr.conf In the current build of the DBHA Postgres image, the repmgr.conf is located in /etc/repmgr/11 . It includes the log_level which is commented out by default. After it was changed, the configuration can be reloaded without restarting Repmgr with kill -SIGHUP `cat /tmp/repmgrd.pid` Additional changes In order to make those changes applicable, we need to make some slight changes to the start_postgres.sh script. We need to remove the following lines to enable logging to stdout : __repmgr_set_property \"log_file\" \"${PG_CONFDIR}/repmgr/log/repmgr.log\" \"${REPMGR_CONF_DIR}/repmgr.conf\" __repmgr_set_property \"log_level\" \"NOTICE\" \"${REPMGR_CONF_DIR}/repmgr.conf\" Periodically check for global log config and translate to compatible log config We can create a shell script that runs in the image and periodically checks for changes in the global log config that is mounted in the container at /etc/global-config/log.persistenceNode . Example log string: pers-db:::info,pers-repmgr:::info #!/bin/bash # #################################################################### # Licensed Materials - Property of HCL # # # # Copyright HCL Technologies Ltd. 2021. All Rights Reserved. # # # # Note to US Government Users Restricted Rights: # # # # Use, duplication or disclosure restricted by GSA ADP Schedule # #################################################################### # # Load libraries . /set_repmgr_property.sh # Define service name PG_SERVICE=\"pers-db\" REPMGR_SERVICE=\"pers-repmgr\" REPMGR_CONF_DIR='/etc/repmgr/11' # Define log level mapping for Postgres declare -A PG_MAP PG_MAP['debug']='DEBUG5' # logs DEBUG5, DEBUG4, DEBUG3, DEBUG2, DEBUG1 PG_MAP['info']='INFO' # logs INFO, NOTICE, WARNING PG_MAP['error']='ERROR' # logs ERROR, LOG, FATAL, PANIC # Define log level mapping for Repmgr declare -A REPMGR_MAP REPMGR_MAP['debug']='DEBUG' # logs DEBUG REPMGR_MAP['info']='INFO' # logs INFO, NOTICE, WARNING REPMGR_MAP['error']='ERROR' # logs ERROR, ALERT, CRIT, EMERG Map_Level() { # Transform to lowercase INPUT_LEVEL=$(echo \"$1\" | tr '[:upper:]' '[:lower:]') SERVICE=$2 # Returns mapped value if it exists, oterwise empty string if [[ $SERVICE == $PG_SERVICE ]]; then echo ${PG_MAP[$INPUT_LEVEL]} elif [[ $SERVICE == $REPMGR_SERVICE ]]; then echo ${REPMGR_MAP[$INPUT_LEVEL]} else echo \"\" fi } Parse_And_Set_Level() { # Split by comma IFS=',' read -ra LOG_STR <<< $1 for LOG_PART in \"${LOG_STR[@]}\"; do # Split by colon IFS=':' read -ra LOG_STR_PARTS <<< $LOG_PART # Get common log pattern elements SERVICE=${LOG_STR_PARTS[0]} # Transform to lowercase LOWERCASE_SERVICE=$(echo \"$SERVICE\" | tr '[:upper:]' '[:lower:]') # Suffix and component are currently not used in this service # SUFFIX=${LOG_STR_PARTS[1]} # COMPONENT=${LOG_STR_PARTS[2]} LEVEL=${LOG_STR_PARTS[${#LOG_STR_PARTS[@]} - 1]} # Only apply logs for the matching service if [[ $LOWERCASE_SERVICE == $PG_SERVICE ]]; then MAPPED_LEVEL=$(Map_Level \"$LEVEL\" $PG_SERVICE) if [[ $MAPPED_LEVEL != \"\" ]]; then # Setting the log level echo \"Setting Postgres log level to $MAPPED_LEVEL\" && \\ psql -c \"ALTER SYSTEM SET log_min_messages = '$MAPPED_LEVEL';\" && \\ psql -c \"SELECT pg_reload_conf();\" || \\ echo \"Failed to set Postgres log level to $MAPPED_LEVEL\" fi elif [[ $LOWERCASE_SERVICE == $REPMGR_SERVICE ]]; then MAPPED_LEVEL=$(Map_Level \"$LEVEL\" $REPMGR_SERVICE) if [[ $MAPPED_LEVEL != \"\" ]]; then # Setting the log level echo \"Setting Repmgr log level to $MAPPED_LEVEL\" && \\ __repmgr_set_property \"log_level\" \"${MAPPED_LEVEL}\" \"${REPMGR_CONF_DIR}/repmgr.conf\" && \\ kill -HUP `cat /tmp/repmgrd.pid` || \\ echo \"Failed to set Repmgr log level to $MAPPED_LEVEL\" fi fi done } Periodically_Read_Ronfigfile() { FILE_PATH=$1 # Sleep time defaults to 10 seconds if not passed as argument SLEEP_TIME_SECONDS=${2:-10} LAST_CONFIG_STRING='' while [ true ]; do if [[ -f ${FILE_PATH} || -L ${FILE_PATH} ]]; then # Deal with the fact that the file referenced could actually be a symlink to a file FULLY_RESOLVED_FILE_PATH=$(readlink -f ${FILE_PATH}) # read exactly the first line from that file read -r first_line < ${FULLY_RESOLVED_FILE_PATH} if [ \"${LAST_CONFIG_STRING}\" != \"${first_line}\" ]; then LAST_CONFIG_STRING=${first_line} Parse_And_Set_Level \"$first_line\" else echo \"Nothing done because log config didn't change in ${FULLY_RESOLVED_FILE_PATH} in last\" ${SLEEP_TIME_SECONDS} \"seconds\" fi else echo \"Config map ${FILE_PATH} doesn't exist. No action taken\" fi sleep ${SLEEP_TIME_SECONDS} done } # Start periodic file check Periodically_Read_Ronfigfile \"/etc/global-config/log.persistenceNode\" 10 Pgpool The log levels for Pgpool are defined by the log_min_messages parameter . Supported values are: DEBUG5 , DEBUG4 , DEBUG3 , DEBUG2 , DEBUG1 , INFO , NOTICE , WARNING , ERROR , LOG , FATAL , PANIC Log levels for Pgpool can be adjusted in different ways. Update pgpool.conf In the current build of the DBHA Pgpool image, the pgpool.conf is located in /conf . It includes the log_min_messages which is commented out by default. After it was changed, the configuration can be reloaded without restarting Postgres with pgpool --config-file=/conf/pgpool.conf --hba-file=/conf/pool_hba.conf reload We are currently using Pgpool 4.1. With 4.2, a new command pcp_reload_config was introduced . If we upgrade to 4.2 in the future, this can probably replace the command above. Update in database (Do not use for our case) Another way of configuring the log level is to alter the entry in the database directly. For that we can connect to the database inside the container using Important Note: Do not use this for our use case! This can in some cases work unreliably if it is executed during failover or the setting is not present in all nodes. The config file is better to store the setting in a persistent way. psql postgres://<PGPOOL_SR_CHECK_USER>:<PGPOOL_SR_CHECK_PASSWORD>@localhost/postgres Example: psql postgres://repdxuser:d1gitalRepExperience@localhost/postgres To change and apply the level change, we use PGPOOL SET log_min_messages = 'DEBUG5'; Periodically check for global log config and translate to compatible log config We can create a shell script that runs in the image and periodically checks for changes in the global log config that is mounted in the container at /etc/global-config/log.persistenceConnectionPool . Example log string: pers-pool:::info #!/bin/bash # #################################################################### # Licensed Materials - Property of HCL # # # # Copyright HCL Technologies Ltd. 2021. All Rights Reserved. # # # # Note to US Government Users Restricted Rights: # # # # Use, duplication or disclosure restricted by GSA ADP Schedule # #################################################################### # # Load libraries . /scripts/libpgpool.sh # Load Pgpool env. variables eval \"$(pgpool_env)\" # Define service name CURRENT_SERVICE=\"pers-pool\" # Define log level mapping declare -A map map['debug']='DEBUG5' # logs DEBUG5, DEBUG4, DEBUG3, DEBUG2, DEBUG1 map['info']='INFO' # logs INFO, NOTICE, WARNING map['error']='ERROR' # logs ERROR, LOG, FATAL, PANIC Map_Level() { # Transform to lowercase INPUT_LEVEL=$(echo \"$1\" | tr '[:upper:]' '[:lower:]') # Returns mapped value if it exists, oterwise empty string echo ${map[$INPUT_LEVEL]} } Parse_And_Set_Level() { # Ex: \"pool:::info\" # Split by comma IFS=',' read -ra LOG_STR <<< $1 for LOG_PART in \"${LOG_STR[@]}\"; do # Split by colon IFS=':' read -ra LOG_STR_PARTS <<< $LOG_PART # Get common log pattern elements SERVICE=${LOG_STR_PARTS[0]} # Transform to lowercase LOWERCASE_SERVICE=$(echo \"$SERVICE\" | tr '[:upper:]' '[:lower:]') # Suffix and component are currently not used in this service # SUFFIX=${LOG_STR_PARTS[1]} # COMPONENT=${LOG_STR_PARTS[2]} LEVEL=${LOG_STR_PARTS[${#LOG_STR_PARTS[@]} - 1]} # Only apply logs for the matching service if [[ $LOWERCASE_SERVICE == $CURRENT_SERVICE ]]; then MAPPED_LEVEL=$(Map_Level \"$LEVEL\") if [[ $MAPPED_LEVEL != \"\" ]]; then # Setting the log level echo \"Setting log level to $MAPPED_LEVEL\" && \\ pgpool_set_property \"log_min_messages\" $MAPPED_LEVEL && \\ pgpool --config-file=$PGPOOL_CONF_FILE --hba-file=$PGPOOL_PGHBA_FILE reload || \\ echo \"Failed to set log level to $MAPPED_LEVEL\" fi fi done } Periodically_Read_Ronfigfile() { FILE_PATH=$1 # Sleep time defaults to 10 seconds if not passed as argument SLEEP_TIME_SECONDS=${2:-10} LAST_CONFIG_STRING='' while [ true ]; do if [[ -f ${FILE_PATH} || -L ${FILE_PATH} ]]; then # Deal with the fact that the file referenced could actually be a symlink to a file FULLY_RESOLVED_FILE_PATH=$(readlink -f ${FILE_PATH}) # read exactly the first line from that file read -r first_line < ${FULLY_RESOLVED_FILE_PATH} if [ \"${LAST_CONFIG_STRING}\" != \"${first_line}\" ]; then LAST_CONFIG_STRING=${first_line} Parse_And_Set_Level \"$first_line\" else echo \"Nothing done because log config didn't change in ${FULLY_RESOLVED_FILE_PATH} in last\" ${SLEEP_TIME_SECONDS} \"seconds\" fi else echo \"Config map ${FILE_PATH} doesn't exist. No action taken\" fi sleep ${SLEEP_TIME_SECONDS} done } # Start periodic file check Periodically_Read_Ronfigfile \"/etc/global-config/log.persistenceConnectionPool\" 10 It needs to be called from the entrypoint of the image and run permanently in the background.","title":"Runtime log levels - DBHA"},{"location":"kube/k8s-next-runtime-log-levels-dbha/#postgres","text":"The log levels for Postgres are defined by the log_min_messages parameter . Supported values are: DEBUG5 , DEBUG4 , DEBUG3 , DEBUG2 , DEBUG1 , INFO , NOTICE , WARNING , ERROR , LOG , FATAL , PANIC Log levels for Postgres can be adjusted in different ways.","title":"Postgres"},{"location":"kube/k8s-next-runtime-log-levels-dbha/#update-postgresqlconf","text":"In the current build of the DBHA Postgres image, the postgresql.conf is located in /var/lib/pgsql/11/data/dx/ . It includes the log_min_messages which is commented out by default. After it was changed, the configuration can be reloaded without restarting Postgres with repmgr node service --action=reload","title":"Update postgresql.conf"},{"location":"kube/k8s-next-runtime-log-levels-dbha/#update-in-database","text":"Another way of configuring the log level is to alter the entry in the database directly. For that we can connect to the database inside the container using psql . To change and apply the level change, we use ALTER SYSTEM SET log_min_messages = 'DEBUG5'; SELECT pg_reload_conf();","title":"Update in database"},{"location":"kube/k8s-next-runtime-log-levels-dbha/#repmgr","text":"The log levels for the Repmgr are defined by the log_level parameter . Supported values are: DEBUG , INFO , NOTICE , WARNING , ERROR , ALERT , CRIT , EMERG Log levels for Postgres can be adjusted the following way.","title":"Repmgr"},{"location":"kube/k8s-next-runtime-log-levels-dbha/#update-repmgrconf","text":"In the current build of the DBHA Postgres image, the repmgr.conf is located in /etc/repmgr/11 . It includes the log_level which is commented out by default. After it was changed, the configuration can be reloaded without restarting Repmgr with kill -SIGHUP `cat /tmp/repmgrd.pid`","title":"Update repmgr.conf"},{"location":"kube/k8s-next-runtime-log-levels-dbha/#additional-changes","text":"In order to make those changes applicable, we need to make some slight changes to the start_postgres.sh script. We need to remove the following lines to enable logging to stdout : __repmgr_set_property \"log_file\" \"${PG_CONFDIR}/repmgr/log/repmgr.log\" \"${REPMGR_CONF_DIR}/repmgr.conf\" __repmgr_set_property \"log_level\" \"NOTICE\" \"${REPMGR_CONF_DIR}/repmgr.conf\"","title":"Additional changes"},{"location":"kube/k8s-next-runtime-log-levels-dbha/#periodically-check-for-global-log-config-and-translate-to-compatible-log-config","text":"We can create a shell script that runs in the image and periodically checks for changes in the global log config that is mounted in the container at /etc/global-config/log.persistenceNode . Example log string: pers-db:::info,pers-repmgr:::info #!/bin/bash # #################################################################### # Licensed Materials - Property of HCL # # # # Copyright HCL Technologies Ltd. 2021. All Rights Reserved. # # # # Note to US Government Users Restricted Rights: # # # # Use, duplication or disclosure restricted by GSA ADP Schedule # #################################################################### # # Load libraries . /set_repmgr_property.sh # Define service name PG_SERVICE=\"pers-db\" REPMGR_SERVICE=\"pers-repmgr\" REPMGR_CONF_DIR='/etc/repmgr/11' # Define log level mapping for Postgres declare -A PG_MAP PG_MAP['debug']='DEBUG5' # logs DEBUG5, DEBUG4, DEBUG3, DEBUG2, DEBUG1 PG_MAP['info']='INFO' # logs INFO, NOTICE, WARNING PG_MAP['error']='ERROR' # logs ERROR, LOG, FATAL, PANIC # Define log level mapping for Repmgr declare -A REPMGR_MAP REPMGR_MAP['debug']='DEBUG' # logs DEBUG REPMGR_MAP['info']='INFO' # logs INFO, NOTICE, WARNING REPMGR_MAP['error']='ERROR' # logs ERROR, ALERT, CRIT, EMERG Map_Level() { # Transform to lowercase INPUT_LEVEL=$(echo \"$1\" | tr '[:upper:]' '[:lower:]') SERVICE=$2 # Returns mapped value if it exists, oterwise empty string if [[ $SERVICE == $PG_SERVICE ]]; then echo ${PG_MAP[$INPUT_LEVEL]} elif [[ $SERVICE == $REPMGR_SERVICE ]]; then echo ${REPMGR_MAP[$INPUT_LEVEL]} else echo \"\" fi } Parse_And_Set_Level() { # Split by comma IFS=',' read -ra LOG_STR <<< $1 for LOG_PART in \"${LOG_STR[@]}\"; do # Split by colon IFS=':' read -ra LOG_STR_PARTS <<< $LOG_PART # Get common log pattern elements SERVICE=${LOG_STR_PARTS[0]} # Transform to lowercase LOWERCASE_SERVICE=$(echo \"$SERVICE\" | tr '[:upper:]' '[:lower:]') # Suffix and component are currently not used in this service # SUFFIX=${LOG_STR_PARTS[1]} # COMPONENT=${LOG_STR_PARTS[2]} LEVEL=${LOG_STR_PARTS[${#LOG_STR_PARTS[@]} - 1]} # Only apply logs for the matching service if [[ $LOWERCASE_SERVICE == $PG_SERVICE ]]; then MAPPED_LEVEL=$(Map_Level \"$LEVEL\" $PG_SERVICE) if [[ $MAPPED_LEVEL != \"\" ]]; then # Setting the log level echo \"Setting Postgres log level to $MAPPED_LEVEL\" && \\ psql -c \"ALTER SYSTEM SET log_min_messages = '$MAPPED_LEVEL';\" && \\ psql -c \"SELECT pg_reload_conf();\" || \\ echo \"Failed to set Postgres log level to $MAPPED_LEVEL\" fi elif [[ $LOWERCASE_SERVICE == $REPMGR_SERVICE ]]; then MAPPED_LEVEL=$(Map_Level \"$LEVEL\" $REPMGR_SERVICE) if [[ $MAPPED_LEVEL != \"\" ]]; then # Setting the log level echo \"Setting Repmgr log level to $MAPPED_LEVEL\" && \\ __repmgr_set_property \"log_level\" \"${MAPPED_LEVEL}\" \"${REPMGR_CONF_DIR}/repmgr.conf\" && \\ kill -HUP `cat /tmp/repmgrd.pid` || \\ echo \"Failed to set Repmgr log level to $MAPPED_LEVEL\" fi fi done } Periodically_Read_Ronfigfile() { FILE_PATH=$1 # Sleep time defaults to 10 seconds if not passed as argument SLEEP_TIME_SECONDS=${2:-10} LAST_CONFIG_STRING='' while [ true ]; do if [[ -f ${FILE_PATH} || -L ${FILE_PATH} ]]; then # Deal with the fact that the file referenced could actually be a symlink to a file FULLY_RESOLVED_FILE_PATH=$(readlink -f ${FILE_PATH}) # read exactly the first line from that file read -r first_line < ${FULLY_RESOLVED_FILE_PATH} if [ \"${LAST_CONFIG_STRING}\" != \"${first_line}\" ]; then LAST_CONFIG_STRING=${first_line} Parse_And_Set_Level \"$first_line\" else echo \"Nothing done because log config didn't change in ${FULLY_RESOLVED_FILE_PATH} in last\" ${SLEEP_TIME_SECONDS} \"seconds\" fi else echo \"Config map ${FILE_PATH} doesn't exist. No action taken\" fi sleep ${SLEEP_TIME_SECONDS} done } # Start periodic file check Periodically_Read_Ronfigfile \"/etc/global-config/log.persistenceNode\" 10","title":"Periodically check for global log config and translate to compatible log config"},{"location":"kube/k8s-next-runtime-log-levels-dbha/#pgpool","text":"The log levels for Pgpool are defined by the log_min_messages parameter . Supported values are: DEBUG5 , DEBUG4 , DEBUG3 , DEBUG2 , DEBUG1 , INFO , NOTICE , WARNING , ERROR , LOG , FATAL , PANIC Log levels for Pgpool can be adjusted in different ways.","title":"Pgpool"},{"location":"kube/k8s-next-runtime-log-levels-dbha/#update-pgpoolconf","text":"In the current build of the DBHA Pgpool image, the pgpool.conf is located in /conf . It includes the log_min_messages which is commented out by default. After it was changed, the configuration can be reloaded without restarting Postgres with pgpool --config-file=/conf/pgpool.conf --hba-file=/conf/pool_hba.conf reload We are currently using Pgpool 4.1. With 4.2, a new command pcp_reload_config was introduced . If we upgrade to 4.2 in the future, this can probably replace the command above.","title":"Update pgpool.conf"},{"location":"kube/k8s-next-runtime-log-levels-dbha/#update-in-database-do-not-use-for-our-case","text":"Another way of configuring the log level is to alter the entry in the database directly. For that we can connect to the database inside the container using Important Note: Do not use this for our use case! This can in some cases work unreliably if it is executed during failover or the setting is not present in all nodes. The config file is better to store the setting in a persistent way. psql postgres://<PGPOOL_SR_CHECK_USER>:<PGPOOL_SR_CHECK_PASSWORD>@localhost/postgres Example: psql postgres://repdxuser:d1gitalRepExperience@localhost/postgres To change and apply the level change, we use PGPOOL SET log_min_messages = 'DEBUG5';","title":"Update in database (Do not use for our case)"},{"location":"kube/k8s-next-runtime-log-levels-dbha/#periodically-check-for-global-log-config-and-translate-to-compatible-log-config_1","text":"We can create a shell script that runs in the image and periodically checks for changes in the global log config that is mounted in the container at /etc/global-config/log.persistenceConnectionPool . Example log string: pers-pool:::info #!/bin/bash # #################################################################### # Licensed Materials - Property of HCL # # # # Copyright HCL Technologies Ltd. 2021. All Rights Reserved. # # # # Note to US Government Users Restricted Rights: # # # # Use, duplication or disclosure restricted by GSA ADP Schedule # #################################################################### # # Load libraries . /scripts/libpgpool.sh # Load Pgpool env. variables eval \"$(pgpool_env)\" # Define service name CURRENT_SERVICE=\"pers-pool\" # Define log level mapping declare -A map map['debug']='DEBUG5' # logs DEBUG5, DEBUG4, DEBUG3, DEBUG2, DEBUG1 map['info']='INFO' # logs INFO, NOTICE, WARNING map['error']='ERROR' # logs ERROR, LOG, FATAL, PANIC Map_Level() { # Transform to lowercase INPUT_LEVEL=$(echo \"$1\" | tr '[:upper:]' '[:lower:]') # Returns mapped value if it exists, oterwise empty string echo ${map[$INPUT_LEVEL]} } Parse_And_Set_Level() { # Ex: \"pool:::info\" # Split by comma IFS=',' read -ra LOG_STR <<< $1 for LOG_PART in \"${LOG_STR[@]}\"; do # Split by colon IFS=':' read -ra LOG_STR_PARTS <<< $LOG_PART # Get common log pattern elements SERVICE=${LOG_STR_PARTS[0]} # Transform to lowercase LOWERCASE_SERVICE=$(echo \"$SERVICE\" | tr '[:upper:]' '[:lower:]') # Suffix and component are currently not used in this service # SUFFIX=${LOG_STR_PARTS[1]} # COMPONENT=${LOG_STR_PARTS[2]} LEVEL=${LOG_STR_PARTS[${#LOG_STR_PARTS[@]} - 1]} # Only apply logs for the matching service if [[ $LOWERCASE_SERVICE == $CURRENT_SERVICE ]]; then MAPPED_LEVEL=$(Map_Level \"$LEVEL\") if [[ $MAPPED_LEVEL != \"\" ]]; then # Setting the log level echo \"Setting log level to $MAPPED_LEVEL\" && \\ pgpool_set_property \"log_min_messages\" $MAPPED_LEVEL && \\ pgpool --config-file=$PGPOOL_CONF_FILE --hba-file=$PGPOOL_PGHBA_FILE reload || \\ echo \"Failed to set log level to $MAPPED_LEVEL\" fi fi done } Periodically_Read_Ronfigfile() { FILE_PATH=$1 # Sleep time defaults to 10 seconds if not passed as argument SLEEP_TIME_SECONDS=${2:-10} LAST_CONFIG_STRING='' while [ true ]; do if [[ -f ${FILE_PATH} || -L ${FILE_PATH} ]]; then # Deal with the fact that the file referenced could actually be a symlink to a file FULLY_RESOLVED_FILE_PATH=$(readlink -f ${FILE_PATH}) # read exactly the first line from that file read -r first_line < ${FULLY_RESOLVED_FILE_PATH} if [ \"${LAST_CONFIG_STRING}\" != \"${first_line}\" ]; then LAST_CONFIG_STRING=${first_line} Parse_And_Set_Level \"$first_line\" else echo \"Nothing done because log config didn't change in ${FULLY_RESOLVED_FILE_PATH} in last\" ${SLEEP_TIME_SECONDS} \"seconds\" fi else echo \"Config map ${FILE_PATH} doesn't exist. No action taken\" fi sleep ${SLEEP_TIME_SECONDS} done } # Start periodic file check Periodically_Read_Ronfigfile \"/etc/global-config/log.persistenceConnectionPool\" 10 It needs to be called from the entrypoint of the image and run permanently in the background.","title":"Periodically check for global log config and translate to compatible log config"},{"location":"kube/k8s-next-runtime-log-levels/","text":"Introduction For each application of our DX deployment, we should have a consistent way to set log settings during runtime. Also, the log settings should be stored in a central place. Due to various technologies, different log technologies are used, like: DEBUG - NodeJS applications LOG4J, JUL - Java applications The log configuration formats of the different log technologies differ and therefore we need a way to unify/align them. With this proposal we do not intend to adjust the log output format. Assumptions / Restriction It will only work for applications on a Kubernetes deployment. (DX Core in a hybrid deployment will be excluded) Supported applications: DAM, CC, Core, RingAPI, Design Studio, Image Processor, Runtime Controller, Remote Search, DAM Persistence Open LDAP logging can not be configured this way Only the server log settings will be covered, the UI part is excluded. (Short side note: The logging for the React applications embedded in core will be configured through the same trace settings as for the server.) Only available for a HELM-based deployments Solution proposal General approach A global Config Map can be used as a central place for all log settings. The application Config Maps is not a good approach. (It is not a central place and each change will recycle the pods) We should also introduce a common log configuration format to align the different log technology formats (like DEBUG, JUL, LOG4J). Each application should monitor its own log settings from the global Config Map. Each application should also provide a functionality to change log settings at runtime. Global Config Map and monitoring own log settings Here is an example of how the global Config Map can look. apiVersion: \"v1\" kind: \"ConfigMap\" metadata: name: \"{{ .Release.Name }}-global\" labels: app: \"{{ .Release.Name }}-global\" # Using general labels, see _deployment_helpers.tpl {{- include \"hcl-dx-deployment.labels\" . | nindent 4 }} data: \"log.core\": \"<core log information>\" \"log.digitalAssetManagement\": \"<dam log information>\" All log settings are available inside the pods as mounted config maps under /etc/global-config . The application-specific log settings in the mounted config map need to be monitored by the application to adjust its log levels during runtime. This will obviously also require a change to the stateful set / replica set Helm charts to mount the additional config map. Short example to arrange that. volumeMounts: - name: \"global-config\" mountPath: \"/etc/global-config\" volumes: - name: \"global-config\" configMap: name: \"{{ .Release.Name }}-global\" Change log settings during runtime Each application should be able to consume and execute the new log settings without a restart of the application itself. A change of a log setting should be directly applied to the application log components (Logger). And a transformation of the common log format to the application-specific log would be needed. That means that we will have to do some adjustments to the application itself. Common log settings format The different logging frameworks offer a variety of levels. These levels will not be unified. LOG4J (Java): all, trace, debug, info, warn, error, fatal, off JUL: (Java) finest, finer, fine, config, info, warning, severe DEBUG (NodeJS): debug, info, error The usage of wildcards should also be possible. Each section of a single application component must be separated by a unique delimiter, the colon should be a good delimiter. For a definition of multiple application components, we need also a delimiter, the comma should be a good option for that. Here is an example of the proposed common log pattern: <component>:<pattern>=<level>,<component>:<pattern>=<level> - component > api, worker, wp_profile, cw_profile, prs_profile, repmgr, pgpool, psql (No Wildcards) - pattern > com.hcl.App, com.hcl.util.Data - level > error, info, debug (Always includes higher levels) LOG4J example: logging framework specific log settings: com.hcl.App=info com.hcl.util.Data=finest - component > wp_profile - pattern > com.hcl.App, com.hcl.util.Data - level > info, finest Common log settings: wp_profile::com.hcl.App=info,wp_profile::com.hcl.util.Data=finest DEBUG example: logging framework specific log settings: INFO:api:server-v1:dist* INFO:worker:server-v1:dist* DEBUG:api:server-v1:dist:server* - component > api, worker - pattern > dist, dist:server - level > info, debug Common log message: api:server-v1:dist=info,worker:server-v1:dist=info,api:server-v1:dist:server=debug, Supported component names Each application consists of one or multiple components. Supported component names are: Application Supported component names Content Composer api Core wp_profile , cw_profile DAM persistence psql , repmgr , pgpool Design Studio api Digital Asset Management api , worker Image Processor api Remote Search prs_profile Ring API api Runtime Controller controller Necessary code changes to the currently existing implementation (end of October 2021) Core and Remote Search Adjust the component identifier to use the new component names. Check if the already implemented logic supports log level settings for cw_profile . Otherwise create a ticket to ensure the implementation. enchanted logger Rework the log output format to move the log level to the beginning of the string in caps Rework the log settings transformation to match the proposed format Adjust the applications to match the components Rename the applications in their package.json files to adjust the package name Logging 3rd party DEBUG packages Many other packages like express or loopback utilize the same logging framework called DEBUG . Unfortunately there is no way to change the log level for these packages during runtime. Therefore we need to provide instructions for our support team on how to set tracings for these packages by setting the DEBUG environment variable. Adjusting the DEBUG environment variable will change the pod spec which will lead to a restart of the pod. This implicitly resolves the inability to change these log settings during runtime. Support log levels We have decided that we are supporting only three log levels to align the log levels for all our applications. Only DX Core and Remote Search are excluded as they require a more flexible configuration. Which log levels are we supporting? error, info, debug All NodeJs applications are following this restriction and using only this three log level. For other application (like JAVA) a mapping is needed. Mapping the levels that can be set in the Helm values to the log levels of the services Service debug info error Postgres DEBUG5 , DEBUG4 , DEBUG3 , DEBUG2 , DEBUG1 INFO , NOTICE , WARNING ERROR , LOG , FATAL , PANIC Repmgr DEBUG INFO , NOTICE , WARNING ERROR , ALERT , CRIT , EMERG Pgpool DEBUG5 , DEBUG4 , DEBUG3 , DEBUG2 , DEBUG1 INFO , NOTICE , WARNING ERROR , LOG , FATAL , PANIC Runtime Controller FINEST , FINER , FINE , CONFIG INFO , WARNING SEVERE Content Composer debug info error Digital Asset Management debug info error Ring API debug info error Design Studio debug info error Image Processor debug info error Log output for NodeJS apps We move the log level to the front of a log message, thus making the output more similar to other applications e.g. the ones that are based on WAS. Sample log: INFO:api:server-v1:dist:server.js[12] hello world ERROR:api:server-v1:dist:server.js[12] hello world DEBUG:api:server-v1:dist:server.js[12] hello world","title":"Runtime log levels"},{"location":"kube/k8s-next-runtime-log-levels/#introduction","text":"For each application of our DX deployment, we should have a consistent way to set log settings during runtime. Also, the log settings should be stored in a central place. Due to various technologies, different log technologies are used, like: DEBUG - NodeJS applications LOG4J, JUL - Java applications The log configuration formats of the different log technologies differ and therefore we need a way to unify/align them. With this proposal we do not intend to adjust the log output format.","title":"Introduction"},{"location":"kube/k8s-next-runtime-log-levels/#assumptions-restriction","text":"It will only work for applications on a Kubernetes deployment. (DX Core in a hybrid deployment will be excluded) Supported applications: DAM, CC, Core, RingAPI, Design Studio, Image Processor, Runtime Controller, Remote Search, DAM Persistence Open LDAP logging can not be configured this way Only the server log settings will be covered, the UI part is excluded. (Short side note: The logging for the React applications embedded in core will be configured through the same trace settings as for the server.) Only available for a HELM-based deployments","title":"Assumptions / Restriction"},{"location":"kube/k8s-next-runtime-log-levels/#solution-proposal","text":"","title":"Solution proposal"},{"location":"kube/k8s-next-runtime-log-levels/#general-approach","text":"A global Config Map can be used as a central place for all log settings. The application Config Maps is not a good approach. (It is not a central place and each change will recycle the pods) We should also introduce a common log configuration format to align the different log technology formats (like DEBUG, JUL, LOG4J). Each application should monitor its own log settings from the global Config Map. Each application should also provide a functionality to change log settings at runtime.","title":"General approach"},{"location":"kube/k8s-next-runtime-log-levels/#global-config-map-and-monitoring-own-log-settings","text":"Here is an example of how the global Config Map can look. apiVersion: \"v1\" kind: \"ConfigMap\" metadata: name: \"{{ .Release.Name }}-global\" labels: app: \"{{ .Release.Name }}-global\" # Using general labels, see _deployment_helpers.tpl {{- include \"hcl-dx-deployment.labels\" . | nindent 4 }} data: \"log.core\": \"<core log information>\" \"log.digitalAssetManagement\": \"<dam log information>\" All log settings are available inside the pods as mounted config maps under /etc/global-config . The application-specific log settings in the mounted config map need to be monitored by the application to adjust its log levels during runtime. This will obviously also require a change to the stateful set / replica set Helm charts to mount the additional config map. Short example to arrange that. volumeMounts: - name: \"global-config\" mountPath: \"/etc/global-config\" volumes: - name: \"global-config\" configMap: name: \"{{ .Release.Name }}-global\"","title":"Global Config Map and monitoring own log settings"},{"location":"kube/k8s-next-runtime-log-levels/#change-log-settings-during-runtime","text":"Each application should be able to consume and execute the new log settings without a restart of the application itself. A change of a log setting should be directly applied to the application log components (Logger). And a transformation of the common log format to the application-specific log would be needed. That means that we will have to do some adjustments to the application itself.","title":"Change log settings during runtime"},{"location":"kube/k8s-next-runtime-log-levels/#common-log-settings-format","text":"The different logging frameworks offer a variety of levels. These levels will not be unified. LOG4J (Java): all, trace, debug, info, warn, error, fatal, off JUL: (Java) finest, finer, fine, config, info, warning, severe DEBUG (NodeJS): debug, info, error The usage of wildcards should also be possible. Each section of a single application component must be separated by a unique delimiter, the colon should be a good delimiter. For a definition of multiple application components, we need also a delimiter, the comma should be a good option for that. Here is an example of the proposed common log pattern: <component>:<pattern>=<level>,<component>:<pattern>=<level> - component > api, worker, wp_profile, cw_profile, prs_profile, repmgr, pgpool, psql (No Wildcards) - pattern > com.hcl.App, com.hcl.util.Data - level > error, info, debug (Always includes higher levels) LOG4J example: logging framework specific log settings: com.hcl.App=info com.hcl.util.Data=finest - component > wp_profile - pattern > com.hcl.App, com.hcl.util.Data - level > info, finest Common log settings: wp_profile::com.hcl.App=info,wp_profile::com.hcl.util.Data=finest DEBUG example: logging framework specific log settings: INFO:api:server-v1:dist* INFO:worker:server-v1:dist* DEBUG:api:server-v1:dist:server* - component > api, worker - pattern > dist, dist:server - level > info, debug Common log message: api:server-v1:dist=info,worker:server-v1:dist=info,api:server-v1:dist:server=debug,","title":"Common log settings format"},{"location":"kube/k8s-next-runtime-log-levels/#supported-component-names","text":"Each application consists of one or multiple components. Supported component names are: Application Supported component names Content Composer api Core wp_profile , cw_profile DAM persistence psql , repmgr , pgpool Design Studio api Digital Asset Management api , worker Image Processor api Remote Search prs_profile Ring API api Runtime Controller controller","title":"Supported component names"},{"location":"kube/k8s-next-runtime-log-levels/#necessary-code-changes-to-the-currently-existing-implementation-end-of-october-2021","text":"","title":"Necessary code changes to the currently existing implementation (end of October 2021)"},{"location":"kube/k8s-next-runtime-log-levels/#core-and-remote-search","text":"Adjust the component identifier to use the new component names. Check if the already implemented logic supports log level settings for cw_profile . Otherwise create a ticket to ensure the implementation.","title":"Core and Remote Search"},{"location":"kube/k8s-next-runtime-log-levels/#enchanted-logger","text":"Rework the log output format to move the log level to the beginning of the string in caps Rework the log settings transformation to match the proposed format Adjust the applications to match the components Rename the applications in their package.json files to adjust the package name","title":"enchanted logger"},{"location":"kube/k8s-next-runtime-log-levels/#logging-3rd-party-debug-packages","text":"Many other packages like express or loopback utilize the same logging framework called DEBUG . Unfortunately there is no way to change the log level for these packages during runtime. Therefore we need to provide instructions for our support team on how to set tracings for these packages by setting the DEBUG environment variable. Adjusting the DEBUG environment variable will change the pod spec which will lead to a restart of the pod. This implicitly resolves the inability to change these log settings during runtime.","title":"Logging 3rd party DEBUG packages"},{"location":"kube/k8s-next-runtime-log-levels/#support-log-levels","text":"We have decided that we are supporting only three log levels to align the log levels for all our applications. Only DX Core and Remote Search are excluded as they require a more flexible configuration. Which log levels are we supporting? error, info, debug All NodeJs applications are following this restriction and using only this three log level. For other application (like JAVA) a mapping is needed. Mapping the levels that can be set in the Helm values to the log levels of the services Service debug info error Postgres DEBUG5 , DEBUG4 , DEBUG3 , DEBUG2 , DEBUG1 INFO , NOTICE , WARNING ERROR , LOG , FATAL , PANIC Repmgr DEBUG INFO , NOTICE , WARNING ERROR , ALERT , CRIT , EMERG Pgpool DEBUG5 , DEBUG4 , DEBUG3 , DEBUG2 , DEBUG1 INFO , NOTICE , WARNING ERROR , LOG , FATAL , PANIC Runtime Controller FINEST , FINER , FINE , CONFIG INFO , WARNING SEVERE Content Composer debug info error Digital Asset Management debug info error Ring API debug info error Design Studio debug info error Image Processor debug info error","title":"Support log levels"},{"location":"kube/k8s-next-runtime-log-levels/#log-output-for-nodejs-apps","text":"We move the log level to the front of a log message, thus making the output more similar to other applications e.g. the ones that are based on WAS. Sample log: INFO:api:server-v1:dist:server.js[12] hello world ERROR:api:server-v1:dist:server.js[12] hello world DEBUG:api:server-v1:dist:server.js[12] hello world","title":"Log output for NodeJS apps"},{"location":"kube/k8s-next-undeploy/","text":"Status Date APPROVED 29th of April 2021 Introduction Undeploy unlike delete will not just kill and destroy an existing deployment no matter of the result. We want to provide an easy to handle, reliable and transparent Kubernetes undeployment. From the handling perspective this means that customers should have the ability to select what they want to keep (i.e. namespace information, app configurations, data) from the existing deployment. This document provides a proposal on how this undeployment could look like. Initial considerations To not interfere with running actions within the system (i.e. active client upload, active backup process) an undeployment should run in a fully controlled system. That means no external client interactions should be possible and no ongoing data nor configuration changes should be active. To drop external interactions manually we need to provide a documentation to the customer on how to do this or we can provide this as an automation before starting the bare undeployment. For backup and config job we need to provide an automated checkup before the undeployment. Minimum undeploy actions The minimal functionallity for a Kube undeployment is what we currently provide in our automation: Delete the namespace (or OpenShift project) which in turn deletes all scoped resoucres under it Delete actions not yet provided but maybe considered In addition to the delete actions we already provide we should add optional actions the customer can choose from. Delete CRDs Check for possible Kube V1 artifacts Save data and configurations In addition to just deleting the environment we need to provide the capabilty to save data, configurations, and/or deployment informations from the existing environment for import in a new deployment. This includes the following actions: Export current configuration Save persistent data Save exisitng backups Save existing logs Undeployment CLI and UI Besides the pure delete actions the undeployment needs to provide a UI and CLI. An UI would better visualize the user input and actions rather than just running single commands from command line but is not mandatory in the first place. The CLI/UI takes the appropriate input from the customer: List of parts always undeployed (no choice) List of undeployable parts the customer can select List of data, configurations, deployment information the customer can select to save them Dry-Run feature Final question if undeployment really should run Undeploy process After the customer has confirmed the undeploy process will start. The undeploy process needs to run seamless for the various Kube flavors. Therefor the process needs a configuration file or runtime parameter to provide default information (i.e. Kube flavor, userid/password, tools to run certain actions, etc.). Following is an illustration about the straight forward undeploy process. Customer addons In addition the final undeploy process should provide pre- and post-hooks (customer entry points) for certain actions so the customer can add his own tools and/or flavors.","title":"Undeploy"},{"location":"kube/k8s-next-undeploy/#introduction","text":"Undeploy unlike delete will not just kill and destroy an existing deployment no matter of the result. We want to provide an easy to handle, reliable and transparent Kubernetes undeployment. From the handling perspective this means that customers should have the ability to select what they want to keep (i.e. namespace information, app configurations, data) from the existing deployment. This document provides a proposal on how this undeployment could look like.","title":"Introduction"},{"location":"kube/k8s-next-undeploy/#initial-considerations","text":"To not interfere with running actions within the system (i.e. active client upload, active backup process) an undeployment should run in a fully controlled system. That means no external client interactions should be possible and no ongoing data nor configuration changes should be active. To drop external interactions manually we need to provide a documentation to the customer on how to do this or we can provide this as an automation before starting the bare undeployment. For backup and config job we need to provide an automated checkup before the undeployment.","title":"Initial considerations"},{"location":"kube/k8s-next-undeploy/#minimum-undeploy-actions","text":"The minimal functionallity for a Kube undeployment is what we currently provide in our automation: Delete the namespace (or OpenShift project) which in turn deletes all scoped resoucres under it","title":"Minimum undeploy actions"},{"location":"kube/k8s-next-undeploy/#delete-actions-not-yet-provided-but-maybe-considered","text":"In addition to the delete actions we already provide we should add optional actions the customer can choose from. Delete CRDs Check for possible Kube V1 artifacts","title":"Delete actions not yet provided but maybe considered"},{"location":"kube/k8s-next-undeploy/#save-data-and-configurations","text":"In addition to just deleting the environment we need to provide the capabilty to save data, configurations, and/or deployment informations from the existing environment for import in a new deployment. This includes the following actions: Export current configuration Save persistent data Save exisitng backups Save existing logs","title":"Save data and configurations"},{"location":"kube/k8s-next-undeploy/#undeployment-cli-and-ui","text":"Besides the pure delete actions the undeployment needs to provide a UI and CLI. An UI would better visualize the user input and actions rather than just running single commands from command line but is not mandatory in the first place. The CLI/UI takes the appropriate input from the customer: List of parts always undeployed (no choice) List of undeployable parts the customer can select List of data, configurations, deployment information the customer can select to save them Dry-Run feature Final question if undeployment really should run","title":"Undeployment CLI and UI"},{"location":"kube/k8s-next-undeploy/#undeploy-process","text":"After the customer has confirmed the undeploy process will start. The undeploy process needs to run seamless for the various Kube flavors. Therefor the process needs a configuration file or runtime parameter to provide default information (i.e. Kube flavor, userid/password, tools to run certain actions, etc.). Following is an illustration about the straight forward undeploy process.","title":"Undeploy process"},{"location":"kube/k8s-next-undeploy/#customer-addons","text":"In addition the final undeploy process should provide pre- and post-hooks (customer entry points) for certain actions so the customer can add his own tools and/or flavors.","title":"Customer addons"},{"location":"kube/k8s-next-update/","text":"Status Date APPROVED 22nd of April 2021 Introduction For our Kubernetes deployments we also need to provide the possibility for customers to do updates. Ideally for our customers there would be no downtime when they perform an update. The update process should also cover possibilities to perform rollbacks if necessary. Affected Resources This chapter lists all important resources in our deployments and their requirements regarding updates. Stateful Applications We currently have three stateful applications with distinct requirements: DX Core DX Core uses two persistent storages for its operation. One of them is a (multi-pod shared) persistent volume which contains the wp_profile. The second persistence is realized using a relational database. That database is not part of the current DX Kube deployment. Therefore, all interactions with that DB need to be done by the operation/administration team of the customer OR by DX Core directly. DAM DAM uses two persistent storages for its operation. One of them is a (multi-pod shared) persistent volume which contains the DAM upload data and acts as a storage for binaries. The second persistence is realized using a relational database in form of PostgreSQL. That database is deployed inside our DX Kube deployments and can therefore be accessed by us during updated directly. It would be possibly for either the operation/administration team of the customer, the DX operator or DAM itself to perform necessary actions on that DB. DAM Persistence (PostgreSQL) The DAM persistence layer is one of the two persistence parts of DAM. It currently consists of two distinct Pods, where one is a RW primary DB node and the other one is a RO secondary DB node. Both have their own persistent volumes that are not shared. The persistence layer container images contain no schemas or application data, since they will be initialized by DAM. Changes to schemas would also be done by DAM. From an update perspective the most important aspect is to keep the existing data storage of the Pods compatible with the new DB version that comes in a newer container. This basically relies on how PostgreSQL behaves between version updates and needs to be verified. Stateless Applications RingAPI RingAPI is a stateless application that only uses configuration parameters and acts as a wrapping API server around DX Core. From an update perspective there are no special requirements that need to be taken into account. Updating RingAPI basically boils down to deploying a new version of the container image. Content Composer Content Composer is a stateless application that only uses configuration parameters and acts as a webserver hosting the resources for the Content Composer UI. From an update perspective there are no special requirements that need to be taken into account. Updating Content Composer basically boils down to deploying a new version of the container image. Image Processor Content Composer is a stateless application that only uses configuration parameters and acts as a media transformation service. It only uses HTTP as communication protocol and requires no persistence. From an update perspective there are no special requirements that need to be taken into account. Updating the Image Processor basically boils down to deploying a new version of the container image. Configurations ConfigMaps For general configuration we use two types von ConfigMaps. The first one is an application specific ConfigMap which contains all configuration necessary for individual applications to run. Those ConfigMaps are only consumed by the related application. The second type of ConfigMap is a global deployment wide ConfigMap, that contains configuration that affects all applications. The values of these ConfigMap entries will be directly updated during the update process and applications will be informed about the changes. Secrets For storing secure data like certificates and access credentials, we use Kubernetes Secrets. These secrets will be consumed by our applications. Content of those secrets will be directly updated during the update process and applications will be informed about the changes. Utility Resources Utility Resources e.g. HorizontalPodAutoscalers will be updated directly through the update process. Ambassador deployment We need to replace Ambassador Resources if they have changes. This will happen directly through the update process. Updating Backup Before updating, we need to provide customers with a way to create a backup of the important parts of the deployment. This includes persistent volumes and the data in the DBs of DX Core and DAM. This could be done by: A Kubernetes Job running, triggered by Helm or DXCTL The applications themselves before updating the schemas Update Categories Updates we perform can have multiple implications. Based on the way they change existing deployments, we can categorize them in different update categories. Based on in which category an update fits in, different actions or handling might be necessary. A - Application changes As we implement features and bugfixes to our applications, the application runtime code will change. This can have implications to configuration and persistence schemas, but that is not always the case. Often enough, application logic will be altered, without affecting existing environmental resources. C - Configuration changes As we perform development on our applications, it might be necessary to either introduce new configuration parameters or alter existing ones to reflect the current state of implementation. It can also happen that default behaviors/values might be adjusted between releases, e.g. if performance assessments have shown, that a Pod needs fewer resources and we lower the Kubernetes resource requests. P - Persistence schema changes For our stateful applications, such as DAM or DX Core, it may become necessary to perform changes to our persistence schemas. This would usually affect the schema of the DAM DB or the DX Core DB. This can happen if we need to introduce new fields into existing data structures. N - New application introduction As we move on with our developments for DX, new applications will arise that will be deployed within a DX deployment. Those applications will require new configurations, Kubernetes resources and perhaps preparation. As an example the roll-out of site-manager would fall into this category. R - Removal/replacement of an application Similar to when we add applications, there may also be parts that will be removed because they perhaps got obsolete or are being replaced with other applications. In that case it is important to ensure that the old resources get cleaned up, the configuration, if necessary, gets removed and the DX deployment is made aware that the application is gone and is not having any dependency anymore. Scenario: Update only containing configuration and application changes During this update the customer can either use helm or dxctl to deploy the new version of our DX deployment. With that, all resources affected will receive their respective changes. Statefulsets Since we directly define the Statefulsets in our Helm chart, the update will cause them to receive the new specification to the kubernetes resource. Kubernetes itself will then start its reconciliation and will ensure that the Pods of the statefulset will be restarted with the new configuration. See Statefulset Rolling Update for reference. This logic is implemented in kubernetes itself and will roll out the changes to the application and will only continue updating the next Pod, if the first one has been restarted with the new configuration and got ready. Deployments/Replicasets Applications that are stateless and run as a Deployment will be receiving the updated specification as well directly via the update process. As soon as the Deployment Resource gets altered, Kubernetes will perform a rollover, spawning new Pods with the new configuration. As soon as the new Pods are running successfully and are ready, the old Pods will be terminated. See Deployment Rollover Update for reference. ConfigMaps/Secrets ConfigMaps and Secrets will directly be updated via the update process. Utility Resource Utility Resources like HorizontalPodAutoscalers etc. will directly be updated via the update process. Scenario: Update also including persistence schema changes During this update the customer can either use helm or dxctl to deploy the new version of our DX deployment. With that, all resources affected will receive their respective changes. StatefulSets Since we directly define the StatefulSets in our Helm chart, the update will cause them to receive the new specification to the kubernetes resource. Kubernetes itself will then start its reconciliation and will ensure that the Pods of the StatefulSet will be restarted with the new configuration. See StatefulSet Rolling Update for reference. This logic is implemented in kubernetes itself and will roll out the changes to the application and will only continue updating the next Pod, if the first one has been restarted with the new configuration and got ready. As soon as schema changes are involved, it may become necessary that a preflight task needs to be run to e.g. to migrate a database schema before deploying new Deployments etc. We could leverage one of the following approaches to run necessary pre-flight tasks: Container integrated logic that checks if schema migration tasks are necessary to run Kubernetes Init Containers attached to the Pods, running specific tasks Kubernetes Jobs that run once and report back completion (could also run as a Helm Hook) While DAM will have automated logic to handle version to version migration of database schemas, for DX Core it might be necessary to perform schema updates that are triggered by the operators/administrators of the customer. Since we do not directly manage the DX Database ourselves, Database operations would preferably not done by our deployment directly, unless we are sure the logic we implemented in DX Core is failsafe. Deployments/Replicasets Applications that are stateless and run as a Deployment will be receiving the updated specification as well directly via the update process. As soon as the Deployment Resource gets altered, Kubernetes will perform a rollover, spawning new Pods with the new configuration. As soon as the new Pods are running successfully and are ready, the old Pods will be terminated. See Deployment Rollover Update for reference. ConfigMaps/Secrets ConfigMaps and Secrets will directly be updated via the update process. Utility Resource Utility Resources like HorizontalPodAutoscalers etc. will directly be updated via the update process. Rollback Rolling back our application is mostly relying on the fact if our applications can roll back to older persistence schemas. For disaster recovery we would use the backups done before. Using those backups of persistent volumes and DBs would allow a customer to go back to a previous version of our deployment, assuming the performed a rollback to the old versions of the deployment e.g. using helm rollback . Tooling Helm Customers should use helm upgrade to perform updates on their existing deployments. Using helm for upgrades provides both us and our customers with the benefit of hierarchical configurations. Per default, a helm chart uses the values.yaml to determine what values should be used in the deployment. A helm chart is always delivered with that file, since it includes all the necessary default values for the deployment. For customers there might be customer specific configuration, e.g. scaling settings. We do not want customers to have the need to manually copy over their existing settings into our default values.yaml OR to copy the new values of the default values.yaml into their custom settings file. This can create merge conflicts, wrong configuration and makes the updating process more complicated for customers. With the hierarchical nature of helm configurations, we have the following pattern at hand: Especially important in our case is the use of our default values.yaml and a user defined custom_values.yaml . With every version of DX we ship, we will deliver a default values.yaml that contains all necessary information to get a DX Kube deployment up and running. This also includes image names, version tags etc. If a customer wants to customize the deployment, they could either edit our default values.yaml and apply the changes, or create their own file and only change the values that are of importance to them. The benefit is quiet obvious: Customers can keep customized deployment configuration in one file, while we can always ship a fitting set of defaults that does not overwrite the customers values. The lifecycle of that configuration can be seen in the following diagram: Customers can retain their own custom values file, while we always ship a new one with each release. With that, we don't override customer settings, but always deliver working default deployment values. Note: The custom values file approach can also be used to provide different deployment templates to our customers, without overwriting our default values.yaml DXCTL DXTCL should ideally be able to consume outputs of the Helm templating engine and use that output to update existing deployments in a similar fashion as it does today already.","title":"Updating"},{"location":"kube/k8s-next-update/#introduction","text":"For our Kubernetes deployments we also need to provide the possibility for customers to do updates. Ideally for our customers there would be no downtime when they perform an update. The update process should also cover possibilities to perform rollbacks if necessary.","title":"Introduction"},{"location":"kube/k8s-next-update/#affected-resources","text":"This chapter lists all important resources in our deployments and their requirements regarding updates.","title":"Affected Resources"},{"location":"kube/k8s-next-update/#stateful-applications","text":"We currently have three stateful applications with distinct requirements:","title":"Stateful Applications"},{"location":"kube/k8s-next-update/#dx-core","text":"DX Core uses two persistent storages for its operation. One of them is a (multi-pod shared) persistent volume which contains the wp_profile. The second persistence is realized using a relational database. That database is not part of the current DX Kube deployment. Therefore, all interactions with that DB need to be done by the operation/administration team of the customer OR by DX Core directly.","title":"DX Core"},{"location":"kube/k8s-next-update/#dam","text":"DAM uses two persistent storages for its operation. One of them is a (multi-pod shared) persistent volume which contains the DAM upload data and acts as a storage for binaries. The second persistence is realized using a relational database in form of PostgreSQL. That database is deployed inside our DX Kube deployments and can therefore be accessed by us during updated directly. It would be possibly for either the operation/administration team of the customer, the DX operator or DAM itself to perform necessary actions on that DB.","title":"DAM"},{"location":"kube/k8s-next-update/#dam-persistence-postgresql","text":"The DAM persistence layer is one of the two persistence parts of DAM. It currently consists of two distinct Pods, where one is a RW primary DB node and the other one is a RO secondary DB node. Both have their own persistent volumes that are not shared. The persistence layer container images contain no schemas or application data, since they will be initialized by DAM. Changes to schemas would also be done by DAM. From an update perspective the most important aspect is to keep the existing data storage of the Pods compatible with the new DB version that comes in a newer container. This basically relies on how PostgreSQL behaves between version updates and needs to be verified.","title":"DAM Persistence (PostgreSQL)"},{"location":"kube/k8s-next-update/#stateless-applications","text":"","title":"Stateless Applications"},{"location":"kube/k8s-next-update/#ringapi","text":"RingAPI is a stateless application that only uses configuration parameters and acts as a wrapping API server around DX Core. From an update perspective there are no special requirements that need to be taken into account. Updating RingAPI basically boils down to deploying a new version of the container image.","title":"RingAPI"},{"location":"kube/k8s-next-update/#content-composer","text":"Content Composer is a stateless application that only uses configuration parameters and acts as a webserver hosting the resources for the Content Composer UI. From an update perspective there are no special requirements that need to be taken into account. Updating Content Composer basically boils down to deploying a new version of the container image.","title":"Content Composer"},{"location":"kube/k8s-next-update/#image-processor","text":"Content Composer is a stateless application that only uses configuration parameters and acts as a media transformation service. It only uses HTTP as communication protocol and requires no persistence. From an update perspective there are no special requirements that need to be taken into account. Updating the Image Processor basically boils down to deploying a new version of the container image.","title":"Image Processor"},{"location":"kube/k8s-next-update/#configurations","text":"","title":"Configurations"},{"location":"kube/k8s-next-update/#configmaps","text":"For general configuration we use two types von ConfigMaps. The first one is an application specific ConfigMap which contains all configuration necessary for individual applications to run. Those ConfigMaps are only consumed by the related application. The second type of ConfigMap is a global deployment wide ConfigMap, that contains configuration that affects all applications. The values of these ConfigMap entries will be directly updated during the update process and applications will be informed about the changes.","title":"ConfigMaps"},{"location":"kube/k8s-next-update/#secrets","text":"For storing secure data like certificates and access credentials, we use Kubernetes Secrets. These secrets will be consumed by our applications. Content of those secrets will be directly updated during the update process and applications will be informed about the changes.","title":"Secrets"},{"location":"kube/k8s-next-update/#utility-resources","text":"Utility Resources e.g. HorizontalPodAutoscalers will be updated directly through the update process.","title":"Utility Resources"},{"location":"kube/k8s-next-update/#ambassador-deployment","text":"We need to replace Ambassador Resources if they have changes. This will happen directly through the update process.","title":"Ambassador deployment"},{"location":"kube/k8s-next-update/#updating","text":"","title":"Updating"},{"location":"kube/k8s-next-update/#backup","text":"Before updating, we need to provide customers with a way to create a backup of the important parts of the deployment. This includes persistent volumes and the data in the DBs of DX Core and DAM. This could be done by: A Kubernetes Job running, triggered by Helm or DXCTL The applications themselves before updating the schemas","title":"Backup"},{"location":"kube/k8s-next-update/#update-categories","text":"Updates we perform can have multiple implications. Based on the way they change existing deployments, we can categorize them in different update categories. Based on in which category an update fits in, different actions or handling might be necessary.","title":"Update Categories"},{"location":"kube/k8s-next-update/#a-application-changes","text":"As we implement features and bugfixes to our applications, the application runtime code will change. This can have implications to configuration and persistence schemas, but that is not always the case. Often enough, application logic will be altered, without affecting existing environmental resources.","title":"A - Application changes"},{"location":"kube/k8s-next-update/#c-configuration-changes","text":"As we perform development on our applications, it might be necessary to either introduce new configuration parameters or alter existing ones to reflect the current state of implementation. It can also happen that default behaviors/values might be adjusted between releases, e.g. if performance assessments have shown, that a Pod needs fewer resources and we lower the Kubernetes resource requests.","title":"C - Configuration changes"},{"location":"kube/k8s-next-update/#p-persistence-schema-changes","text":"For our stateful applications, such as DAM or DX Core, it may become necessary to perform changes to our persistence schemas. This would usually affect the schema of the DAM DB or the DX Core DB. This can happen if we need to introduce new fields into existing data structures.","title":"P - Persistence schema changes"},{"location":"kube/k8s-next-update/#n-new-application-introduction","text":"As we move on with our developments for DX, new applications will arise that will be deployed within a DX deployment. Those applications will require new configurations, Kubernetes resources and perhaps preparation. As an example the roll-out of site-manager would fall into this category.","title":"N - New application introduction"},{"location":"kube/k8s-next-update/#r-removalreplacement-of-an-application","text":"Similar to when we add applications, there may also be parts that will be removed because they perhaps got obsolete or are being replaced with other applications. In that case it is important to ensure that the old resources get cleaned up, the configuration, if necessary, gets removed and the DX deployment is made aware that the application is gone and is not having any dependency anymore.","title":"R - Removal/replacement of an application"},{"location":"kube/k8s-next-update/#scenario-update-only-containing-configuration-and-application-changes","text":"During this update the customer can either use helm or dxctl to deploy the new version of our DX deployment. With that, all resources affected will receive their respective changes.","title":"Scenario: Update only containing configuration and application changes"},{"location":"kube/k8s-next-update/#statefulsets","text":"Since we directly define the Statefulsets in our Helm chart, the update will cause them to receive the new specification to the kubernetes resource. Kubernetes itself will then start its reconciliation and will ensure that the Pods of the statefulset will be restarted with the new configuration. See Statefulset Rolling Update for reference. This logic is implemented in kubernetes itself and will roll out the changes to the application and will only continue updating the next Pod, if the first one has been restarted with the new configuration and got ready.","title":"Statefulsets"},{"location":"kube/k8s-next-update/#deploymentsreplicasets","text":"Applications that are stateless and run as a Deployment will be receiving the updated specification as well directly via the update process. As soon as the Deployment Resource gets altered, Kubernetes will perform a rollover, spawning new Pods with the new configuration. As soon as the new Pods are running successfully and are ready, the old Pods will be terminated. See Deployment Rollover Update for reference.","title":"Deployments/Replicasets"},{"location":"kube/k8s-next-update/#configmapssecrets","text":"ConfigMaps and Secrets will directly be updated via the update process.","title":"ConfigMaps/Secrets"},{"location":"kube/k8s-next-update/#utility-resource","text":"Utility Resources like HorizontalPodAutoscalers etc. will directly be updated via the update process.","title":"Utility Resource"},{"location":"kube/k8s-next-update/#scenario-update-also-including-persistence-schema-changes","text":"During this update the customer can either use helm or dxctl to deploy the new version of our DX deployment. With that, all resources affected will receive their respective changes.","title":"Scenario: Update also including persistence schema changes"},{"location":"kube/k8s-next-update/#statefulsets_1","text":"Since we directly define the StatefulSets in our Helm chart, the update will cause them to receive the new specification to the kubernetes resource. Kubernetes itself will then start its reconciliation and will ensure that the Pods of the StatefulSet will be restarted with the new configuration. See StatefulSet Rolling Update for reference. This logic is implemented in kubernetes itself and will roll out the changes to the application and will only continue updating the next Pod, if the first one has been restarted with the new configuration and got ready. As soon as schema changes are involved, it may become necessary that a preflight task needs to be run to e.g. to migrate a database schema before deploying new Deployments etc. We could leverage one of the following approaches to run necessary pre-flight tasks: Container integrated logic that checks if schema migration tasks are necessary to run Kubernetes Init Containers attached to the Pods, running specific tasks Kubernetes Jobs that run once and report back completion (could also run as a Helm Hook) While DAM will have automated logic to handle version to version migration of database schemas, for DX Core it might be necessary to perform schema updates that are triggered by the operators/administrators of the customer. Since we do not directly manage the DX Database ourselves, Database operations would preferably not done by our deployment directly, unless we are sure the logic we implemented in DX Core is failsafe.","title":"StatefulSets"},{"location":"kube/k8s-next-update/#deploymentsreplicasets_1","text":"Applications that are stateless and run as a Deployment will be receiving the updated specification as well directly via the update process. As soon as the Deployment Resource gets altered, Kubernetes will perform a rollover, spawning new Pods with the new configuration. As soon as the new Pods are running successfully and are ready, the old Pods will be terminated. See Deployment Rollover Update for reference.","title":"Deployments/Replicasets"},{"location":"kube/k8s-next-update/#configmapssecrets_1","text":"ConfigMaps and Secrets will directly be updated via the update process.","title":"ConfigMaps/Secrets"},{"location":"kube/k8s-next-update/#utility-resource_1","text":"Utility Resources like HorizontalPodAutoscalers etc. will directly be updated via the update process.","title":"Utility Resource"},{"location":"kube/k8s-next-update/#rollback","text":"Rolling back our application is mostly relying on the fact if our applications can roll back to older persistence schemas. For disaster recovery we would use the backups done before. Using those backups of persistent volumes and DBs would allow a customer to go back to a previous version of our deployment, assuming the performed a rollback to the old versions of the deployment e.g. using helm rollback .","title":"Rollback"},{"location":"kube/k8s-next-update/#tooling","text":"","title":"Tooling"},{"location":"kube/k8s-next-update/#helm","text":"Customers should use helm upgrade to perform updates on their existing deployments. Using helm for upgrades provides both us and our customers with the benefit of hierarchical configurations. Per default, a helm chart uses the values.yaml to determine what values should be used in the deployment. A helm chart is always delivered with that file, since it includes all the necessary default values for the deployment. For customers there might be customer specific configuration, e.g. scaling settings. We do not want customers to have the need to manually copy over their existing settings into our default values.yaml OR to copy the new values of the default values.yaml into their custom settings file. This can create merge conflicts, wrong configuration and makes the updating process more complicated for customers. With the hierarchical nature of helm configurations, we have the following pattern at hand: Especially important in our case is the use of our default values.yaml and a user defined custom_values.yaml . With every version of DX we ship, we will deliver a default values.yaml that contains all necessary information to get a DX Kube deployment up and running. This also includes image names, version tags etc. If a customer wants to customize the deployment, they could either edit our default values.yaml and apply the changes, or create their own file and only change the values that are of importance to them. The benefit is quiet obvious: Customers can keep customized deployment configuration in one file, while we can always ship a fitting set of defaults that does not overwrite the customers values. The lifecycle of that configuration can be seen in the following diagram: Customers can retain their own custom values file, while we always ship a new one with each release. With that, we don't override customer settings, but always deliver working default deployment values. Note: The custom values file approach can also be used to provide different deployment templates to our customers, without overwriting our default values.yaml","title":"Helm"},{"location":"kube/k8s-next-update/#dxctl","text":"DXTCL should ideally be able to consume outputs of the Helm templating engine and use that output to update existing deployments in a similar fashion as it does today already.","title":"DXCTL"},{"location":"kube/Migration/k8s-next-migrate-detail-core/","text":"This section will documented a overview how it is possible to migrate a core profile from a operator based deployment to a HELM based deployment. Also a output of this design is a first draft for the raw documentation. Migration-Mode for the HELM deployment Before we can start to document the backup and restore/migration of the core profile we should implemented a migration mode. The goal of this mode is to start the core pod but without starting DX. The Pod should stay alive without DX runninng, so it is possible to connect to the pod and perform actions on the file system. For the migration mode need to should adjust the following areas: Adding a migration parameter to the HELM chart Liveness, readiness, startUp - checks Core start scripts Here are some example implementations on how to do that, as I have tested this in feature branches. https://git.cwp.pnp-hcl.com/websphere-portal-incubator/dx-helm-charts/tree/feature/DXQ-19119 https://git.cwp.pnp-hcl.com/Team-Q/Portal-Docker-Images/compare/feature/DXQ-19119 The migration mode should be also interested for DAM. Here are the needed changes. HELM charts add config map entry \"dam.config.dam.migration\": \"{{ .Values.migration.enabled }}\" # Liveness probe {{- if .Values.migration.enabled }} livenessProbe: exec: command: - /bin/sh - -c - exit 0 failureThreshold: {{ .failureThreshold }} initialDelaySeconds: {{ .initialDelaySeconds }} periodSeconds: {{ .periodSeconds }} successThreshold: {{ .successThreshold }} timeoutSeconds: {{ .timeoutSeconds }} {{ else }} livenessProbe: {{- with .Values.probes.digitalAssetManagement.livenessProbe }} {{- toYaml . | nindent 12 }} {{- end }} {{- end }} # Readiness probe {{- if .Values.migration.enabled }} readinessProbe: exec: command: - /bin/sh - -c - exit 0 failureThreshold: {{ .failureThreshold }} initialDelaySeconds: {{ .initialDelaySeconds }} periodSeconds: {{ .periodSeconds }} successThreshold: {{ .successThreshold }} timeoutSeconds: {{ .timeoutSeconds }} {{ else }} readinessProbe: {{- with .Values.probes.digitalAssetManagement.readinessProbe }} {{- toYaml . | nindent 12 }} {{- end }} {{- end }} Start scripts: #!/bin/bash # Uses environment variables from a config map if mounted for file in /etc/config/dam.config.dam.*; do # Loop through all the files in directory and check if file exists. if test -f \"$file\"; then # Split the filename into array with dot(.) delimiter. IFS='.' read -r -a array <<< \"$(basename \"$file\")\" # Get the last element of array for eg. postgres_db. envVariableTemp=${array[@]: -1:1} # Capitilise the string. envVariable=$(tr '[a-z]' '[A-Z]' <<< $envVariableTemp) fileName=\"${file##*/}\" # Assign the variables with contents of file. eg. POSTGRES_DB='testdb'. printf -v $envVariable $(cat /etc/config/$fileName) # export the variable as environment variable. export $envVariable fi done clean_up () { echo \"Sending SIGTERM to all node instances...\" cd /opt/app/server-v1 && npm run docker:stop-daemon } stop_container () { echo \"Caught SIGTERM, stopping DX Core container\" exit 0 } echo \"Migration is: $MIGRATION\" if [ \"$MIGRATION\" == \"true\" ]; then trap stop_container SIGTERM echo \"Listening for SIGTERM\" echo \"Migration is enabled, stay alive and do nothing ...\" # ensure the docker container stays alive and can listen for signals every second while true; do sleep 1; done else trap clean_up SIGHUP SIGINT SIGTERM # run server version 1 cd /opt/app/server-v1 && npm run docker:start fi Backup and restore process (raw doc) This section documents the migration process of the core profile. Backup form the operator based deployment 1. Ensure that only one core pod is running Check how many pods are running. Use the following command for that. kubectl -n <namespace> get all If more then one core pod running, scaling down the core pods to only one. On the operator deployment adjust the DXCTL property file and apply this changes via the DXCTL tool. 2. Connecting to the core pod With the following command it is possible to jump directly into the running container. kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-0 -n dxns -- /bin/bash 3. Stop the server Before a backup from the wp-profile can be create, the core application should be stopped. Navigate to the profile bin folder and run the stopServer command. cd /opt/HCL/wp_profile/bin/ ./stopServer.sh WebSphere_Portal -username <username> -password <password> 4. Compress the profile For the operator-based deployment is the whole core profile located under /opt/HCL/wp_profile . This folder must be compressed. cd /opt/HCL tar -cvpzf core_prof_95_CF197.tar.gz --exclude=/core_prof_95_CF197.tar.gz --one-file-system wp_profile 5. Download the backup profile From a local system it is now possible to download the backup core profile from the core pod container. kubectl cp <namespace>/<pod-name>:<source-file> <target-file> Example: kubectl cp dxns/dx-deployment-0:opt/HCL/core_prof_95_CF197.tar.gz /tmp/core_prof_95_CF197.tar.gz Restore to the HELM based deployment 1. Start the HELM deployment Before we can start with the restore we should ensure that the HELM based deployment is in correct state. Some adjustments are necessary. The extraction of kubernetes DX configuration from the operator based deployment to a valid values.yaml file is done. Enable the migration mode. For the first start the runtimeController and the core application should be enabled. Now the HELM deployment can be started. helm install -n <namespace> --create-namespace -f <values.yaml> <prefix> <chart> Example: helm install -n dxns --create-namespace -f hcl-dx-deployment/value-samples/internal-repos.yaml hha hcl-dx-deployment What we expected now is: The core pod is running and keep alive. The core application is NOT running. No default profile should be created automatically. The folder /opt/HCL/wp_profile is empty. 2. Upload the backup profile Now it is possible to transfer the backup profile to the remote core pod container. kubectl cp <source-file> <namespace>/<pod-name>:<target-file> Example: kubectl cp /tmp/core_prof_95_CF197.tar.gz dxns/hha-core-0:/tmp/core_prof_95_CF197.tar.gz 3. Connect to the core pod With the following command it is possible to connect directly into the running container. kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/hha-core-0 -n dxns -- /bin/bash 4. Extracting the profile At first the backup file core_prof_95_CF197.tar.gz should moved from /tmp folder to the profile folder /opt/HCL/profiles . And the extraction can be started. tar -xf /tmp/core_prof_95_CF197.tar.gz --directory /opt/HCL/profiles mv /opt/HCL/profiles/wp_profile /opt/HCL/profiles/prof_95_CF197 rm /tmp/core_prof_95_CF197.tar.gz The next step is to create a symlink. rm -r /opt/HCL/wp_profile ln -s /opt/HCL/profiles/prof_95_CF197 /opt/HCL/wp_profile 5. Disable the migration mode and the deployment Before we can start with the final upgrad of the HELM deployment some adjustments are necessary. Disable the migration mode. Enable all relevant applications. Now the HELM deployment can be upgraded. helm upgrade -n <namespace> --create-namespace -f <values.yaml> <prefix> <chart> Example: helm upgrade -n dxns --create-namespace -f hcl-dx-deployment/value-samples/internal-repos.yaml hha hcl-dx-deployment","title":"Operator migration - core"},{"location":"kube/Migration/k8s-next-migrate-detail-core/#migration-mode-for-the-helm-deployment","text":"Before we can start to document the backup and restore/migration of the core profile we should implemented a migration mode. The goal of this mode is to start the core pod but without starting DX. The Pod should stay alive without DX runninng, so it is possible to connect to the pod and perform actions on the file system. For the migration mode need to should adjust the following areas: Adding a migration parameter to the HELM chart Liveness, readiness, startUp - checks Core start scripts Here are some example implementations on how to do that, as I have tested this in feature branches. https://git.cwp.pnp-hcl.com/websphere-portal-incubator/dx-helm-charts/tree/feature/DXQ-19119 https://git.cwp.pnp-hcl.com/Team-Q/Portal-Docker-Images/compare/feature/DXQ-19119 The migration mode should be also interested for DAM. Here are the needed changes. HELM charts add config map entry \"dam.config.dam.migration\": \"{{ .Values.migration.enabled }}\" # Liveness probe {{- if .Values.migration.enabled }} livenessProbe: exec: command: - /bin/sh - -c - exit 0 failureThreshold: {{ .failureThreshold }} initialDelaySeconds: {{ .initialDelaySeconds }} periodSeconds: {{ .periodSeconds }} successThreshold: {{ .successThreshold }} timeoutSeconds: {{ .timeoutSeconds }} {{ else }} livenessProbe: {{- with .Values.probes.digitalAssetManagement.livenessProbe }} {{- toYaml . | nindent 12 }} {{- end }} {{- end }} # Readiness probe {{- if .Values.migration.enabled }} readinessProbe: exec: command: - /bin/sh - -c - exit 0 failureThreshold: {{ .failureThreshold }} initialDelaySeconds: {{ .initialDelaySeconds }} periodSeconds: {{ .periodSeconds }} successThreshold: {{ .successThreshold }} timeoutSeconds: {{ .timeoutSeconds }} {{ else }} readinessProbe: {{- with .Values.probes.digitalAssetManagement.readinessProbe }} {{- toYaml . | nindent 12 }} {{- end }} {{- end }} Start scripts: #!/bin/bash # Uses environment variables from a config map if mounted for file in /etc/config/dam.config.dam.*; do # Loop through all the files in directory and check if file exists. if test -f \"$file\"; then # Split the filename into array with dot(.) delimiter. IFS='.' read -r -a array <<< \"$(basename \"$file\")\" # Get the last element of array for eg. postgres_db. envVariableTemp=${array[@]: -1:1} # Capitilise the string. envVariable=$(tr '[a-z]' '[A-Z]' <<< $envVariableTemp) fileName=\"${file##*/}\" # Assign the variables with contents of file. eg. POSTGRES_DB='testdb'. printf -v $envVariable $(cat /etc/config/$fileName) # export the variable as environment variable. export $envVariable fi done clean_up () { echo \"Sending SIGTERM to all node instances...\" cd /opt/app/server-v1 && npm run docker:stop-daemon } stop_container () { echo \"Caught SIGTERM, stopping DX Core container\" exit 0 } echo \"Migration is: $MIGRATION\" if [ \"$MIGRATION\" == \"true\" ]; then trap stop_container SIGTERM echo \"Listening for SIGTERM\" echo \"Migration is enabled, stay alive and do nothing ...\" # ensure the docker container stays alive and can listen for signals every second while true; do sleep 1; done else trap clean_up SIGHUP SIGINT SIGTERM # run server version 1 cd /opt/app/server-v1 && npm run docker:start fi","title":"Migration-Mode for the HELM deployment"},{"location":"kube/Migration/k8s-next-migrate-detail-core/#backup-and-restore-process-raw-doc","text":"This section documents the migration process of the core profile.","title":"Backup and restore process (raw doc)"},{"location":"kube/Migration/k8s-next-migrate-detail-core/#backup-form-the-operator-based-deployment","text":"","title":"Backup form the operator based deployment"},{"location":"kube/Migration/k8s-next-migrate-detail-core/#1-ensure-that-only-one-core-pod-is-running","text":"Check how many pods are running. Use the following command for that. kubectl -n <namespace> get all If more then one core pod running, scaling down the core pods to only one. On the operator deployment adjust the DXCTL property file and apply this changes via the DXCTL tool.","title":"1. Ensure that only one core pod is running"},{"location":"kube/Migration/k8s-next-migrate-detail-core/#2-connecting-to-the-core-pod","text":"With the following command it is possible to jump directly into the running container. kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-0 -n dxns -- /bin/bash","title":"2. Connecting to the core pod"},{"location":"kube/Migration/k8s-next-migrate-detail-core/#3-stop-the-server","text":"Before a backup from the wp-profile can be create, the core application should be stopped. Navigate to the profile bin folder and run the stopServer command. cd /opt/HCL/wp_profile/bin/ ./stopServer.sh WebSphere_Portal -username <username> -password <password>","title":"3. Stop the server"},{"location":"kube/Migration/k8s-next-migrate-detail-core/#4-compress-the-profile","text":"For the operator-based deployment is the whole core profile located under /opt/HCL/wp_profile . This folder must be compressed. cd /opt/HCL tar -cvpzf core_prof_95_CF197.tar.gz --exclude=/core_prof_95_CF197.tar.gz --one-file-system wp_profile","title":"4. Compress the profile"},{"location":"kube/Migration/k8s-next-migrate-detail-core/#5-download-the-backup-profile","text":"From a local system it is now possible to download the backup core profile from the core pod container. kubectl cp <namespace>/<pod-name>:<source-file> <target-file> Example: kubectl cp dxns/dx-deployment-0:opt/HCL/core_prof_95_CF197.tar.gz /tmp/core_prof_95_CF197.tar.gz","title":"5. Download the backup profile"},{"location":"kube/Migration/k8s-next-migrate-detail-core/#restore-to-the-helm-based-deployment","text":"","title":"Restore to the HELM based deployment"},{"location":"kube/Migration/k8s-next-migrate-detail-core/#1-start-the-helm-deployment","text":"Before we can start with the restore we should ensure that the HELM based deployment is in correct state. Some adjustments are necessary. The extraction of kubernetes DX configuration from the operator based deployment to a valid values.yaml file is done. Enable the migration mode. For the first start the runtimeController and the core application should be enabled. Now the HELM deployment can be started. helm install -n <namespace> --create-namespace -f <values.yaml> <prefix> <chart> Example: helm install -n dxns --create-namespace -f hcl-dx-deployment/value-samples/internal-repos.yaml hha hcl-dx-deployment What we expected now is: The core pod is running and keep alive. The core application is NOT running. No default profile should be created automatically. The folder /opt/HCL/wp_profile is empty.","title":"1. Start the HELM deployment"},{"location":"kube/Migration/k8s-next-migrate-detail-core/#2-upload-the-backup-profile","text":"Now it is possible to transfer the backup profile to the remote core pod container. kubectl cp <source-file> <namespace>/<pod-name>:<target-file> Example: kubectl cp /tmp/core_prof_95_CF197.tar.gz dxns/hha-core-0:/tmp/core_prof_95_CF197.tar.gz","title":"2. Upload the backup profile"},{"location":"kube/Migration/k8s-next-migrate-detail-core/#3-connect-to-the-core-pod","text":"With the following command it is possible to connect directly into the running container. kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/hha-core-0 -n dxns -- /bin/bash","title":"3. Connect to the core pod"},{"location":"kube/Migration/k8s-next-migrate-detail-core/#4-extracting-the-profile","text":"At first the backup file core_prof_95_CF197.tar.gz should moved from /tmp folder to the profile folder /opt/HCL/profiles . And the extraction can be started. tar -xf /tmp/core_prof_95_CF197.tar.gz --directory /opt/HCL/profiles mv /opt/HCL/profiles/wp_profile /opt/HCL/profiles/prof_95_CF197 rm /tmp/core_prof_95_CF197.tar.gz The next step is to create a symlink. rm -r /opt/HCL/wp_profile ln -s /opt/HCL/profiles/prof_95_CF197 /opt/HCL/wp_profile","title":"4. Extracting the profile"},{"location":"kube/Migration/k8s-next-migrate-detail-core/#5-disable-the-migration-mode-and-the-deployment","text":"Before we can start with the final upgrad of the HELM deployment some adjustments are necessary. Disable the migration mode. Enable all relevant applications. Now the HELM deployment can be upgraded. helm upgrade -n <namespace> --create-namespace -f <values.yaml> <prefix> <chart> Example: helm upgrade -n dxns --create-namespace -f hcl-dx-deployment/value-samples/internal-repos.yaml hha hcl-dx-deployment","title":"5. Disable the migration mode and the deployment"},{"location":"kube/Migration/k8s-next-migrate-detail-dam/","text":"This section will documented a overview how it is possible to migrate a dam persistence and binaries from a operator based deployment to a HELM based deployment. Also a output of this design is a first draft for the raw documentation. This document may change after DBHA implementation. Migration-Mode for the HELM deployment Before we can start to document the backup and restore of the dam persistence and binaries we should implemented a migration mode. The details of implementing migration mode will be available here . Backup and restore process This section documents the backup and restore process of the dam persistence and binaries. Backup form the operator based deployment 1. Ensure persistence(read write) and dam pod are running The following command will help to find pods current status. kubectl -n <namespace> get all Ensure pods are up and running. If more than one dam and persistence pods are running, then scale down the pods to one. 2. Database backup from persistence(read write) a. Connect to persistence pod. The following command is used to connect with persistence container. kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-persistence-0 -n dx-ns -- /bin/bash b. Dump the current database using pg_dump. [dx_user@dx-deployment-persistence-0 /]$ pg_dump dxmediadb > /tmp/dxmediadb.dmp c. Download the dumped database to local system by using the following command. kubectl cp <namespace>/<pod-name>:<source-file> <target-file> Example kubectl cp dx-ns/dx-deployment-persistence-0:/tmp/dxmediadb.dmp /backup/dxmediadb.dmp 3. DAM binaries and profile backup a. Connect to DAM pod. The following command is used to connect with container. kubectl exec --stdin --tty pod/dx-deployment-dam-0 -n dx-ns -- /bin/bash b. Compress the DAM binaries which are located under /opt/app/upload directory. tar -cvpzf backupml.tar.gz --exclude=/backupml.tar.gz --one-file-system /opt/app/upload c. Compress the DAM profiles which are located under /etc/config directory. tar -cvpzf backupmlcfg.tar.gz --exclude=/backupmlcfg.tar.gz --one-file-system /etc/config d. Download the compressed binaries and profiles to the local system. kubectl cp dx-ns/dx-deployment-dam-0:/opt/app/server-v1/backupml.tar.gz /backup/backupml.tar.gz kubectl cp dx-ns/dx-deployment-dam-0:/opt/app/server-v1/backupmlcfg.tar.gz /backup/backupmlcfg.tar.gz Restore to the HELM based deployment 1. Start the HELM deployment Before we can start with the restore we must ensure that the HELM based deployment is in correct state. Some adjustments are necessary. The extraction of kubernetes DX configuration from the operator based deployment to a valid values.yaml file is done. Enable the migration mode. Now the HELM deployment can be started. helm install -n <namespace> --create-namespace -f <values.yaml> <prefix> <chart> Example: helm install -n dxns-helm --create-namespace -f hcl-dx-deployment/value-sample.yaml dx-deployment hcl-dx-deployment At the moment, the dam and persistence pods are alive. But application is not ready since migration mode is enabled. 2. Restore the database a. Upload the backup database to persistence pod. kubectl cp /backup/dxmediadb.dmp dxns-helm/pod/dx-deployment-persistence-rw-0:/tmp/dxmediadb.dmp b. Connect to persistence read write pod and perform the restore process. kubectl exec --stdin --tty pod/dx-deployment-persistence-rw-0 -n dxns-helm -- /bin/bash [dx_user@dx-deployment-persistence-rw-0 tmp]$ dropdb dxmediadb [dx_user@dx-deployment-persistence-rw-0 tmp]$ createdb -O dxuser dxmediadb [dx_user@dx-deployment-persistence-rw-0 tmp]$ psql dxmediadb < dxmediadb.dmp 3. Restore the binaries and profiles a. Upload the backup binary and profiles to dam pod. kubectl cp /backup/backupml.tar.gz dxns-helm/pod/dx-deployment-digital-asset-management-0:/opt/app/server-v1/backupml.tar.gz kubectl cp /backup/backupmlcfg.tar.gz dxns-helm/pod/dx-deployment-digital-asset-management-0:/opt/app/server-v1/backupmlcfg.tar.gz b. Connect to dam pod and perform the restore process. kubectl exec --stdin --tty pod/dx-deployment-digital-asset-management-0 -n dxns-helm -- /bin/bash [dx_user@dx-deployment-digital-asset-management-0 server-v1]$ tar -xf /backupml.tar.gz --directory /opt/app/upload [dx_user@dx-deployment-digital-asset-management-0 server-v1]$ tar -xf /backupmlcfg.tar.gz --directory /etc/config [dx_user@dx-deployment-digital-asset-management-0 server-v1]$ rm /backupml.tar.gz [dx_user@dx-deployment-digital-asset-management-0 server-v1]$ rm /backupmlcfg.tar.gz 4. Disable the migration mode and the deployment Before we can start with the final upgrade of the HELM deployment some adjustments are necessary. Disable the migration mode. Enable all relevant applications. Now the HELM deployment can be upgraded. helm upgrade -n <namespace> --create-namespace -f <values.yaml> <prefix> <chart> Example: helm upgrade -n dxns-helm --create-namespace -f hcl-dx-deployment/value-sample.yaml dx-deployment hcl-dx-deployment","title":"Operator migration - DAM"},{"location":"kube/Migration/k8s-next-migrate-detail-dam/#migration-mode-for-the-helm-deployment","text":"Before we can start to document the backup and restore of the dam persistence and binaries we should implemented a migration mode. The details of implementing migration mode will be available here .","title":"Migration-Mode for the HELM deployment"},{"location":"kube/Migration/k8s-next-migrate-detail-dam/#backup-and-restore-process","text":"This section documents the backup and restore process of the dam persistence and binaries.","title":"Backup and restore process"},{"location":"kube/Migration/k8s-next-migrate-detail-dam/#backup-form-the-operator-based-deployment","text":"","title":"Backup form the operator based deployment"},{"location":"kube/Migration/k8s-next-migrate-detail-dam/#1-ensure-persistenceread-write-and-dam-pod-are-running","text":"The following command will help to find pods current status. kubectl -n <namespace> get all Ensure pods are up and running. If more than one dam and persistence pods are running, then scale down the pods to one.","title":"1. Ensure persistence(read write) and dam pod are running"},{"location":"kube/Migration/k8s-next-migrate-detail-dam/#2-database-backup-from-persistenceread-write","text":"a. Connect to persistence pod. The following command is used to connect with persistence container. kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-persistence-0 -n dx-ns -- /bin/bash b. Dump the current database using pg_dump. [dx_user@dx-deployment-persistence-0 /]$ pg_dump dxmediadb > /tmp/dxmediadb.dmp c. Download the dumped database to local system by using the following command. kubectl cp <namespace>/<pod-name>:<source-file> <target-file> Example kubectl cp dx-ns/dx-deployment-persistence-0:/tmp/dxmediadb.dmp /backup/dxmediadb.dmp","title":"2. Database backup from persistence(read write)"},{"location":"kube/Migration/k8s-next-migrate-detail-dam/#3-dam-binaries-and-profile-backup","text":"a. Connect to DAM pod. The following command is used to connect with container. kubectl exec --stdin --tty pod/dx-deployment-dam-0 -n dx-ns -- /bin/bash b. Compress the DAM binaries which are located under /opt/app/upload directory. tar -cvpzf backupml.tar.gz --exclude=/backupml.tar.gz --one-file-system /opt/app/upload c. Compress the DAM profiles which are located under /etc/config directory. tar -cvpzf backupmlcfg.tar.gz --exclude=/backupmlcfg.tar.gz --one-file-system /etc/config d. Download the compressed binaries and profiles to the local system. kubectl cp dx-ns/dx-deployment-dam-0:/opt/app/server-v1/backupml.tar.gz /backup/backupml.tar.gz kubectl cp dx-ns/dx-deployment-dam-0:/opt/app/server-v1/backupmlcfg.tar.gz /backup/backupmlcfg.tar.gz","title":"3. DAM binaries and profile backup"},{"location":"kube/Migration/k8s-next-migrate-detail-dam/#restore-to-the-helm-based-deployment","text":"","title":"Restore to the HELM based deployment"},{"location":"kube/Migration/k8s-next-migrate-detail-dam/#1-start-the-helm-deployment","text":"Before we can start with the restore we must ensure that the HELM based deployment is in correct state. Some adjustments are necessary. The extraction of kubernetes DX configuration from the operator based deployment to a valid values.yaml file is done. Enable the migration mode. Now the HELM deployment can be started. helm install -n <namespace> --create-namespace -f <values.yaml> <prefix> <chart> Example: helm install -n dxns-helm --create-namespace -f hcl-dx-deployment/value-sample.yaml dx-deployment hcl-dx-deployment At the moment, the dam and persistence pods are alive. But application is not ready since migration mode is enabled.","title":"1. Start the HELM deployment"},{"location":"kube/Migration/k8s-next-migrate-detail-dam/#2-restore-the-database","text":"a. Upload the backup database to persistence pod. kubectl cp /backup/dxmediadb.dmp dxns-helm/pod/dx-deployment-persistence-rw-0:/tmp/dxmediadb.dmp b. Connect to persistence read write pod and perform the restore process. kubectl exec --stdin --tty pod/dx-deployment-persistence-rw-0 -n dxns-helm -- /bin/bash [dx_user@dx-deployment-persistence-rw-0 tmp]$ dropdb dxmediadb [dx_user@dx-deployment-persistence-rw-0 tmp]$ createdb -O dxuser dxmediadb [dx_user@dx-deployment-persistence-rw-0 tmp]$ psql dxmediadb < dxmediadb.dmp","title":"2. Restore the database"},{"location":"kube/Migration/k8s-next-migrate-detail-dam/#3-restore-the-binaries-and-profiles","text":"a. Upload the backup binary and profiles to dam pod. kubectl cp /backup/backupml.tar.gz dxns-helm/pod/dx-deployment-digital-asset-management-0:/opt/app/server-v1/backupml.tar.gz kubectl cp /backup/backupmlcfg.tar.gz dxns-helm/pod/dx-deployment-digital-asset-management-0:/opt/app/server-v1/backupmlcfg.tar.gz b. Connect to dam pod and perform the restore process. kubectl exec --stdin --tty pod/dx-deployment-digital-asset-management-0 -n dxns-helm -- /bin/bash [dx_user@dx-deployment-digital-asset-management-0 server-v1]$ tar -xf /backupml.tar.gz --directory /opt/app/upload [dx_user@dx-deployment-digital-asset-management-0 server-v1]$ tar -xf /backupmlcfg.tar.gz --directory /etc/config [dx_user@dx-deployment-digital-asset-management-0 server-v1]$ rm /backupml.tar.gz [dx_user@dx-deployment-digital-asset-management-0 server-v1]$ rm /backupmlcfg.tar.gz","title":"3. Restore the binaries and profiles"},{"location":"kube/Migration/k8s-next-migrate-detail-dam/#4-disable-the-migration-mode-and-the-deployment","text":"Before we can start with the final upgrade of the HELM deployment some adjustments are necessary. Disable the migration mode. Enable all relevant applications. Now the HELM deployment can be upgraded. helm upgrade -n <namespace> --create-namespace -f <values.yaml> <prefix> <chart> Example: helm upgrade -n dxns-helm --create-namespace -f hcl-dx-deployment/value-sample.yaml dx-deployment hcl-dx-deployment","title":"4. Disable the migration mode and the deployment"},{"location":"kube/Migration/k8s-next-migrate-from-operator/","text":"Requirements A proper installed Operator deployment is existing The installed DX release version should be the same for the operator and HELM deployment to minimize conflicts Use the same database for the operator and the HELM deployment to also minimize conflicts Idea of migration path The migration will be separated in three major steps. Extracting kube config's and create a HELM extracted-values.yaml Deployment of a new DX instance via HELM charts and the extracted-values.yaml DX component migration Extracting kube config's and create the helm extracted-values.yaml Extracting configuration data from the Operator deployment Extracting related configuration values from Operator deployment to adjust the values of the helm deployment. The following parameter may be important to look at if you manually adjusted these. default.repository image and tags pullpolicy which applications are enabled cors configuration hybrid configuration request/limit - cpu and memory minreplicas, maxreplicas, targetcpuutilizationpercent, targetmemoryutilizationpercent persistent volume configuration (storageclass, ...) custom resource labels node selector configuration (for DX Core) The extracted values may need to be transfered it into a custom values.yaml helm file. Extracting the values from the dxctl property file Via this approach we should only analyse the DXCTL property file and extract the config values from there. This extracted config values can be use to create a custom values.yaml file. Deployment a new DX instance via HELM charts and the extracted-values.yaml Using the extracted-values.yaml from the operator deployment to get an adjusted HELM deployment. Run the HELM install CMD. For example: helm install -n <namespace> -f ./extracted-values.yaml <suffix> hcl-dx-deployment DX component migration The following components we need to migrate from the operator to the HELM deployment Core: Custom Themes Custom Portlets Pages WCM content/libs DAM assets Personalization rules (PZN) Credential Vault Virtual Portals Customer extensions: Shared libs Resource Environment Providers Some special WAS configs Extensions like preprocessor, filters, ... ... DAM: Collections, Items, Metadata Assets (binaries) We have checked three approaches to migrate the DX Core components and customer extensions. Using the dxclient tool Creating and deploying the initial release - HCL public documentation Profile migration To migrate the DAM data, we have have two options. Using the EXIM tool Using a manual backup and recover path DX Core and customer extensions migration dxclient The dxclient tool should be a good option to do that. For the most DX core components provided the tool a export and import functionality. But some components are missing also it is a problem to cover all the customer stuff, like some extensions, preprocessors, filters and what ever. Besides of that, the export can also be taking really long time, if the customer has create many pages or WCM data. Creating and deploying the initial release The initial release approach is creating a PAA application from some of the most DX core components, but also not for all. Also here the export and import can be taking really long time, if the customer has create many pages or WCM data. At the end this approach has not worked. The export process was successful but the import has some issues. I haven open a bug ( DXQ-18719 ) for that. Profile migration The profile migration was a idea from Thomas, which it looks like is a really good approach to do that. How it works this? Transferring the DB data - it is only relevant if the customer decide to use a new DB instance Stop the core package from HELM deployment Transfer the 'wp-profile' data from the operator deployment to the helm deployment Changed the database via the WAS console - it is only relevant if the customer decide to use a new DB instance Also we should fixed all hardcoded absolute URLs (like the URLs for the DAM assets) DX DAM migration EXIM Using the EXIM tool. The tool helps us to exports and imports all metadata and binaries. But it has some missing functionality and bugs, which are needs to solved it at first. I have open some stories and bugs to solve this. DXQ-18810 - EXIM are using always ports, but the kube deployment are not exporting some ports DXQ-18812 - EXIM has also a problem with the connection agains the Kube deployment. DXQ-18813 - The import from binaries are not working as expected. DXQ-18814 - The import from metadata are not working as expected, the reason is that the DAM collections access rights are not transferred. Workaround to solve it: Before we can import the metadata, we should transfer manually the binaries from the operator deployment to the HELM deployment. The binaries are located in the DAM persistent volume under the dx-dam-media folder. Also we should transfer manually all DAM collections resources permissions. Via XMLAccess we can extract all DAM collections resources permissions. For this export we should use the /opt/HCL/PortalServer/doc/xml-samples/ExportAllDamCollections.xml file to do that. And the generated XML output should be used to import it on the HELM deployment side. If all DAM data's are transferred successful, we need to adjust the stored references to WCM assets in WCM. The references are stored as fully qualified URLs. For this problem exists solution to set a alternate hostname. WCM DAM alternate hostname Backup and recover The following documentation provided a manual backup and recovery way. But it looks that the documentation is not in a good shape. https://help.hcltechsw.com/digital-experience/9.5/containerization/backup_and_recovery_procedures.html https://support.hcltechsw.com/community?id=community_question&sys_id=9e73a02e1b20b850f37655352a4bcbaa Summary Migration from operator-based to helm-based deployments will only be possible with exactly one DX version (presumable CF198). Both deployments have to be at the same version level. Moving Kubernetes configurations : We will only provide documentation on how to extract configurations from an existing operator-based deployment using dxctl and how to interpret these. We will also document how a customer would be able to reflect these in their custom values.yaml for their helm-based deployment. For the DX Core migration the best approach is to copy the wp_profile from the operator-based deployment to the helm-based deployment. We expect other connected infrastructures (e.g. LDAP) to remain unchanged, hence no configuration changes are necessary. DX Core database should just stay the same, hence no further changes would be necessary after copying the WAS profile. The old database needs to be reachable from the new helm-based deployment. If a customer needs to change the connected database, he can use database transfer tooling to move database content between the database instances. For DX DAM the DAM EXIM tool would be the preferred way. Unfortunately the EXIM implementation is not finished yet. Therefore the DAM move between an operator-based and a helm-based deployment will need to be done using Postgres database export/import and a copy of the DAM's binary storage. If EXIM gets finished in time, we can consider changing the documentation appropriately.","title":"Operator migration"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#requirements","text":"A proper installed Operator deployment is existing The installed DX release version should be the same for the operator and HELM deployment to minimize conflicts Use the same database for the operator and the HELM deployment to also minimize conflicts","title":"Requirements"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#idea-of-migration-path","text":"The migration will be separated in three major steps. Extracting kube config's and create a HELM extracted-values.yaml Deployment of a new DX instance via HELM charts and the extracted-values.yaml DX component migration","title":"Idea of migration path"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#extracting-kube-configs-and-create-the-helm-extracted-valuesyaml","text":"","title":"Extracting kube config's and create the helm extracted-values.yaml"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#extracting-configuration-data-from-the-operator-deployment","text":"Extracting related configuration values from Operator deployment to adjust the values of the helm deployment. The following parameter may be important to look at if you manually adjusted these. default.repository image and tags pullpolicy which applications are enabled cors configuration hybrid configuration request/limit - cpu and memory minreplicas, maxreplicas, targetcpuutilizationpercent, targetmemoryutilizationpercent persistent volume configuration (storageclass, ...) custom resource labels node selector configuration (for DX Core) The extracted values may need to be transfered it into a custom values.yaml helm file.","title":"Extracting configuration data from the Operator deployment"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#extracting-the-values-from-the-dxctl-property-file","text":"Via this approach we should only analyse the DXCTL property file and extract the config values from there. This extracted config values can be use to create a custom values.yaml file.","title":"Extracting the values from the dxctl property file"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#deployment-a-new-dx-instance-via-helm-charts-and-the-extracted-valuesyaml","text":"Using the extracted-values.yaml from the operator deployment to get an adjusted HELM deployment. Run the HELM install CMD. For example: helm install -n <namespace> -f ./extracted-values.yaml <suffix> hcl-dx-deployment","title":"Deployment a new DX instance via HELM charts and the extracted-values.yaml"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#dx-component-migration","text":"The following components we need to migrate from the operator to the HELM deployment","title":"DX component migration"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#core","text":"Custom Themes Custom Portlets Pages WCM content/libs DAM assets Personalization rules (PZN) Credential Vault Virtual Portals","title":"Core:"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#customer-extensions","text":"Shared libs Resource Environment Providers Some special WAS configs Extensions like preprocessor, filters, ... ...","title":"Customer extensions:"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#dam","text":"Collections, Items, Metadata Assets (binaries) We have checked three approaches to migrate the DX Core components and customer extensions. Using the dxclient tool Creating and deploying the initial release - HCL public documentation Profile migration To migrate the DAM data, we have have two options. Using the EXIM tool Using a manual backup and recover path","title":"DAM:"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#dx-core-and-customer-extensions-migration","text":"","title":"DX Core and customer extensions migration"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#dxclient","text":"The dxclient tool should be a good option to do that. For the most DX core components provided the tool a export and import functionality. But some components are missing also it is a problem to cover all the customer stuff, like some extensions, preprocessors, filters and what ever. Besides of that, the export can also be taking really long time, if the customer has create many pages or WCM data.","title":"dxclient"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#creating-and-deploying-the-initial-release","text":"The initial release approach is creating a PAA application from some of the most DX core components, but also not for all. Also here the export and import can be taking really long time, if the customer has create many pages or WCM data. At the end this approach has not worked. The export process was successful but the import has some issues. I haven open a bug ( DXQ-18719 ) for that.","title":"Creating and deploying the initial release"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#profile-migration","text":"The profile migration was a idea from Thomas, which it looks like is a really good approach to do that. How it works this? Transferring the DB data - it is only relevant if the customer decide to use a new DB instance Stop the core package from HELM deployment Transfer the 'wp-profile' data from the operator deployment to the helm deployment Changed the database via the WAS console - it is only relevant if the customer decide to use a new DB instance Also we should fixed all hardcoded absolute URLs (like the URLs for the DAM assets)","title":"Profile migration"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#dx-dam-migration","text":"","title":"DX DAM migration"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#exim","text":"Using the EXIM tool. The tool helps us to exports and imports all metadata and binaries. But it has some missing functionality and bugs, which are needs to solved it at first. I have open some stories and bugs to solve this. DXQ-18810 - EXIM are using always ports, but the kube deployment are not exporting some ports DXQ-18812 - EXIM has also a problem with the connection agains the Kube deployment. DXQ-18813 - The import from binaries are not working as expected. DXQ-18814 - The import from metadata are not working as expected, the reason is that the DAM collections access rights are not transferred.","title":"EXIM"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#workaround-to-solve-it","text":"Before we can import the metadata, we should transfer manually the binaries from the operator deployment to the HELM deployment. The binaries are located in the DAM persistent volume under the dx-dam-media folder. Also we should transfer manually all DAM collections resources permissions. Via XMLAccess we can extract all DAM collections resources permissions. For this export we should use the /opt/HCL/PortalServer/doc/xml-samples/ExportAllDamCollections.xml file to do that. And the generated XML output should be used to import it on the HELM deployment side. If all DAM data's are transferred successful, we need to adjust the stored references to WCM assets in WCM. The references are stored as fully qualified URLs. For this problem exists solution to set a alternate hostname. WCM DAM alternate hostname","title":"Workaround to solve it:"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#backup-and-recover","text":"The following documentation provided a manual backup and recovery way. But it looks that the documentation is not in a good shape. https://help.hcltechsw.com/digital-experience/9.5/containerization/backup_and_recovery_procedures.html https://support.hcltechsw.com/community?id=community_question&sys_id=9e73a02e1b20b850f37655352a4bcbaa","title":"Backup and recover"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#summary","text":"Migration from operator-based to helm-based deployments will only be possible with exactly one DX version (presumable CF198). Both deployments have to be at the same version level. Moving Kubernetes configurations : We will only provide documentation on how to extract configurations from an existing operator-based deployment using dxctl and how to interpret these. We will also document how a customer would be able to reflect these in their custom values.yaml for their helm-based deployment. For the DX Core migration the best approach is to copy the wp_profile from the operator-based deployment to the helm-based deployment. We expect other connected infrastructures (e.g. LDAP) to remain unchanged, hence no configuration changes are necessary. DX Core database should just stay the same, hence no further changes would be necessary after copying the WAS profile. The old database needs to be reachable from the new helm-based deployment. If a customer needs to change the connected database, he can use database transfer tooling to move database content between the database instances. For DX DAM the DAM EXIM tool would be the preferred way. Unfortunately the EXIM implementation is not finished yet. Therefore the DAM move between an operator-based and a helm-based deployment will need to be done using Postgres database export/import and a copy of the DAM's binary storage. If EXIM gets finished in time, we can consider changing the documentation appropriately.","title":"Summary"},{"location":"kube/Migration/k8s-next-property-migration/","text":"Introduction In order to migrate from Operator based deployments to Helm, it might be necessary that you migrate the deployment configuration of your old Operator deployment first. This page will provide you with a mapping that allows you to re-use values from your old deployment.properties file in your new custom-values.yaml . Preparation Ensure that you have followed the preparation process for a fresh Helm deployment, including creation of your custom-values.yaml . You will need the properties file that you used with DXCTL to perform your old Operator deployment. If you do not have that property file at hand, you can refer to the DXCTL documentation on how to use the getProperties function of DXCTL to extract a properties file from your existing deployment. Property mappings This chapter will go through relevant properties that can be found in a DXCTL property file and how they may be mapped to the new custom-values.yaml . Please note: You should only transfer settings that you have adjusted for your Operator deployment. It is not recommended to overwrite all Helm defaults with the defaults of the old Operator deployment. Only migrate settings that are relevant for you or have been adjusted by you prior deploying the Operator with DXCTL. General properties DXCTL property values.yaml key Description dx.namespace n/a Namespace used for the deployment, will be handed to helm directly via CLI dx.name n/a Deployment name, will be handed to helm directly via CLI dx.pullpolicy images.pullPolicy Determines the image pull policy for all container images dx.pod.nodeselector nodeSelector.* NodeSelector used for Pods, can now be done per application in Helm dx.config.authoring configuration.core.tuning.authoring Selects if the instance should be tuned for authoring or not composer.enabled applications.contentComposer Selects if Content Composer will be deployed or not dam.enabled applications.digitalAssetManagement Selects if Digital Asset Management will be deployed or not persist.force-read n/a Read-only fallback enablement, always enabled in Helm Storage properties DXCTL property values.yaml key Description dx.volume volumes.core.profile.volumeName Name of the volume to be used for DX Core Profile dx.volume.size volumes.core.profile.requests.storage Size of the volume to be used for DX Core Profile dx.storageclass volumes.core.profile.storageClassName StorageClass of the volume used for DX Core Profile dx.splitlogging: false n/a Determines if log directory uses a separate volume, always enabled in Helm dx.logging.stgclass volumes.core.log.storageClassName StorageClass for DX Core logging volume dx.logging.size volumes.core.log.requests.storage StorageClass for DX Core logging volume dx.tranlogging n/a Determines if the transaction log directory uses a separate volume, always enabled in Helm dx.tranlogging.reclaim n/a Reclaimpolicy for DX Core transaction log volume. Determined by PV instead of Helm dx.tranlogging.stgclass volumes.core.tranlog.storageClassName StorageClass for the DX Core transaction log volume dx.tranlogging.size volumes.core.tranlog.requests.storage Size used for the DX Core transaction log volume remote.search.volume volumes.remoteSearch.prsprofile.volumeName Name of the volume used for the DX Remote Search profile remote.search.stgclass volumes.remoteSearch.prsprofile.storageClassName StorageClass of the volume for the DX Remote Search profile dam.volume volumes.digitalAssetManagement.binaries.volumeName Name of the volume used for DAM dam.stgclass volumes.digitalAssetManagement.binaries.storageClassName StorageClass of the volume used for DAM Networking properties DXCTL property values.yaml key Description dx.path.contextroot networking.core.contextRoot Context root used for DX dx.path.personalized networking.core.personalizedHome Personalized URL path for DX dx.path.home networking.core.home Non personalized URL path for DX dx.deploy.host.override networking.core.host Hostname that should be used instead of the Loadbalancer hostname dx.deploy.host.override.force n/a Force the use of the override host, obsolete in helm dx.config.cors networking.addon.*.corsOrigin CORS configuration for applications, can be configured per application in Helm hybrid.enabled n/a Determines if hybrid is enabled or not, Helm will derive this from other networking and application settings hybrid.url networking.core.host URL of the DX Core instance in a hybrid deployment hybrid.port networking.core.port Port of the DX Core instance in a hybrid deployment Scaling properties Core DXCTL property values.yaml key Description dx.minreplicas scaling.horizontalPodAutoScaler.core.minReplicas Minimum amount of Pods when scaling is enabled dx.maxreplicas scaling.horizontalPodAutoScaler.core.maxReplicas Maximum amount of Pods when scaling is enabled dx.replicas scaling.replicas.core Default amount of Pods when scaling is disabled dx.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.core.targetCPUUtilizationPercentage CPU Target for autoscaling dx.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.core.targetMemoryUtilizationPercentage Memory Target for autoscaling Ring API DXCTL property values.yaml key Description api.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.ringApi.targetCPUUtilizationPercentage CPU Target for autoscaling api.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.ringApi.targetMemoryUtilizationPercentage Memory Target for autoscaling api.minreplicas scaling.horizontalPodAutoScaler.ringApi.minReplicas Minimum amount of Pods when scaling is enabled api.maxreplicas scaling.horizontalPodAutoScaler.ringApi.maxReplicas Maximum amount of Pods when scaling is enabled Content Composer DXCTL property values.yaml key Description composer.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.contentComposer.targetCPUUtilizationPercentage CPU Target for autoscaling composer.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.contentComposer.targetMemoryUtilizationPercentage Memory Target for autoscaling composer.minreplicas scaling.horizontalPodAutoScaler.contentComposer.minReplicas Minimum amount of Pods when scaling is enabled composer.maxreplicas scaling.horizontalPodAutoScaler.contentComposer.maxReplicas Maximum amount of Pods when scaling is enabled Digital Asset Management DXCTL property values.yaml key Description dam.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.digitalAssetManagement.targetCPUUtilizationPercentage CPU Target for autoscaling dam.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.digitalAssetManagement.targetMemoryUtilizationPercentage Memory Target for autoscaling dam.minreplicas scaling.horizontalPodAutoScaler.digitalAssetManagement.minReplicas Minimum amount of Pods when scaling is enabled dam.maxreplicas scaling.horizontalPodAutoScaler.digitalAssetManagement.maxReplicas Maximum amount of Pods when scaling is enabled Image Processor DXCTL property values.yaml key Description imgproc.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.imageProcessor.targetCPUUtilizationPercentage CPU Target for autoscaling imgproc.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.imageProcessor.targetMemoryUtilizationPercentage Memory Target for autoscaling imgproc.minreplicas scaling.horizontalPodAutoScaler.imageProcessor.minReplicas Minimum amount of Pods when scaling is enabled imgproc.maxreplicas scaling.horizontalPodAutoScaler.imageProcessor.maxReplicas Maximum amount of Pods when scaling is enabled Resource allocation properties Core DXCTL property values.yaml key Description dx.request.cpu resources.core.requests.cpu CPU Request dx.request.memory resources.core.requests.memory Memory Request dx.limit.cpu resources.core.limits.cpu CPU Limit dx.limit.memory resources.core.limits.memory Memory Limit Ring API DXCTL property values.yaml key Description api.request.cpu resources.ringApi.requests.cpu CPU Request api.request.memory resources.ringApi.requests.memory Memory Request api.limit.cpu resources.ringApi.limits.cpu CPU Limit api.limit.memory resources.ringApi.limits.memory Memory Limit Content Composer DXCTL property values.yaml key Description composer.request.cpu resources.contentComposer.requests.cpu CPU Request composer.request.memory resources.contentComposer.requests.memory Memory Request composer.limit.cpu resources.contentComposer.limits.cpu CPU Limit composer.limit.memory resources.contentComposer.limits.memory Memory Limit Digital Asset Management DXCTL property values.yaml key Description dam.request.cpu resources.digitalAssetManagement.requests.cpu CPU Request dam.request.memory resources.digitalAssetManagement.requests.memory Memory Request dam.limit.cpu resources.digitalAssetManagement.limits.cpu CPU Limit dam.limit.memory resources.digitalAssetManagement.limits.memory Memory Limit Persistence DXCTL property values.yaml key Description persist.request.cpu resources.persistence.requests.cpu CPU Request persist.request.memory resources.persistence.requests.memory Memory Request persist.limit.cpu resources.persistence.limits.cpu CPU Limit persist.limit.memory resources.persistence.limits.memory Memory Limit Image Processor DXCTL property values.yaml key Description imgproc.request.cpu resources.imageProcessor.requests.cpu CPU Request imgproc.request.memory resources.imageProcessor.requests.memory Memory Request imgproc.limit.cpu resources.imageProcessor.limits.cpu CPU Limit imgproc.limit.memory resources.imageProcessor.limits.memory Memory Limit","title":"Operator property migration"},{"location":"kube/Migration/k8s-next-property-migration/#introduction","text":"In order to migrate from Operator based deployments to Helm, it might be necessary that you migrate the deployment configuration of your old Operator deployment first. This page will provide you with a mapping that allows you to re-use values from your old deployment.properties file in your new custom-values.yaml .","title":"Introduction"},{"location":"kube/Migration/k8s-next-property-migration/#preparation","text":"Ensure that you have followed the preparation process for a fresh Helm deployment, including creation of your custom-values.yaml . You will need the properties file that you used with DXCTL to perform your old Operator deployment. If you do not have that property file at hand, you can refer to the DXCTL documentation on how to use the getProperties function of DXCTL to extract a properties file from your existing deployment.","title":"Preparation"},{"location":"kube/Migration/k8s-next-property-migration/#property-mappings","text":"This chapter will go through relevant properties that can be found in a DXCTL property file and how they may be mapped to the new custom-values.yaml . Please note: You should only transfer settings that you have adjusted for your Operator deployment. It is not recommended to overwrite all Helm defaults with the defaults of the old Operator deployment. Only migrate settings that are relevant for you or have been adjusted by you prior deploying the Operator with DXCTL.","title":"Property mappings"},{"location":"kube/Migration/k8s-next-property-migration/#general-properties","text":"DXCTL property values.yaml key Description dx.namespace n/a Namespace used for the deployment, will be handed to helm directly via CLI dx.name n/a Deployment name, will be handed to helm directly via CLI dx.pullpolicy images.pullPolicy Determines the image pull policy for all container images dx.pod.nodeselector nodeSelector.* NodeSelector used for Pods, can now be done per application in Helm dx.config.authoring configuration.core.tuning.authoring Selects if the instance should be tuned for authoring or not composer.enabled applications.contentComposer Selects if Content Composer will be deployed or not dam.enabled applications.digitalAssetManagement Selects if Digital Asset Management will be deployed or not persist.force-read n/a Read-only fallback enablement, always enabled in Helm","title":"General properties"},{"location":"kube/Migration/k8s-next-property-migration/#storage-properties","text":"DXCTL property values.yaml key Description dx.volume volumes.core.profile.volumeName Name of the volume to be used for DX Core Profile dx.volume.size volumes.core.profile.requests.storage Size of the volume to be used for DX Core Profile dx.storageclass volumes.core.profile.storageClassName StorageClass of the volume used for DX Core Profile dx.splitlogging: false n/a Determines if log directory uses a separate volume, always enabled in Helm dx.logging.stgclass volumes.core.log.storageClassName StorageClass for DX Core logging volume dx.logging.size volumes.core.log.requests.storage StorageClass for DX Core logging volume dx.tranlogging n/a Determines if the transaction log directory uses a separate volume, always enabled in Helm dx.tranlogging.reclaim n/a Reclaimpolicy for DX Core transaction log volume. Determined by PV instead of Helm dx.tranlogging.stgclass volumes.core.tranlog.storageClassName StorageClass for the DX Core transaction log volume dx.tranlogging.size volumes.core.tranlog.requests.storage Size used for the DX Core transaction log volume remote.search.volume volumes.remoteSearch.prsprofile.volumeName Name of the volume used for the DX Remote Search profile remote.search.stgclass volumes.remoteSearch.prsprofile.storageClassName StorageClass of the volume for the DX Remote Search profile dam.volume volumes.digitalAssetManagement.binaries.volumeName Name of the volume used for DAM dam.stgclass volumes.digitalAssetManagement.binaries.storageClassName StorageClass of the volume used for DAM","title":"Storage properties"},{"location":"kube/Migration/k8s-next-property-migration/#networking-properties","text":"DXCTL property values.yaml key Description dx.path.contextroot networking.core.contextRoot Context root used for DX dx.path.personalized networking.core.personalizedHome Personalized URL path for DX dx.path.home networking.core.home Non personalized URL path for DX dx.deploy.host.override networking.core.host Hostname that should be used instead of the Loadbalancer hostname dx.deploy.host.override.force n/a Force the use of the override host, obsolete in helm dx.config.cors networking.addon.*.corsOrigin CORS configuration for applications, can be configured per application in Helm hybrid.enabled n/a Determines if hybrid is enabled or not, Helm will derive this from other networking and application settings hybrid.url networking.core.host URL of the DX Core instance in a hybrid deployment hybrid.port networking.core.port Port of the DX Core instance in a hybrid deployment","title":"Networking properties"},{"location":"kube/Migration/k8s-next-property-migration/#scaling-properties","text":"","title":"Scaling properties"},{"location":"kube/Migration/k8s-next-property-migration/#core","text":"DXCTL property values.yaml key Description dx.minreplicas scaling.horizontalPodAutoScaler.core.minReplicas Minimum amount of Pods when scaling is enabled dx.maxreplicas scaling.horizontalPodAutoScaler.core.maxReplicas Maximum amount of Pods when scaling is enabled dx.replicas scaling.replicas.core Default amount of Pods when scaling is disabled dx.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.core.targetCPUUtilizationPercentage CPU Target for autoscaling dx.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.core.targetMemoryUtilizationPercentage Memory Target for autoscaling","title":"Core"},{"location":"kube/Migration/k8s-next-property-migration/#ring-api","text":"DXCTL property values.yaml key Description api.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.ringApi.targetCPUUtilizationPercentage CPU Target for autoscaling api.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.ringApi.targetMemoryUtilizationPercentage Memory Target for autoscaling api.minreplicas scaling.horizontalPodAutoScaler.ringApi.minReplicas Minimum amount of Pods when scaling is enabled api.maxreplicas scaling.horizontalPodAutoScaler.ringApi.maxReplicas Maximum amount of Pods when scaling is enabled","title":"Ring API"},{"location":"kube/Migration/k8s-next-property-migration/#content-composer","text":"DXCTL property values.yaml key Description composer.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.contentComposer.targetCPUUtilizationPercentage CPU Target for autoscaling composer.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.contentComposer.targetMemoryUtilizationPercentage Memory Target for autoscaling composer.minreplicas scaling.horizontalPodAutoScaler.contentComposer.minReplicas Minimum amount of Pods when scaling is enabled composer.maxreplicas scaling.horizontalPodAutoScaler.contentComposer.maxReplicas Maximum amount of Pods when scaling is enabled","title":"Content Composer"},{"location":"kube/Migration/k8s-next-property-migration/#digital-asset-management","text":"DXCTL property values.yaml key Description dam.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.digitalAssetManagement.targetCPUUtilizationPercentage CPU Target for autoscaling dam.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.digitalAssetManagement.targetMemoryUtilizationPercentage Memory Target for autoscaling dam.minreplicas scaling.horizontalPodAutoScaler.digitalAssetManagement.minReplicas Minimum amount of Pods when scaling is enabled dam.maxreplicas scaling.horizontalPodAutoScaler.digitalAssetManagement.maxReplicas Maximum amount of Pods when scaling is enabled","title":"Digital Asset Management"},{"location":"kube/Migration/k8s-next-property-migration/#image-processor","text":"DXCTL property values.yaml key Description imgproc.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.imageProcessor.targetCPUUtilizationPercentage CPU Target for autoscaling imgproc.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.imageProcessor.targetMemoryUtilizationPercentage Memory Target for autoscaling imgproc.minreplicas scaling.horizontalPodAutoScaler.imageProcessor.minReplicas Minimum amount of Pods when scaling is enabled imgproc.maxreplicas scaling.horizontalPodAutoScaler.imageProcessor.maxReplicas Maximum amount of Pods when scaling is enabled","title":"Image Processor"},{"location":"kube/Migration/k8s-next-property-migration/#resource-allocation-properties","text":"","title":"Resource allocation properties"},{"location":"kube/Migration/k8s-next-property-migration/#core_1","text":"DXCTL property values.yaml key Description dx.request.cpu resources.core.requests.cpu CPU Request dx.request.memory resources.core.requests.memory Memory Request dx.limit.cpu resources.core.limits.cpu CPU Limit dx.limit.memory resources.core.limits.memory Memory Limit","title":"Core"},{"location":"kube/Migration/k8s-next-property-migration/#ring-api_1","text":"DXCTL property values.yaml key Description api.request.cpu resources.ringApi.requests.cpu CPU Request api.request.memory resources.ringApi.requests.memory Memory Request api.limit.cpu resources.ringApi.limits.cpu CPU Limit api.limit.memory resources.ringApi.limits.memory Memory Limit","title":"Ring API"},{"location":"kube/Migration/k8s-next-property-migration/#content-composer_1","text":"DXCTL property values.yaml key Description composer.request.cpu resources.contentComposer.requests.cpu CPU Request composer.request.memory resources.contentComposer.requests.memory Memory Request composer.limit.cpu resources.contentComposer.limits.cpu CPU Limit composer.limit.memory resources.contentComposer.limits.memory Memory Limit","title":"Content Composer"},{"location":"kube/Migration/k8s-next-property-migration/#digital-asset-management_1","text":"DXCTL property values.yaml key Description dam.request.cpu resources.digitalAssetManagement.requests.cpu CPU Request dam.request.memory resources.digitalAssetManagement.requests.memory Memory Request dam.limit.cpu resources.digitalAssetManagement.limits.cpu CPU Limit dam.limit.memory resources.digitalAssetManagement.limits.memory Memory Limit","title":"Digital Asset Management"},{"location":"kube/Migration/k8s-next-property-migration/#persistence","text":"DXCTL property values.yaml key Description persist.request.cpu resources.persistence.requests.cpu CPU Request persist.request.memory resources.persistence.requests.memory Memory Request persist.limit.cpu resources.persistence.limits.cpu CPU Limit persist.limit.memory resources.persistence.limits.memory Memory Limit","title":"Persistence"},{"location":"kube/Migration/k8s-next-property-migration/#image-processor_1","text":"DXCTL property values.yaml key Description imgproc.request.cpu resources.imageProcessor.requests.cpu CPU Request imgproc.request.memory resources.imageProcessor.requests.memory Memory Request imgproc.limit.cpu resources.imageProcessor.limits.cpu CPU Limit imgproc.limit.memory resources.imageProcessor.limits.memory Memory Limit","title":"Image Processor"},{"location":"kube/Prometheus/k8s-next-nodejs-prom/","text":"PoC Goals The goal is to evaluate how we can expose Prometheus compatible metrics in our NodeJS applications and expose metrics via the main NodeJS process of one of our applications. The outcome of that PoC should be documented so it can be reused for a thorough implementation in our NodeJS applications as required. For this PoC, the DAM is used as an example application. Due to the similar structure of all skeleton based NodeJS projects, the steps shown here can be re-used in other applications like RingAPI as well. Prepare application to expose metrics Install prom-client to the server package We use the package prom-client to generate Prometheus compatible metrics. This package exposes NodeJS default metrics and is also able to expose custom metrics if desired. cd packages/server-v1 npm install --save prom-client After the install is successful, the prom-client can be used. Expose metrics endpoint The prom-client itself is only creating a Prometheus compatible data structure, but not exposing those via any HTTP endpoint. Therefore, we create a metrics probe endpoint at the path /probe/metrics . Since application metrics and logging are mostly infrastructure related information, choosing this path appeared appropriate. We create the file /packages/server-v1/src/probes/metrics.probe.ts with the following contents: /* ******************************************************************** * Licensed Materials - Property of HCL * * * * Copyright HCL Technologies Ltd. 2021. All Rights Reserved. * * * * Note to US Government Users Restricted Rights: * * * * Use, duplication or disclosure restricted by GSA ADP Schedule * ******************************************************************** */ import { NextFunction, Request, Response } from 'express'; import * as client from 'prom-client'; import { loggerFactory } from '@enchanted-prod/logger'; const logger = loggerFactory(); client.collectDefaultMetrics({ gcDurationBuckets: [0.001, 0.01, 0.1, 1, 2, 5], // These are the default buckets. }); /* * This probe provides prometheus consumable metrics. */ export class MetricsProbe { public metricsProbeHandler = async (req: Request, res: Response, next: NextFunction) => { logger.debug('Metric Probe is called.'); const reg = client.register; const defaultMetrics = await reg.metrics(); return res.send(defaultMetrics).status(200); } } After that, we adjust the file /packages/server-v1/src/probes/index.js to also include the metrics endpoint. /* ******************************************************************** * Licensed Materials - Property of HCL * * * * Copyright HCL Technologies Ltd. 2021. All Rights Reserved. * * * * Note to US Government Users Restricted Rights: * * * * Use, duplication or disclosure restricted by GSA ADP Schedule * ******************************************************************** */ export * from './ready.probe'; export * from './live.probe'; export * from './metrics.probe'; With that configured, we are now able to enable the endpoint inside the /packages/server-v1/src/server.ts where the other probes are already in place: const readyProbe = new ReadyProbe(this.lbApp, this.operationScheduler); this.app.use('/probe/ready', readyProbe.readyProbeHandler); const liveProbe = new LiveProbe(); this.app.use('/probe/live', liveProbe.liveProbeHandler); // Expose the metrics probe endpoint const metricsProbe = new MetricsProbe(); this.app.use('/probe/metrics', metricsProbe.metricsProbeHandler) const staticUi = new StaticUi(); With that configured, the DAM Pod will now expose metrics data at /probe/metrics . Add custom metrics To expose custom metrics, we can leverage prom-client again and add custom metrics like counters or gauges. The example adds the gauge values dam_active_workers and dam_free_workers as a custom metric. Therefore we add two custom gauges in the file /packages/server-v1/src/operations/scheduler.ts . const activeWorkerGauge = new client.Gauge({ name: 'dam_active_workers', help: 'Active workers handling operations.' }); const freeWorkerGauge = new client.Gauge({ name: 'dam_free_workers', help: 'Free workers ready for operations.' }); It contains the name of the metric, as well as a text description that will be shown as a help for the metrics values. Whenever the count of active or free workers change, we need to update the previously created gauge by setting the new value. activeWorkerGauge.set(this.activeWorkers.workers.length); freeWorkerGauge.set(this.freeWorkers.workers.length); In case of DAM, we enhance the activeWorkers and freeWorkers arrays to trigger the update of the gauges themselves whenever workers are added to or removed from it. Deploying DAM with NodeJS metrics enabled in Kubernetes For building the image and pushing it to artifactory, a build pipeline for DAM has been used on PJT. Deploy DX including the DAM image with metrics The DX deployment used in this PoC is running on k3s locally and with a minimal configuration. The Helm Charts used are hcl-dx-deployment-v2.0.0_20210716-1545_rohan_develop.tgz . Only DAM, Core, Persistence, Image Processor, RingAPI and Ambassador are deployed, with a minimal request set of resources. The following custom values are being used for deployment: #******************************************************************** #* Licensed Materials - Property of HCL * #* * #* Copyright HCL Technologies Ltd. 2021. All Rights Reserved. * #* * #* Note to US Government Users Restricted Rights: * #* * #* Use, duplication or disclosure restricted by GSA ADP Schedule * #******************************************************************** # Prometheus DAM PoC values, smallest deployment # Image related configuration images: # Container repository used to retrieve the images repository: \"quintana-docker.artifactory.cwp.pnp-hcl.com/dx-build-output\" # Image tag for each application tags: core: \"v95_CF197_20210802-055523_rohan_develop_6107af8e\" digitalAssetManagement: \"v1.9.0_20210802-1721_pjd_feature_DXQ-16661-prom-exporter-poc\" imageProcessor: \"v1.10.0_20210721-1401_rohan_release_95_CF197\" persistence: \"v1.10.0_20210727-1300_rohan_release_95_CF197\" ringApi: \"v1.10.0_20210726-1106_rohan_develop\" ambassadorIngress: \"1.5.4\" ambassadorRedis: \"5.0.1\" # Image name for each application names: core: \"core/dxen\" digitalAssetManagement: \"core-addon/media-library\" imageProcessor: \"core-addon/image-processor\" persistence: \"core-addon/persistence/postgres\" ringApi: \"core-addon/api/ringapi\" ambassadorIngress: \"common/ambassador\" ambassadorRedis: \"common/redis\" # Resource allocation settings, definition per pod # Use number + unit, e.g. 1500m for CPU or 1500M for Memory resources: # Content composer resource allocation contentComposer: requests: cpu: \"100m\" memory: \"128Mi\" # Core resource allocation core: requests: cpu: \"1000m\" memory: \"3072Mi\" # Design Studio resource allocation designStudio: requests: cpu: \"100m\" memory: \"128Mi\" # Digital asset management resource allocation digitalAssetManagement: requests: cpu: \"250m\" memory: \"1G\" # Image processor resource allocation imageProcessor: requests: cpu: \"100m\" memory: \"1280Mi\" # Open LDAP resource allocation openLdap: requests: cpu: \"200m\" memory: \"512Mi\" # Persistence resource allocation persistence: requests: cpu: \"250m\" memory: \"512Mi\" # Remote Search resource allocation remoteSearch: requests: cpu: \"500m\" memory: \"768Mi\" # Ring API resource allocation ringApi: requests: cpu: \"100m\" memory: \"128Mi\" # Ambassador ingress resource allocation ambassadorIngress: requests: cpu: \"200m\" memory: \"300Mi\" # Ambassador Redis resource allocation ambassadorRedis: requests: cpu: \"100m\" memory: \"256Mi\" # Runtime Controller resource allocation runtimeController: requests: cpu: \"100m\" memory: \"256Mi\" applications: contentComposer: false core: true designStudio: false digitalAssetManagement: true imageProcessor: true openLdap: false persistence: true remoteSearch: false ringApi: true ambassador: true runtimeController: false Install Prometheus To install prometheus, we use Helm. Add Prometheus Helm Chart Repo to Helm: helm repo add prometheus-community https://prometheus-community.github.io/helm-charts We can extract the default values via Helm: helm show values prometheus-community/prometheus > prom-values.yaml We'll use the following values for a simple PoC deployment of Prometheus, disabling persistence and additional services. serviceAccounts: alertmanager: create: false nodeExporter: create: false alertmanager: enabled: false nodeExporter: enabled: false server: enabled: true persistentVolume: enabled: false service: type: NodePort pushgateway: enabled: false Install the Prometheus Application: helm install prometheus prometheus-community/prometheus -n prom -f prom-values.yaml Find the NodePort that is used and access Prometheus: NODEPORT=$(kubectl get -o jsonpath=\"{.spec.ports[0].nodePort}\" services prometheus-server -n prom) echo $NODEPORT You can now access Prometheus using your Browser: http://<NODE_IP>:<NODE_PORT> Adjust DAM Pods to be scraped by Prometheus Add the following annotations to the DAM Pods in the StatefulSet to have Prometheus automatically scrape the metrics endpoint. kubectl edit StatefulSet dx-digital-asset-management -n prom spec: template: metadata: annotations: prometheus.io/scrape: \"true\" prometheus.io/path: \"/probe/metrics\" prometheus.io/port: \"3000\" The DAM Pod will be restarted shortly and Prometheus should scrape the DAM Pod. Configure a test dashboard for DAM You can use the metrics dam_active_workers and dam_free_workers . Here is a sample JSON which can be used: { \"annotations\": { \"list\": [ { \"builtIn\": 1, \"datasource\": \"-- Grafana --\", \"enable\": true, \"hide\": true, \"iconColor\": \"rgba(0, 211, 255, 1)\", \"name\": \"Annotations & Alerts\", \"type\": \"dashboard\" } ] }, \"editable\": true, \"gnetId\": null, \"graphTooltip\": 0, \"id\": null, \"links\": [], \"panels\": [ { \"datasource\": null, \"fieldConfig\": { \"defaults\": { \"color\": { \"mode\": \"palette-classic\" }, \"custom\": { \"axisLabel\": \"\", \"axisPlacement\": \"auto\", \"barAlignment\": 0, \"drawStyle\": \"line\", \"fillOpacity\": 0, \"gradientMode\": \"none\", \"hideFrom\": { \"legend\": false, \"tooltip\": false, \"viz\": false }, \"lineInterpolation\": \"linear\", \"lineWidth\": 1, \"pointSize\": 5, \"scaleDistribution\": { \"type\": \"linear\" }, \"showPoints\": \"auto\", \"spanNulls\": false, \"stacking\": { \"group\": \"A\", \"mode\": \"none\" }, \"thresholdsStyle\": { \"mode\": \"off\" } }, \"mappings\": [], \"thresholds\": { \"mode\": \"absolute\", \"steps\": [ { \"color\": \"green\", \"value\": null }, { \"color\": \"red\", \"value\": 80 } ] } }, \"overrides\": [ { \"__systemRef\": \"hideSeriesFrom\", \"matcher\": { \"id\": \"byNames\", \"options\": { \"mode\": \"exclude\", \"names\": [ \"dam_free_workers{app=\\\"dx-digital-asset-management\\\", app_kubernetes_io_instance=\\\"dx\\\", app_kubernetes_io_managed_by=\\\"Helm\\\", app_kubernetes_io_name=\\\"hcl-dx-deployment\\\", app_kubernetes_io_version=\\\"95_CF197\\\", controller_revision_hash=\\\"dx-digital-asset-management-554ccc9d7b\\\", helm_sh_chart=\\\"hcl-dx-deployment-2.0.0\\\", instance=\\\"10.42.0.75:3000\\\", job=\\\"kubernetes-pods\\\", kubernetes_namespace=\\\"prom\\\", kubernetes_pod_name=\\\"dx-digital-asset-management-0\\\", release=\\\"dx\\\", statefulset_kubernetes_io_pod_name=\\\"dx-digital-asset-management-0\\\"}\" ], \"prefix\": \"All except:\", \"readOnly\": true } }, \"properties\": [ { \"id\": \"custom.hideFrom\", \"value\": { \"legend\": false, \"tooltip\": false, \"viz\": true } } ] } ] }, \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 0 }, \"id\": 4, \"options\": { \"legend\": { \"calcs\": [], \"displayMode\": \"list\", \"placement\": \"bottom\" }, \"tooltip\": { \"mode\": \"single\" } }, \"targets\": [ { \"exemplar\": true, \"expr\": \"dam_free_workers\", \"interval\": \"\", \"legendFormat\": \"\", \"refId\": \"A\" } ], \"title\": \"DAM Free Workers\", \"type\": \"timeseries\" }, { \"datasource\": null, \"description\": \"\", \"fieldConfig\": { \"defaults\": { \"color\": { \"mode\": \"palette-classic\" }, \"custom\": { \"axisLabel\": \"\", \"axisPlacement\": \"auto\", \"barAlignment\": 0, \"drawStyle\": \"line\", \"fillOpacity\": 0, \"gradientMode\": \"none\", \"hideFrom\": { \"legend\": false, \"tooltip\": false, \"viz\": false }, \"lineInterpolation\": \"linear\", \"lineWidth\": 1, \"pointSize\": 5, \"scaleDistribution\": { \"type\": \"linear\" }, \"showPoints\": \"auto\", \"spanNulls\": false, \"stacking\": { \"group\": \"A\", \"mode\": \"none\" }, \"thresholdsStyle\": { \"mode\": \"off\" } }, \"mappings\": [], \"thresholds\": { \"mode\": \"absolute\", \"steps\": [ { \"color\": \"green\", \"value\": null }, { \"color\": \"red\", \"value\": 80 } ] } }, \"overrides\": [] }, \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 8 }, \"id\": 2, \"options\": { \"legend\": { \"calcs\": [], \"displayMode\": \"list\", \"placement\": \"bottom\" }, \"tooltip\": { \"mode\": \"single\" } }, \"targets\": [ { \"exemplar\": true, \"expr\": \"dam_active_workers\", \"interval\": \"\", \"legendFormat\": \"\", \"refId\": \"A\" } ], \"title\": \"DAM Active Workers\", \"type\": \"timeseries\" } ], \"refresh\": \"10s\", \"schemaVersion\": 30, \"style\": \"dark\", \"tags\": [], \"templating\": { \"list\": [] }, \"time\": { \"from\": \"now-15m\", \"to\": \"now\" }, \"timepicker\": {}, \"timezone\": \"\", \"title\": \"New dashboard\", \"uid\": null, \"version\": 0 } The output looks like this: Using existing Grafana Dashboards There are also existing Grafana dashboards that can be leveraged. One that is related to NodeJS and prom-client can be found here Grafana Dashboard 11159 . Conclusion The use of prom-client enables us to easily expose default NodeJS metrics as well as custom applications metrics that are bound to our application logic. The implementation is straight forward and the main effort lies within defining metrics that should be exposed. We will need to adjust our Helm charts to add the necessary annotations to all application Pods that expose metrics for prometheus. With that, prometheus will automatically scrape those applications and aggregate the metrics data. The NodeJS default metrics also allow for in-depth analysis on performance issues, that can be seen in monitoring of the NodeJS runtime.","title":"NodeJS Metrics"},{"location":"kube/Prometheus/k8s-next-nodejs-prom/#poc-goals","text":"The goal is to evaluate how we can expose Prometheus compatible metrics in our NodeJS applications and expose metrics via the main NodeJS process of one of our applications. The outcome of that PoC should be documented so it can be reused for a thorough implementation in our NodeJS applications as required. For this PoC, the DAM is used as an example application. Due to the similar structure of all skeleton based NodeJS projects, the steps shown here can be re-used in other applications like RingAPI as well.","title":"PoC Goals"},{"location":"kube/Prometheus/k8s-next-nodejs-prom/#prepare-application-to-expose-metrics","text":"","title":"Prepare application to expose metrics"},{"location":"kube/Prometheus/k8s-next-nodejs-prom/#install-prom-client-to-the-server-package","text":"We use the package prom-client to generate Prometheus compatible metrics. This package exposes NodeJS default metrics and is also able to expose custom metrics if desired. cd packages/server-v1 npm install --save prom-client After the install is successful, the prom-client can be used.","title":"Install prom-client to the server package"},{"location":"kube/Prometheus/k8s-next-nodejs-prom/#expose-metrics-endpoint","text":"The prom-client itself is only creating a Prometheus compatible data structure, but not exposing those via any HTTP endpoint. Therefore, we create a metrics probe endpoint at the path /probe/metrics . Since application metrics and logging are mostly infrastructure related information, choosing this path appeared appropriate. We create the file /packages/server-v1/src/probes/metrics.probe.ts with the following contents: /* ******************************************************************** * Licensed Materials - Property of HCL * * * * Copyright HCL Technologies Ltd. 2021. All Rights Reserved. * * * * Note to US Government Users Restricted Rights: * * * * Use, duplication or disclosure restricted by GSA ADP Schedule * ******************************************************************** */ import { NextFunction, Request, Response } from 'express'; import * as client from 'prom-client'; import { loggerFactory } from '@enchanted-prod/logger'; const logger = loggerFactory(); client.collectDefaultMetrics({ gcDurationBuckets: [0.001, 0.01, 0.1, 1, 2, 5], // These are the default buckets. }); /* * This probe provides prometheus consumable metrics. */ export class MetricsProbe { public metricsProbeHandler = async (req: Request, res: Response, next: NextFunction) => { logger.debug('Metric Probe is called.'); const reg = client.register; const defaultMetrics = await reg.metrics(); return res.send(defaultMetrics).status(200); } } After that, we adjust the file /packages/server-v1/src/probes/index.js to also include the metrics endpoint. /* ******************************************************************** * Licensed Materials - Property of HCL * * * * Copyright HCL Technologies Ltd. 2021. All Rights Reserved. * * * * Note to US Government Users Restricted Rights: * * * * Use, duplication or disclosure restricted by GSA ADP Schedule * ******************************************************************** */ export * from './ready.probe'; export * from './live.probe'; export * from './metrics.probe'; With that configured, we are now able to enable the endpoint inside the /packages/server-v1/src/server.ts where the other probes are already in place: const readyProbe = new ReadyProbe(this.lbApp, this.operationScheduler); this.app.use('/probe/ready', readyProbe.readyProbeHandler); const liveProbe = new LiveProbe(); this.app.use('/probe/live', liveProbe.liveProbeHandler); // Expose the metrics probe endpoint const metricsProbe = new MetricsProbe(); this.app.use('/probe/metrics', metricsProbe.metricsProbeHandler) const staticUi = new StaticUi(); With that configured, the DAM Pod will now expose metrics data at /probe/metrics .","title":"Expose metrics endpoint"},{"location":"kube/Prometheus/k8s-next-nodejs-prom/#add-custom-metrics","text":"To expose custom metrics, we can leverage prom-client again and add custom metrics like counters or gauges. The example adds the gauge values dam_active_workers and dam_free_workers as a custom metric. Therefore we add two custom gauges in the file /packages/server-v1/src/operations/scheduler.ts . const activeWorkerGauge = new client.Gauge({ name: 'dam_active_workers', help: 'Active workers handling operations.' }); const freeWorkerGauge = new client.Gauge({ name: 'dam_free_workers', help: 'Free workers ready for operations.' }); It contains the name of the metric, as well as a text description that will be shown as a help for the metrics values. Whenever the count of active or free workers change, we need to update the previously created gauge by setting the new value. activeWorkerGauge.set(this.activeWorkers.workers.length); freeWorkerGauge.set(this.freeWorkers.workers.length); In case of DAM, we enhance the activeWorkers and freeWorkers arrays to trigger the update of the gauges themselves whenever workers are added to or removed from it.","title":"Add custom metrics"},{"location":"kube/Prometheus/k8s-next-nodejs-prom/#deploying-dam-with-nodejs-metrics-enabled-in-kubernetes","text":"For building the image and pushing it to artifactory, a build pipeline for DAM has been used on PJT.","title":"Deploying DAM with NodeJS metrics enabled in Kubernetes"},{"location":"kube/Prometheus/k8s-next-nodejs-prom/#deploy-dx-including-the-dam-image-with-metrics","text":"The DX deployment used in this PoC is running on k3s locally and with a minimal configuration. The Helm Charts used are hcl-dx-deployment-v2.0.0_20210716-1545_rohan_develop.tgz . Only DAM, Core, Persistence, Image Processor, RingAPI and Ambassador are deployed, with a minimal request set of resources. The following custom values are being used for deployment: #******************************************************************** #* Licensed Materials - Property of HCL * #* * #* Copyright HCL Technologies Ltd. 2021. All Rights Reserved. * #* * #* Note to US Government Users Restricted Rights: * #* * #* Use, duplication or disclosure restricted by GSA ADP Schedule * #******************************************************************** # Prometheus DAM PoC values, smallest deployment # Image related configuration images: # Container repository used to retrieve the images repository: \"quintana-docker.artifactory.cwp.pnp-hcl.com/dx-build-output\" # Image tag for each application tags: core: \"v95_CF197_20210802-055523_rohan_develop_6107af8e\" digitalAssetManagement: \"v1.9.0_20210802-1721_pjd_feature_DXQ-16661-prom-exporter-poc\" imageProcessor: \"v1.10.0_20210721-1401_rohan_release_95_CF197\" persistence: \"v1.10.0_20210727-1300_rohan_release_95_CF197\" ringApi: \"v1.10.0_20210726-1106_rohan_develop\" ambassadorIngress: \"1.5.4\" ambassadorRedis: \"5.0.1\" # Image name for each application names: core: \"core/dxen\" digitalAssetManagement: \"core-addon/media-library\" imageProcessor: \"core-addon/image-processor\" persistence: \"core-addon/persistence/postgres\" ringApi: \"core-addon/api/ringapi\" ambassadorIngress: \"common/ambassador\" ambassadorRedis: \"common/redis\" # Resource allocation settings, definition per pod # Use number + unit, e.g. 1500m for CPU or 1500M for Memory resources: # Content composer resource allocation contentComposer: requests: cpu: \"100m\" memory: \"128Mi\" # Core resource allocation core: requests: cpu: \"1000m\" memory: \"3072Mi\" # Design Studio resource allocation designStudio: requests: cpu: \"100m\" memory: \"128Mi\" # Digital asset management resource allocation digitalAssetManagement: requests: cpu: \"250m\" memory: \"1G\" # Image processor resource allocation imageProcessor: requests: cpu: \"100m\" memory: \"1280Mi\" # Open LDAP resource allocation openLdap: requests: cpu: \"200m\" memory: \"512Mi\" # Persistence resource allocation persistence: requests: cpu: \"250m\" memory: \"512Mi\" # Remote Search resource allocation remoteSearch: requests: cpu: \"500m\" memory: \"768Mi\" # Ring API resource allocation ringApi: requests: cpu: \"100m\" memory: \"128Mi\" # Ambassador ingress resource allocation ambassadorIngress: requests: cpu: \"200m\" memory: \"300Mi\" # Ambassador Redis resource allocation ambassadorRedis: requests: cpu: \"100m\" memory: \"256Mi\" # Runtime Controller resource allocation runtimeController: requests: cpu: \"100m\" memory: \"256Mi\" applications: contentComposer: false core: true designStudio: false digitalAssetManagement: true imageProcessor: true openLdap: false persistence: true remoteSearch: false ringApi: true ambassador: true runtimeController: false","title":"Deploy DX including the DAM image with metrics"},{"location":"kube/Prometheus/k8s-next-nodejs-prom/#install-prometheus","text":"To install prometheus, we use Helm. Add Prometheus Helm Chart Repo to Helm: helm repo add prometheus-community https://prometheus-community.github.io/helm-charts We can extract the default values via Helm: helm show values prometheus-community/prometheus > prom-values.yaml We'll use the following values for a simple PoC deployment of Prometheus, disabling persistence and additional services. serviceAccounts: alertmanager: create: false nodeExporter: create: false alertmanager: enabled: false nodeExporter: enabled: false server: enabled: true persistentVolume: enabled: false service: type: NodePort pushgateway: enabled: false Install the Prometheus Application: helm install prometheus prometheus-community/prometheus -n prom -f prom-values.yaml Find the NodePort that is used and access Prometheus: NODEPORT=$(kubectl get -o jsonpath=\"{.spec.ports[0].nodePort}\" services prometheus-server -n prom) echo $NODEPORT You can now access Prometheus using your Browser: http://<NODE_IP>:<NODE_PORT>","title":"Install Prometheus"},{"location":"kube/Prometheus/k8s-next-nodejs-prom/#adjust-dam-pods-to-be-scraped-by-prometheus","text":"Add the following annotations to the DAM Pods in the StatefulSet to have Prometheus automatically scrape the metrics endpoint. kubectl edit StatefulSet dx-digital-asset-management -n prom spec: template: metadata: annotations: prometheus.io/scrape: \"true\" prometheus.io/path: \"/probe/metrics\" prometheus.io/port: \"3000\" The DAM Pod will be restarted shortly and Prometheus should scrape the DAM Pod.","title":"Adjust DAM Pods to be scraped by Prometheus"},{"location":"kube/Prometheus/k8s-next-nodejs-prom/#configure-a-test-dashboard-for-dam","text":"You can use the metrics dam_active_workers and dam_free_workers . Here is a sample JSON which can be used: { \"annotations\": { \"list\": [ { \"builtIn\": 1, \"datasource\": \"-- Grafana --\", \"enable\": true, \"hide\": true, \"iconColor\": \"rgba(0, 211, 255, 1)\", \"name\": \"Annotations & Alerts\", \"type\": \"dashboard\" } ] }, \"editable\": true, \"gnetId\": null, \"graphTooltip\": 0, \"id\": null, \"links\": [], \"panels\": [ { \"datasource\": null, \"fieldConfig\": { \"defaults\": { \"color\": { \"mode\": \"palette-classic\" }, \"custom\": { \"axisLabel\": \"\", \"axisPlacement\": \"auto\", \"barAlignment\": 0, \"drawStyle\": \"line\", \"fillOpacity\": 0, \"gradientMode\": \"none\", \"hideFrom\": { \"legend\": false, \"tooltip\": false, \"viz\": false }, \"lineInterpolation\": \"linear\", \"lineWidth\": 1, \"pointSize\": 5, \"scaleDistribution\": { \"type\": \"linear\" }, \"showPoints\": \"auto\", \"spanNulls\": false, \"stacking\": { \"group\": \"A\", \"mode\": \"none\" }, \"thresholdsStyle\": { \"mode\": \"off\" } }, \"mappings\": [], \"thresholds\": { \"mode\": \"absolute\", \"steps\": [ { \"color\": \"green\", \"value\": null }, { \"color\": \"red\", \"value\": 80 } ] } }, \"overrides\": [ { \"__systemRef\": \"hideSeriesFrom\", \"matcher\": { \"id\": \"byNames\", \"options\": { \"mode\": \"exclude\", \"names\": [ \"dam_free_workers{app=\\\"dx-digital-asset-management\\\", app_kubernetes_io_instance=\\\"dx\\\", app_kubernetes_io_managed_by=\\\"Helm\\\", app_kubernetes_io_name=\\\"hcl-dx-deployment\\\", app_kubernetes_io_version=\\\"95_CF197\\\", controller_revision_hash=\\\"dx-digital-asset-management-554ccc9d7b\\\", helm_sh_chart=\\\"hcl-dx-deployment-2.0.0\\\", instance=\\\"10.42.0.75:3000\\\", job=\\\"kubernetes-pods\\\", kubernetes_namespace=\\\"prom\\\", kubernetes_pod_name=\\\"dx-digital-asset-management-0\\\", release=\\\"dx\\\", statefulset_kubernetes_io_pod_name=\\\"dx-digital-asset-management-0\\\"}\" ], \"prefix\": \"All except:\", \"readOnly\": true } }, \"properties\": [ { \"id\": \"custom.hideFrom\", \"value\": { \"legend\": false, \"tooltip\": false, \"viz\": true } } ] } ] }, \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 0 }, \"id\": 4, \"options\": { \"legend\": { \"calcs\": [], \"displayMode\": \"list\", \"placement\": \"bottom\" }, \"tooltip\": { \"mode\": \"single\" } }, \"targets\": [ { \"exemplar\": true, \"expr\": \"dam_free_workers\", \"interval\": \"\", \"legendFormat\": \"\", \"refId\": \"A\" } ], \"title\": \"DAM Free Workers\", \"type\": \"timeseries\" }, { \"datasource\": null, \"description\": \"\", \"fieldConfig\": { \"defaults\": { \"color\": { \"mode\": \"palette-classic\" }, \"custom\": { \"axisLabel\": \"\", \"axisPlacement\": \"auto\", \"barAlignment\": 0, \"drawStyle\": \"line\", \"fillOpacity\": 0, \"gradientMode\": \"none\", \"hideFrom\": { \"legend\": false, \"tooltip\": false, \"viz\": false }, \"lineInterpolation\": \"linear\", \"lineWidth\": 1, \"pointSize\": 5, \"scaleDistribution\": { \"type\": \"linear\" }, \"showPoints\": \"auto\", \"spanNulls\": false, \"stacking\": { \"group\": \"A\", \"mode\": \"none\" }, \"thresholdsStyle\": { \"mode\": \"off\" } }, \"mappings\": [], \"thresholds\": { \"mode\": \"absolute\", \"steps\": [ { \"color\": \"green\", \"value\": null }, { \"color\": \"red\", \"value\": 80 } ] } }, \"overrides\": [] }, \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 8 }, \"id\": 2, \"options\": { \"legend\": { \"calcs\": [], \"displayMode\": \"list\", \"placement\": \"bottom\" }, \"tooltip\": { \"mode\": \"single\" } }, \"targets\": [ { \"exemplar\": true, \"expr\": \"dam_active_workers\", \"interval\": \"\", \"legendFormat\": \"\", \"refId\": \"A\" } ], \"title\": \"DAM Active Workers\", \"type\": \"timeseries\" } ], \"refresh\": \"10s\", \"schemaVersion\": 30, \"style\": \"dark\", \"tags\": [], \"templating\": { \"list\": [] }, \"time\": { \"from\": \"now-15m\", \"to\": \"now\" }, \"timepicker\": {}, \"timezone\": \"\", \"title\": \"New dashboard\", \"uid\": null, \"version\": 0 } The output looks like this:","title":"Configure a test dashboard for DAM"},{"location":"kube/Prometheus/k8s-next-nodejs-prom/#using-existing-grafana-dashboards","text":"There are also existing Grafana dashboards that can be leveraged. One that is related to NodeJS and prom-client can be found here Grafana Dashboard 11159 .","title":"Using existing Grafana Dashboards"},{"location":"kube/Prometheus/k8s-next-nodejs-prom/#conclusion","text":"The use of prom-client enables us to easily expose default NodeJS metrics as well as custom applications metrics that are bound to our application logic. The implementation is straight forward and the main effort lies within defining metrics that should be exposed. We will need to adjust our Helm charts to add the necessary annotations to all application Pods that expose metrics for prometheus. With that, prometheus will automatically scrape those applications and aggregate the metrics data. The NodeJS default metrics also allow for in-depth analysis on performance issues, that can be seen in monitoring of the NodeJS runtime.","title":"Conclusion"},{"location":"kube/Prometheus/k8s-next-postgres-prom/","text":"PoC Goals The goal is to evaluate how we can expose Prometheus compatible metrics in our Postgres persistence. The outcome of that PoC should be documented so it can be reused for a thorough implementation in our Helm deployment. The PoC will use the new persistence deployment with Pgpool and Repmgr (DX internally called \"dbHA\" ), but the described pattern can be used for the \"old\" persistence as well. Postgres Metrics A Postgres Prometheus exporter exists as a prebuilt Docker image as well as on Github . It connects to the database and exposes the Prometheus compatible metrics as an HTTP endpoint. The default metrics are described in the queries.yaml file. Custom metrics can be defined as a YAML file and attached when the exporter is started by using the extend.query-path flag . Adding a sidecar container to the Postgres Pod The preferred way to run the Exporter is to add it to the Postgres Pod as a sidecar container. The configuration below describes the basic setup using the public postgres-exporter image. spec: template: metadata: annotations: # Tell Prometheus to scrape the metrics and where to find them prometheus.io/path: \"/metrics\" prometheus.io/port: \"9187\" prometheus.io/scrape: \"true\" spec: containers: - name: \"persistence-node-metrics\" image: \"quay.io/prometheuscommunity/postgres-exporter\" env: - name: DATA_SOURCE_URI # If implemented like that, we should check fo a way to not hard-code the \"dxmediadb\" value: \"127.0.0.1:5432/dxmediadb?sslmode=disable\" - name: \"DATA_SOURCE_PASS\" valueFrom: secretKeyRef: key: \"password\" name: \"{{ .Release.Name }}-persistence-user\" - name: \"DATA_SOURCE_USER\" valueFrom: secretKeyRef: key: \"username\" name: \"{{ .Release.Name }}-persistence-user\" ports: - name: metrics containerPort: 9187 protocol: TCP Create Postgres Exporter image based on the UBI image When shipped as part of DX, the Postgres Exporter should be included as a separate image, based on the UBI image. We can use the published releases from Github and add it to the UBI image. # #################################################################### # Licensed Materials - Property of HCL # # # # Copyright HCL Technologies Ltd. 2021. All Rights Reserved. # # # # Note to US Government Users Restricted Rights: # # # # Use, duplication or disclosure restricted by GSA ADP Schedule # #################################################################### # ARG REPOSITORY_URL=\"quintana-docker-prod.artifactory.cwp.pnp-hcl.com\" ARG DOCKER_UBI_BASE_IMAGE=\"dx-build-output/common/dxubi:v1.0.0_8.4-205\" FROM $REPOSITORY_URL/$DOCKER_UBI_BASE_IMAGE ARG BUILD_LABEL ARG VERSION LABEL \"product\"=\"HCL Digital Experience Postgres exporter\" LABEL \"version\"=\"${VERSION}\" LABEL \"description\"=\"DX postgres exporter container\" LABEL \"io.k8s.description\"=\"DX postgres exporter container\" LABEL \"io.k8s.display-name\"=\"DX postgres exporter container\" LABEL \"summary\"=\"DX postgres exporter container\" LABEL \"name\"=\"dx-postgresql-exporter\" LABEL \"release\"=\"${BUILD_LABEL}\" LABEL \"maintainer\"=\"HCL Software\" LABEL \"vendor\"=\"HCL Software\" LABEL \"io.openshift.tags\"=\"hcl dx\" LABEL \"url\"=\"\" LABEL \"authoritative-source-url\"=\"\" MAINTAINER HCL Software RUN curl -LJO https://github.com/prometheus-community/postgres_exporter/releases/download/v0.10.0/postgres_exporter-0.10.0.linux-amd64.tar.gz && \\ tar -xvf postgres_exporter-0.10.0.linux-amd64.tar.gz && \\ rm postgres_exporter-0.10.0.linux-amd64.tar.gz && \\ mv postgres_exporter-0.10.0.linux-amd64/postgres_exporter /usr/bin/postgres_exporter && \\ rm -r postgres_exporter-0.10.0.linux-amd64 USER dx_user:dx_users EXPOSE 9187 ENTRYPOINT [\"/usr/bin/postgres_exporter\"] Pgpool Metrics Similar to the Postgres exporter, a docker image exists to export the metrics of Pgpool. The code and releases can be found on Github . The exposed metrics are described on the same page. As we are currently using Pgpool in version 4.1, some of the metrics are not available for the exporter. To use it to it's full potential, an upgrade of Pgpool to version 4.2 should be considered. Adding a sidecar container to the Pgpool Pod The preferred way to run the Exporter is to add it to the Pgpool Pod as a sidecar container. The configuration below describes the basic setup using the public pgpool-exporter image. spec: template: metadata: annotations: # Tell Prometheus to scrape the metrics and where to find them prometheus.io/path: \"/metrics\" prometheus.io/port: \"9719\" prometheus.io/scrape: \"true\" spec: containers: - name: \"persistence-connection-pool-stats\" image: pgpool/pgpool2_exporter env: - name: \"POSTGRES_USERNAME\" valueFrom: secretKeyRef: key: \"username\" name: \"{{ .Release.Name }}-persistence-user\" - name: \"POSTGRES_PASSWORD\" valueFrom: secretKeyRef: key: \"password\" name: \"{{ .Release.Name }}-persistence-user\" - name: PGPOOL_SERVICE value: \"localhost\" - name: PGPOOL_SERVICE_PORT value: \"5432\" ports: - name: metrics containerPort: 9719 protocol: TCP Create Pgpool Exporter image based on the UBI image When shipped as part of DX, the Pgpool Exporter should be included as a separate image, based on the UBI image. We can use the published releases from Github and add it to the UBI image. # #################################################################### # Licensed Materials - Property of HCL # # # # Copyright HCL Technologies Ltd. 2021. All Rights Reserved. # # # # Note to US Government Users Restricted Rights: # # # # Use, duplication or disclosure restricted by GSA ADP Schedule # #################################################################### # ARG REPOSITORY_URL=\"quintana-docker-prod.artifactory.cwp.pnp-hcl.com\" ARG DOCKER_UBI_BASE_IMAGE=\"dx-build-output/common/dxubi:v1.0.0_8.4-205\" FROM $REPOSITORY_URL/$DOCKER_UBI_BASE_IMAGE ARG BUILD_LABEL ARG VERSION LABEL \"product\"=\"HCL Digital Experience Pgpool exporter\" LABEL \"version\"=\"${VERSION}\" LABEL \"description\"=\"DX pgpool exporter container\" LABEL \"io.k8s.description\"=\"DX pgpool exporter container\" LABEL \"io.k8s.display-name\"=\"DX pgpool exporter container\" LABEL \"summary\"=\"DX pgpool exporter container\" LABEL \"name\"=\"dx-pgpool-exporter\" LABEL \"release\"=\"${BUILD_LABEL}\" LABEL \"maintainer\"=\"HCL Software\" LABEL \"vendor\"=\"HCL Software\" LABEL \"io.openshift.tags\"=\"hcl dx\" LABEL \"url\"=\"\" LABEL \"authoritative-source-url\"=\"\" MAINTAINER HCL Software ENV POSTGRES_USERNAME postgres ENV POSTGRES_PASSWORD postgres ENV PGPOOL_SERVICE localhost ENV PGPOOL_SERVICE_PORT 9999 RUN curl -LJO https://github.com/pgpool/pgpool2_exporter/releases/download/v1.0.0/pgpool2_exporter-1.0.0.linux-amd64.tar.gz && \\ tar -xvf pgpool2_exporter-1.0.0.linux-amd64.tar.gz && \\ rm pgpool2_exporter-1.0.0.linux-amd64.tar.gz && \\ mv pgpool2_exporter-1.0.0.linux-amd64/pgpool2_exporter /usr/bin/pgpool2_exporter && \\ rm -r pgpool2_exporter-1.0.0.linux-amd64 USER dx_user:dx_users EXPOSE 9719 ENTRYPOINT [\"/bin/sh\", \"-c\", \"export DATA_SOURCE_NAME=\\\"postgresql://${POSTGRES_USERNAME}:${POSTGRES_PASSWORD}@${PGPOOL_SERVICE}:${PGPOOL_SERVICE_PORT}/postgres?sslmode=disable\\\"; /usr/bin/pgpool2_exporter\"] Using existing Grafana Dashboards There are also existing Grafana dashboards that can be leveraged. One that is related to Postgres with Prometheuse can be found here Grafana Dashboard 9628 . For the Pgpool exporter, a predefined Dashboard is not available in the official Grafana Dashboard directory.","title":"Postgres Metrics"},{"location":"kube/Prometheus/k8s-next-postgres-prom/#poc-goals","text":"The goal is to evaluate how we can expose Prometheus compatible metrics in our Postgres persistence. The outcome of that PoC should be documented so it can be reused for a thorough implementation in our Helm deployment. The PoC will use the new persistence deployment with Pgpool and Repmgr (DX internally called \"dbHA\" ), but the described pattern can be used for the \"old\" persistence as well.","title":"PoC Goals"},{"location":"kube/Prometheus/k8s-next-postgres-prom/#postgres-metrics","text":"A Postgres Prometheus exporter exists as a prebuilt Docker image as well as on Github . It connects to the database and exposes the Prometheus compatible metrics as an HTTP endpoint. The default metrics are described in the queries.yaml file. Custom metrics can be defined as a YAML file and attached when the exporter is started by using the extend.query-path flag .","title":"Postgres Metrics"},{"location":"kube/Prometheus/k8s-next-postgres-prom/#adding-a-sidecar-container-to-the-postgres-pod","text":"The preferred way to run the Exporter is to add it to the Postgres Pod as a sidecar container. The configuration below describes the basic setup using the public postgres-exporter image. spec: template: metadata: annotations: # Tell Prometheus to scrape the metrics and where to find them prometheus.io/path: \"/metrics\" prometheus.io/port: \"9187\" prometheus.io/scrape: \"true\" spec: containers: - name: \"persistence-node-metrics\" image: \"quay.io/prometheuscommunity/postgres-exporter\" env: - name: DATA_SOURCE_URI # If implemented like that, we should check fo a way to not hard-code the \"dxmediadb\" value: \"127.0.0.1:5432/dxmediadb?sslmode=disable\" - name: \"DATA_SOURCE_PASS\" valueFrom: secretKeyRef: key: \"password\" name: \"{{ .Release.Name }}-persistence-user\" - name: \"DATA_SOURCE_USER\" valueFrom: secretKeyRef: key: \"username\" name: \"{{ .Release.Name }}-persistence-user\" ports: - name: metrics containerPort: 9187 protocol: TCP","title":"Adding a sidecar container to the Postgres Pod"},{"location":"kube/Prometheus/k8s-next-postgres-prom/#create-postgres-exporter-image-based-on-the-ubi-image","text":"When shipped as part of DX, the Postgres Exporter should be included as a separate image, based on the UBI image. We can use the published releases from Github and add it to the UBI image. # #################################################################### # Licensed Materials - Property of HCL # # # # Copyright HCL Technologies Ltd. 2021. All Rights Reserved. # # # # Note to US Government Users Restricted Rights: # # # # Use, duplication or disclosure restricted by GSA ADP Schedule # #################################################################### # ARG REPOSITORY_URL=\"quintana-docker-prod.artifactory.cwp.pnp-hcl.com\" ARG DOCKER_UBI_BASE_IMAGE=\"dx-build-output/common/dxubi:v1.0.0_8.4-205\" FROM $REPOSITORY_URL/$DOCKER_UBI_BASE_IMAGE ARG BUILD_LABEL ARG VERSION LABEL \"product\"=\"HCL Digital Experience Postgres exporter\" LABEL \"version\"=\"${VERSION}\" LABEL \"description\"=\"DX postgres exporter container\" LABEL \"io.k8s.description\"=\"DX postgres exporter container\" LABEL \"io.k8s.display-name\"=\"DX postgres exporter container\" LABEL \"summary\"=\"DX postgres exporter container\" LABEL \"name\"=\"dx-postgresql-exporter\" LABEL \"release\"=\"${BUILD_LABEL}\" LABEL \"maintainer\"=\"HCL Software\" LABEL \"vendor\"=\"HCL Software\" LABEL \"io.openshift.tags\"=\"hcl dx\" LABEL \"url\"=\"\" LABEL \"authoritative-source-url\"=\"\" MAINTAINER HCL Software RUN curl -LJO https://github.com/prometheus-community/postgres_exporter/releases/download/v0.10.0/postgres_exporter-0.10.0.linux-amd64.tar.gz && \\ tar -xvf postgres_exporter-0.10.0.linux-amd64.tar.gz && \\ rm postgres_exporter-0.10.0.linux-amd64.tar.gz && \\ mv postgres_exporter-0.10.0.linux-amd64/postgres_exporter /usr/bin/postgres_exporter && \\ rm -r postgres_exporter-0.10.0.linux-amd64 USER dx_user:dx_users EXPOSE 9187 ENTRYPOINT [\"/usr/bin/postgres_exporter\"]","title":"Create Postgres Exporter image based on the UBI image"},{"location":"kube/Prometheus/k8s-next-postgres-prom/#pgpool-metrics","text":"Similar to the Postgres exporter, a docker image exists to export the metrics of Pgpool. The code and releases can be found on Github . The exposed metrics are described on the same page. As we are currently using Pgpool in version 4.1, some of the metrics are not available for the exporter. To use it to it's full potential, an upgrade of Pgpool to version 4.2 should be considered.","title":"Pgpool Metrics"},{"location":"kube/Prometheus/k8s-next-postgres-prom/#adding-a-sidecar-container-to-the-pgpool-pod","text":"The preferred way to run the Exporter is to add it to the Pgpool Pod as a sidecar container. The configuration below describes the basic setup using the public pgpool-exporter image. spec: template: metadata: annotations: # Tell Prometheus to scrape the metrics and where to find them prometheus.io/path: \"/metrics\" prometheus.io/port: \"9719\" prometheus.io/scrape: \"true\" spec: containers: - name: \"persistence-connection-pool-stats\" image: pgpool/pgpool2_exporter env: - name: \"POSTGRES_USERNAME\" valueFrom: secretKeyRef: key: \"username\" name: \"{{ .Release.Name }}-persistence-user\" - name: \"POSTGRES_PASSWORD\" valueFrom: secretKeyRef: key: \"password\" name: \"{{ .Release.Name }}-persistence-user\" - name: PGPOOL_SERVICE value: \"localhost\" - name: PGPOOL_SERVICE_PORT value: \"5432\" ports: - name: metrics containerPort: 9719 protocol: TCP","title":"Adding a sidecar container to the Pgpool Pod"},{"location":"kube/Prometheus/k8s-next-postgres-prom/#create-pgpool-exporter-image-based-on-the-ubi-image","text":"When shipped as part of DX, the Pgpool Exporter should be included as a separate image, based on the UBI image. We can use the published releases from Github and add it to the UBI image. # #################################################################### # Licensed Materials - Property of HCL # # # # Copyright HCL Technologies Ltd. 2021. All Rights Reserved. # # # # Note to US Government Users Restricted Rights: # # # # Use, duplication or disclosure restricted by GSA ADP Schedule # #################################################################### # ARG REPOSITORY_URL=\"quintana-docker-prod.artifactory.cwp.pnp-hcl.com\" ARG DOCKER_UBI_BASE_IMAGE=\"dx-build-output/common/dxubi:v1.0.0_8.4-205\" FROM $REPOSITORY_URL/$DOCKER_UBI_BASE_IMAGE ARG BUILD_LABEL ARG VERSION LABEL \"product\"=\"HCL Digital Experience Pgpool exporter\" LABEL \"version\"=\"${VERSION}\" LABEL \"description\"=\"DX pgpool exporter container\" LABEL \"io.k8s.description\"=\"DX pgpool exporter container\" LABEL \"io.k8s.display-name\"=\"DX pgpool exporter container\" LABEL \"summary\"=\"DX pgpool exporter container\" LABEL \"name\"=\"dx-pgpool-exporter\" LABEL \"release\"=\"${BUILD_LABEL}\" LABEL \"maintainer\"=\"HCL Software\" LABEL \"vendor\"=\"HCL Software\" LABEL \"io.openshift.tags\"=\"hcl dx\" LABEL \"url\"=\"\" LABEL \"authoritative-source-url\"=\"\" MAINTAINER HCL Software ENV POSTGRES_USERNAME postgres ENV POSTGRES_PASSWORD postgres ENV PGPOOL_SERVICE localhost ENV PGPOOL_SERVICE_PORT 9999 RUN curl -LJO https://github.com/pgpool/pgpool2_exporter/releases/download/v1.0.0/pgpool2_exporter-1.0.0.linux-amd64.tar.gz && \\ tar -xvf pgpool2_exporter-1.0.0.linux-amd64.tar.gz && \\ rm pgpool2_exporter-1.0.0.linux-amd64.tar.gz && \\ mv pgpool2_exporter-1.0.0.linux-amd64/pgpool2_exporter /usr/bin/pgpool2_exporter && \\ rm -r pgpool2_exporter-1.0.0.linux-amd64 USER dx_user:dx_users EXPOSE 9719 ENTRYPOINT [\"/bin/sh\", \"-c\", \"export DATA_SOURCE_NAME=\\\"postgresql://${POSTGRES_USERNAME}:${POSTGRES_PASSWORD}@${PGPOOL_SERVICE}:${PGPOOL_SERVICE_PORT}/postgres?sslmode=disable\\\"; /usr/bin/pgpool2_exporter\"]","title":"Create Pgpool Exporter image based on the UBI image"},{"location":"kube/Prometheus/k8s-next-postgres-prom/#using-existing-grafana-dashboards","text":"There are also existing Grafana dashboards that can be leveraged. One that is related to Postgres with Prometheuse can be found here Grafana Dashboard 9628 . For the Pgpool exporter, a predefined Dashboard is not available in the official Grafana Dashboard directory.","title":"Using existing Grafana Dashboards"},{"location":"kube/Prometheus/k8s-next-prom/","text":"Prometheus is data aggregation tool that is widely used, not only for Kubernetes deployments, but monitoring in general. Prometheus can be configured to pull data from defined data-sources. That pull process is called scraping and happens in configurable intervals. The service/application exposing metrics for Prometheus can often be found under the term exporter , as it exports the values of specific metrics for Prometheus. How is Prometheus scraping data structured? The data format of Prometheus scraping data is simple, it consists of key/value pairs that will be consumed by Prometheus. Those data values can have different types, e.g. Counter or Gauges. There is a good documentation on how exporter data is structured: Prometheus exporter . Install Prometheus and Grafana There are two ways of installing Prometheus and Grafana. Please follow EITHER 1 OR 2: 1. Install Prometheus Operator and Grafana stack The following process describes the installation of the kube-prometheus-stack helm chart that includes: The Prometheus Operator Highly available Prometheus Highly available Alertmanager Prometheus node-exporter Prometheus Adapter for Kubernetes Metrics APIs kube-state-metrics Grafana This helm chart is based on the kube-prometheus repository, which collects: [...] Kubernetes manifests, Grafana dashboards, and Prometheus rules combined with documentation and scripts to provide easy to operate end-to-end Kubernetes cluster monitoring with Prometheus using the Prometheus Operator. This stack comes with a set of tools to monitor the Kubernetes cluster as well as pre-installed Grafana dashboards for visualization. Add the prometheus-community repository to Helm. helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update Then deploy the chart with helm install . helm install -n <namespace> <release name> prometheus-community/kube-prometheus-stack -f <custom-values.yaml> Example: helm install -n dxns prometheus-stack prometheus-community/kube-prometheus-stack -f custom-kube-prometheus-stack.yaml The components of the helm chart can be configured or disabled by adjusting the custom helm values. For a full set of values that can be configured, please follow the configuration section in the repository of the helm chart. grafana: adminPassword: \"prom-operator\" service: type: \"NodePort\" nodePort: 32767 # range 30000-32767 prometheus: service: type: \"NodePort\" nodePort: 32766 # range 30000-32767 prometheusSpec: serviceMonitorSelectorNilUsesHelmValues: false To access and test Prometheus and Grafana, we expose them on two ports and assign an admin password for Grafana. Setting the serviceMonitorSelectorNilUsesHelmValues parameter to false makes sure that the ServiceMonitors are discovered by Prometheus. 2. Install Prometheus (non-operator) To install prometheus, we use Helm. Add Prometheus Helm Chart Repo to Helm: helm repo add prometheus-community https://prometheus-community.github.io/helm-charts We can extract the default values via Helm: helm show values prometheus-community/prometheus > prom-values.yaml We'll use the following values for a simple PoC deployment of Prometheus, disabling persistence and additional services. serviceAccounts: alertmanager: create: false nodeExporter: create: false alertmanager: enabled: false nodeExporter: enabled: false server: enabled: true persistentVolume: enabled: false service: type: NodePort pushgateway: enabled: false Install the Prometheus Application: helm install prometheus prometheus-community/prometheus -n prom -f prom-values.yaml Find the NodePort that is used and access Prometheus: NODEPORT=$(kubectl get -o jsonpath=\"{.spec.ports[0].nodePort}\" services prometheus-server -n prom) echo $NODEPORT You can now access Prometheus using your Browser: http://<NODE_IP>:<NODE_PORT> Deploy Grafana Add Grafana repository to Helm. helm repo add grafana https://grafana.github.io/helm-charts We can extract the default values via Helm: helm show values grafana/grafana > grafana-values.yaml Use the following custom values to configure the Service as NodePort. Persistence is disabled per default. service: enabled: true type: NodePort port: 80 targetPort: 3000 portName: service Install Grafana: helm install grafana -n prom grafana/grafana -f grafana-values.yaml Get the NodePort of Grafana: NODEPORT=$(kubectl get -o jsonpath=\"{.spec.ports[0].nodePort}\" services grafana -n prom) echo $NODEPORT Get the adminn password: kubectl get secret --namespace prom grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echos You can now login with the user admin and the returned password. http://<NODE_IP>:<NODE_PORT> Configure a datasource Navigate to Configuration and then go to DataSources . Click on Add data source and select Prometheus . Add the server url http://prometheus-server . Click on Save & Test . Using existing Grafana Dashboards There are existing Grafana dashboards that can be leveraged. They can be found on the Grafana Website DX metrics in this setup To scrape and visualize the DX metrics in this setup, make sure all applications are configured in the custom-values.yaml with either: For Prometheus Operator yaml scrape: true prometheusDiscoveryType: \"serviceMonitor\" For Prometheus (non-operator) yaml scrape: true prometheusDiscoveryType: \"annotation\"","title":"Prometheus"},{"location":"kube/Prometheus/k8s-next-prom/#how-is-prometheus-scraping-data-structured","text":"The data format of Prometheus scraping data is simple, it consists of key/value pairs that will be consumed by Prometheus. Those data values can have different types, e.g. Counter or Gauges. There is a good documentation on how exporter data is structured: Prometheus exporter .","title":"How is Prometheus scraping data structured?"},{"location":"kube/Prometheus/k8s-next-prom/#install-prometheus-and-grafana","text":"There are two ways of installing Prometheus and Grafana. Please follow EITHER 1 OR 2:","title":"Install Prometheus and Grafana"},{"location":"kube/Prometheus/k8s-next-prom/#1-install-prometheus-operator-and-grafana-stack","text":"The following process describes the installation of the kube-prometheus-stack helm chart that includes: The Prometheus Operator Highly available Prometheus Highly available Alertmanager Prometheus node-exporter Prometheus Adapter for Kubernetes Metrics APIs kube-state-metrics Grafana This helm chart is based on the kube-prometheus repository, which collects: [...] Kubernetes manifests, Grafana dashboards, and Prometheus rules combined with documentation and scripts to provide easy to operate end-to-end Kubernetes cluster monitoring with Prometheus using the Prometheus Operator. This stack comes with a set of tools to monitor the Kubernetes cluster as well as pre-installed Grafana dashboards for visualization. Add the prometheus-community repository to Helm. helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update Then deploy the chart with helm install . helm install -n <namespace> <release name> prometheus-community/kube-prometheus-stack -f <custom-values.yaml> Example: helm install -n dxns prometheus-stack prometheus-community/kube-prometheus-stack -f custom-kube-prometheus-stack.yaml The components of the helm chart can be configured or disabled by adjusting the custom helm values. For a full set of values that can be configured, please follow the configuration section in the repository of the helm chart. grafana: adminPassword: \"prom-operator\" service: type: \"NodePort\" nodePort: 32767 # range 30000-32767 prometheus: service: type: \"NodePort\" nodePort: 32766 # range 30000-32767 prometheusSpec: serviceMonitorSelectorNilUsesHelmValues: false To access and test Prometheus and Grafana, we expose them on two ports and assign an admin password for Grafana. Setting the serviceMonitorSelectorNilUsesHelmValues parameter to false makes sure that the ServiceMonitors are discovered by Prometheus.","title":"1. Install Prometheus Operator and Grafana stack"},{"location":"kube/Prometheus/k8s-next-prom/#2-install-prometheus-non-operator","text":"To install prometheus, we use Helm. Add Prometheus Helm Chart Repo to Helm: helm repo add prometheus-community https://prometheus-community.github.io/helm-charts We can extract the default values via Helm: helm show values prometheus-community/prometheus > prom-values.yaml We'll use the following values for a simple PoC deployment of Prometheus, disabling persistence and additional services. serviceAccounts: alertmanager: create: false nodeExporter: create: false alertmanager: enabled: false nodeExporter: enabled: false server: enabled: true persistentVolume: enabled: false service: type: NodePort pushgateway: enabled: false Install the Prometheus Application: helm install prometheus prometheus-community/prometheus -n prom -f prom-values.yaml Find the NodePort that is used and access Prometheus: NODEPORT=$(kubectl get -o jsonpath=\"{.spec.ports[0].nodePort}\" services prometheus-server -n prom) echo $NODEPORT You can now access Prometheus using your Browser: http://<NODE_IP>:<NODE_PORT>","title":"2. Install Prometheus (non-operator)"},{"location":"kube/Prometheus/k8s-next-prom/#deploy-grafana","text":"Add Grafana repository to Helm. helm repo add grafana https://grafana.github.io/helm-charts We can extract the default values via Helm: helm show values grafana/grafana > grafana-values.yaml Use the following custom values to configure the Service as NodePort. Persistence is disabled per default. service: enabled: true type: NodePort port: 80 targetPort: 3000 portName: service Install Grafana: helm install grafana -n prom grafana/grafana -f grafana-values.yaml Get the NodePort of Grafana: NODEPORT=$(kubectl get -o jsonpath=\"{.spec.ports[0].nodePort}\" services grafana -n prom) echo $NODEPORT Get the adminn password: kubectl get secret --namespace prom grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echos You can now login with the user admin and the returned password. http://<NODE_IP>:<NODE_PORT>","title":"Deploy Grafana"},{"location":"kube/Prometheus/k8s-next-prom/#configure-a-datasource","text":"Navigate to Configuration and then go to DataSources . Click on Add data source and select Prometheus . Add the server url http://prometheus-server . Click on Save & Test .","title":"Configure a datasource"},{"location":"kube/Prometheus/k8s-next-prom/#using-existing-grafana-dashboards","text":"There are existing Grafana dashboards that can be leveraged. They can be found on the Grafana Website","title":"Using existing Grafana Dashboards"},{"location":"kube/Prometheus/k8s-next-prom/#dx-metrics-in-this-setup","text":"To scrape and visualize the DX metrics in this setup, make sure all applications are configured in the custom-values.yaml with either: For Prometheus Operator yaml scrape: true prometheusDiscoveryType: \"serviceMonitor\" For Prometheus (non-operator) yaml scrape: true prometheusDiscoveryType: \"annotation\"","title":"DX metrics in this setup"},{"location":"what%27s-new/new_cf17/","text":"What's new in CF17? Combined Cumulative Fix (CF17) includes new software fixes for the latest version of HCL Digital Experience. Go to the HCL Software Support Site for the list of software fixes for HCL Digital Experience. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. Also, see the following link for Portal maintenance guidance: HCL Digital Experience Roadmap: Applying maintenance Parent topic: Latest Combined CF and Container updates","title":"What's new in CF17?"},{"location":"what%27s-new/new_cf17/#whats-new-in-cf17","text":"Combined Cumulative Fix (CF17) includes new software fixes for the latest version of HCL Digital Experience. Go to the HCL Software Support Site for the list of software fixes for HCL Digital Experience. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. Also, see the following link for Portal maintenance guidance: HCL Digital Experience Roadmap: Applying maintenance Parent topic: Latest Combined CF and Container updates","title":"What's new in CF17?"},{"location":"what%27s-new/new_cf171/","text":"What's new in CF171? Containers The Container Update releases include new features and updates for HCL Digital Experience 9.5 container deployments. Password override in Docker Added option to override password in Docker. See Docker deployment . Password override in OpenShift Added option to override password in OpenShift. See OpenShift deployment . Support for Kubernetes as verified in Amazon Elastic Container Service for Kubernetes (EKS) Added support for Kubernetes on AWS EKS. See Deploy HCL Digital Experience 9.5 Container to Amazon EKS . Support for Auto-scaling and Route configuration Added support for auto-scaling based on available CPU and memory utilization and route configuration. See Customizing the container deployment . Downloading DX products and accessing Customer Support You can go to the HCL Software Licensing Portal to access and download product software. See the Step-by-step guide to downloading DX products and accessing Customer Support for more information. Parent topic: Container Update releases","title":"What's new in CF171? Containers"},{"location":"what%27s-new/new_cf171/#whats-new-in-cf171-containers","text":"The Container Update releases include new features and updates for HCL Digital Experience 9.5 container deployments.","title":"What's new in CF171? Containers"},{"location":"what%27s-new/new_cf171/#password-override-in-docker","text":"Added option to override password in Docker. See Docker deployment .","title":"Password override in Docker"},{"location":"what%27s-new/new_cf171/#password-override-in-openshift","text":"Added option to override password in OpenShift. See OpenShift deployment .","title":"Password override in OpenShift"},{"location":"what%27s-new/new_cf171/#support-for-kubernetes-as-verified-in-amazon-elastic-container-service-for-kubernetes-eks","text":"Added support for Kubernetes on AWS EKS. See Deploy HCL Digital Experience 9.5 Container to Amazon EKS .","title":"Support for Kubernetes as verified in Amazon Elastic Container Service for Kubernetes (EKS)"},{"location":"what%27s-new/new_cf171/#support-for-auto-scaling-and-route-configuration","text":"Added support for auto-scaling based on available CPU and memory utilization and route configuration. See Customizing the container deployment .","title":"Support for Auto-scaling and Route configuration"},{"location":"what%27s-new/new_cf171/#downloading-dx-products-and-accessing-customer-support","text":"You can go to the HCL Software Licensing Portal to access and download product software. See the Step-by-step guide to downloading DX products and accessing Customer Support for more information. Parent topic: Container Update releases","title":"Downloading DX products and accessing Customer Support"},{"location":"what%27s-new/new_cf172/","text":"What's new in CF172? Containers The Container Update release include new features and updates for HCL Digital Experience 9.5 container deployments, including DX tool, Support and API updates. Web Content Manager (WCM) Support Tools The HCL WCM Support Tools portlet is provided to investigate and analyze information related to the WCM Java Content Repository (JCR) nodes. The WCM Support Tools Portlet is provided in the HCL Digital Experience 9.5 CF172 Container Update release, and is accessible from the standard Digital Experience administration panel in the CF172 release. See HCL Web Content Manager Support Tools for details. New Web Content Query Parameter APIs New Web Content Query Parameter APIs are added in HCL Digital Experience 9.5 CF172. See REST Query service for web content for details. New Enhanced Content Template API The Enhanced Web Content Manager Content Template API adds the ability to retrieve the configuration details of content template elements. See How to retrieve the settings of the elements for content templates by using REST for details. Updated HCL Digital Experience 9.5 platform support statements Read the updates to HCL Digital Experience 9.5 detailed system support statements published to the HCL Digital Experience Support site. See This HCL Software Support article for details. Downloading DX products and accessing Customer Support You can go to the HCL Software Licensing Portal to access and download product software. See the Step-by-step guide to downloading DX products and accessing Customer Support for more information. Parent topic: Container Update releases","title":"What's new in CF172? Containers"},{"location":"what%27s-new/new_cf172/#whats-new-in-cf172-containers","text":"The Container Update release include new features and updates for HCL Digital Experience 9.5 container deployments, including DX tool, Support and API updates.","title":"What's new in CF172? Containers"},{"location":"what%27s-new/new_cf172/#web-content-manager-wcm-support-tools","text":"The HCL WCM Support Tools portlet is provided to investigate and analyze information related to the WCM Java Content Repository (JCR) nodes. The WCM Support Tools Portlet is provided in the HCL Digital Experience 9.5 CF172 Container Update release, and is accessible from the standard Digital Experience administration panel in the CF172 release. See HCL Web Content Manager Support Tools for details.","title":"Web Content Manager (WCM) Support Tools"},{"location":"what%27s-new/new_cf172/#new-web-content-query-parameter-apis","text":"New Web Content Query Parameter APIs are added in HCL Digital Experience 9.5 CF172. See REST Query service for web content for details.","title":"New Web Content Query Parameter APIs"},{"location":"what%27s-new/new_cf172/#new-enhanced-content-template-api","text":"The Enhanced Web Content Manager Content Template API adds the ability to retrieve the configuration details of content template elements. See How to retrieve the settings of the elements for content templates by using REST for details.","title":"New Enhanced Content Template API"},{"location":"what%27s-new/new_cf172/#updated-hcl-digital-experience-95-platform-support-statements","text":"Read the updates to HCL Digital Experience 9.5 detailed system support statements published to the HCL Digital Experience Support site. See This HCL Software Support article for details.","title":"Updated HCL Digital Experience 9.5 platform support statements"},{"location":"what%27s-new/new_cf172/#downloading-dx-products-and-accessing-customer-support","text":"You can go to the HCL Software Licensing Portal to access and download product software. See the Step-by-step guide to downloading DX products and accessing Customer Support for more information. Parent topic: Container Update releases","title":"Downloading DX products and accessing Customer Support"},{"location":"what%27s-new/new_cf173/","text":"What's new in CF173? Containers This HCL Digital Experience 9.5 Container Update release include new WCM REST APIs, and Tech Previews for Content Composer and Digital Asset Management, and more. Web Content Manager Mirror syndication - Disable full build option An option to disable the rebuild with the WCM mirror syndication option is now available. This option can be set using the WCM Configuration service on the syndicator. See Manually syndicating items . New WCM Restore Version REST API The Restore version API supports restoring content versions to a previous level. See How to use REST with Versions . New Enhanced WCM Content Template API Element Configuration The Enhanced Content Template API Element Configuration Updates allows the configuration of template elements to be updated. See How to set default content values for content templates by using REST . New WCM Export Digital Asset Management references API The Web Content Manager Export DAM references API REST service can be used to retrieve content or components with references to externally managed resources, using the Digital Asset Manager plugin. See How to use REST with content items . New Experience API samples Two new samples are provided for use with the HCL Digital Experience 9.5 Experience API, supporting Sample login and content update process flow, and Get roles with authentication functions. See the Experience API Sample Calls . New HCL Content Composer \u2013 Tech Preview in HCL Digital Experience 9.5 CF173 Content Composer delivers simplified processes for creating and managing Digital Experience site content. Users can access a Tech Preview of the Content Composer features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 CF173 Content Composer \u2013 Tech Preview for details. New Digital Asset Management \u2013 Tech Preview in HCL Digital Experience 9.5 CF173 Digital Asset Management delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access a Tech Preview of the Digital Asset Management features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 CF173 Digital Asset Management \u2013 Tech Preview for details. Downloading DX products and accessing Customer Support The HCL Digital Experience 9.5 detailed system support statements are updated and published on the HCL Digital Experience Support site. You can go to the HCL Software Licensing Portal to access and download product software. For more information, see the Step-by-step guide to downloading DX products and accessing Customer Support . Parent topic: Container Update releases","title":"What's new in CF173? Containers"},{"location":"what%27s-new/new_cf173/#whats-new-in-cf173-containers","text":"This HCL Digital Experience 9.5 Container Update release include new WCM REST APIs, and Tech Previews for Content Composer and Digital Asset Management, and more.","title":"What's new in CF173? Containers"},{"location":"what%27s-new/new_cf173/#web-content-manager-mirror-syndication-disable-full-build-option","text":"An option to disable the rebuild with the WCM mirror syndication option is now available. This option can be set using the WCM Configuration service on the syndicator. See Manually syndicating items .","title":"Web Content Manager Mirror syndication - Disable full build option"},{"location":"what%27s-new/new_cf173/#new-wcm-restore-version-rest-api","text":"The Restore version API supports restoring content versions to a previous level. See How to use REST with Versions .","title":"New WCM Restore Version REST API"},{"location":"what%27s-new/new_cf173/#new-enhanced-wcm-content-template-api-element-configuration","text":"The Enhanced Content Template API Element Configuration Updates allows the configuration of template elements to be updated. See How to set default content values for content templates by using REST .","title":"New Enhanced WCM Content Template API Element Configuration"},{"location":"what%27s-new/new_cf173/#new-wcm-export-digital-asset-management-references-api","text":"The Web Content Manager Export DAM references API REST service can be used to retrieve content or components with references to externally managed resources, using the Digital Asset Manager plugin. See How to use REST with content items .","title":"New WCM Export Digital Asset Management references API"},{"location":"what%27s-new/new_cf173/#new-experience-api-samples","text":"Two new samples are provided for use with the HCL Digital Experience 9.5 Experience API, supporting Sample login and content update process flow, and Get roles with authentication functions. See the Experience API Sample Calls .","title":"New Experience API samples"},{"location":"what%27s-new/new_cf173/#new-hcl-content-composer-tech-preview-in-hcl-digital-experience-95-cf173","text":"Content Composer delivers simplified processes for creating and managing Digital Experience site content. Users can access a Tech Preview of the Content Composer features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 CF173 Content Composer \u2013 Tech Preview for details.","title":"New HCL Content Composer \u2013 Tech Preview in HCL Digital Experience 9.5 CF173"},{"location":"what%27s-new/new_cf173/#new-digital-asset-management-tech-preview-in-hcl-digital-experience-95-cf173","text":"Digital Asset Management delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access a Tech Preview of the Digital Asset Management features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 CF173 Digital Asset Management \u2013 Tech Preview for details.","title":"New Digital Asset Management \u2013 Tech Preview in HCL Digital Experience 9.5 CF173"},{"location":"what%27s-new/new_cf173/#downloading-dx-products-and-accessing-customer-support","text":"The HCL Digital Experience 9.5 detailed system support statements are updated and published on the HCL Digital Experience Support site. You can go to the HCL Software Licensing Portal to access and download product software. For more information, see the Step-by-step guide to downloading DX products and accessing Customer Support . Parent topic: Container Update releases","title":"Downloading DX products and accessing Customer Support"},{"location":"what%27s-new/new_cf18/","text":"What's new in CF18? Containers This HCL Digital Experience 9.5 Container Update release includes new WCM REST APIs, updated Tech Preview releases of Content Composer and Digital Asset Management, Rich Text Editor and Java Server Faces bridge updates, and more. Go to the HCL Software Support Site for the list of software fixes, including CF18. See What's New in CF18 for HCL Digital Experience for a list of updates in this release. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. JavaServer Faces (JSF) Bridge With the HCL Digital Experience CF18 Container and CF update, an updated JSF Portlet Bridge is introduced and installed in the HCL Digital Experience software. Customers installing HCL Digital Experience 8.5, 9, or 9.5 CF18 or Container Update release CF18, and using an IBM WebSphere Application Server Network Deployment version later than 8.5.5.17 or 9.0.5.2 can utilize the new JSF Portlet Bridge. See JavaServer Faces implementation for more information. Apply Content Template REST API The Apply Content Template API allows a developer to apply a content template to a set of specific content items or all content items of a certain template. It is equivalent to the functionality in the user interface. See How to set default content values for content templates by using REST for details. Enhanced Content Template API The Enhanced Web Content Manager Content Template API adds the ability to retrieve the configuration details of content template elements. See How to retrieve the settings of the elements for content templates by using REST for details. Rich Text Editor Textbox I/O Updates Updates are provided for the Textbox I/O Rich Text Editor, requiring use of Java 8. See Rich text editor toolbar configuration options for details. Sample Guidance to set Storage Class and Volume to deploy HCL Digital Experience 9.5 Containers to Amazon Elastic Kubernetes Service (EKS) and Red Hat OpenShift Additional \"Sample Storage Class and Volume\" guidance is available to HCL Digital Experience 9.5 container administrators. See Sample Storage Class and Volume topic for details. HCL Content Composer \u2013 Tech Preview for HCL Digital Experience 9.5 CF173 or later Content Composer delivers simplified processes for creating and managing Digital Experience site content. Users can access a Tech Preview of the Content Composer features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 Content Composer \u2013 Tech Preview for details. Digital Asset Management \u2013 Tech Preview for HCL Digital Experience 9.5 CF173 or later Digital Asset Management delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access a Tech Preview of the Digital Asset Management features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 Digital Asset Management \u2013 Tech Preview for details. Updated HCL Digital Experience 9.5 platform support statements See the updates to the HCL Digital Experience 9.5 detailed system support statements published to the HCL Digital Experience Support site. Parent topic: Container Update releases","title":"What's new in CF18? Containers"},{"location":"what%27s-new/new_cf18/#whats-new-in-cf18-containers","text":"This HCL Digital Experience 9.5 Container Update release includes new WCM REST APIs, updated Tech Preview releases of Content Composer and Digital Asset Management, Rich Text Editor and Java Server Faces bridge updates, and more. Go to the HCL Software Support Site for the list of software fixes, including CF18. See What's New in CF18 for HCL Digital Experience for a list of updates in this release. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information.","title":"What's new in CF18? Containers"},{"location":"what%27s-new/new_cf18/#javaserver-faces-jsf-bridge","text":"With the HCL Digital Experience CF18 Container and CF update, an updated JSF Portlet Bridge is introduced and installed in the HCL Digital Experience software. Customers installing HCL Digital Experience 8.5, 9, or 9.5 CF18 or Container Update release CF18, and using an IBM WebSphere Application Server Network Deployment version later than 8.5.5.17 or 9.0.5.2 can utilize the new JSF Portlet Bridge. See JavaServer Faces implementation for more information.","title":"JavaServer Faces (JSF) Bridge"},{"location":"what%27s-new/new_cf18/#apply-content-template-rest-api","text":"The Apply Content Template API allows a developer to apply a content template to a set of specific content items or all content items of a certain template. It is equivalent to the functionality in the user interface. See How to set default content values for content templates by using REST for details.","title":"Apply Content Template REST API"},{"location":"what%27s-new/new_cf18/#enhanced-content-template-api","text":"The Enhanced Web Content Manager Content Template API adds the ability to retrieve the configuration details of content template elements. See How to retrieve the settings of the elements for content templates by using REST for details.","title":"Enhanced Content Template API"},{"location":"what%27s-new/new_cf18/#rich-text-editor-textbox-io-updates","text":"Updates are provided for the Textbox I/O Rich Text Editor, requiring use of Java 8. See Rich text editor toolbar configuration options for details.","title":"Rich Text Editor Textbox I/O Updates"},{"location":"what%27s-new/new_cf18/#sample-guidance-to-set-storage-class-and-volume-to-deploy-hcl-digital-experience-95-containers-to-amazon-elastic-kubernetes-service-eks-and-red-hat-openshift","text":"Additional \"Sample Storage Class and Volume\" guidance is available to HCL Digital Experience 9.5 container administrators. See Sample Storage Class and Volume topic for details.","title":"Sample Guidance to set Storage Class and Volume to deploy HCL Digital Experience 9.5 Containers to Amazon Elastic Kubernetes Service (EKS) and Red Hat OpenShift"},{"location":"what%27s-new/new_cf18/#hcl-content-composer-tech-preview-for-hcl-digital-experience-95-cf173-or-later","text":"Content Composer delivers simplified processes for creating and managing Digital Experience site content. Users can access a Tech Preview of the Content Composer features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 Content Composer \u2013 Tech Preview for details.","title":"HCL Content Composer \u2013 Tech Preview for HCL Digital Experience 9.5 CF173 or later"},{"location":"what%27s-new/new_cf18/#digital-asset-management-tech-preview-for-hcl-digital-experience-95-cf173-or-later","text":"Digital Asset Management delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access a Tech Preview of the Digital Asset Management features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 Digital Asset Management \u2013 Tech Preview for details.","title":"Digital Asset Management \u2013 Tech Preview for HCL Digital Experience 9.5 CF173 or later"},{"location":"what%27s-new/new_cf18/#updated-hcl-digital-experience-95-platform-support-statements","text":"See the updates to the HCL Digital Experience 9.5 detailed system support statements published to the HCL Digital Experience Support site. Parent topic: Container Update releases","title":"Updated HCL Digital Experience 9.5 platform support statements"},{"location":"what%27s-new/new_cf181/","text":"What's new in CF181? Containers This HCL Digital Experience 9.5 Container Update release includes new Production releases of Content Composer, Digital Asset Management, and Experience API, new WCM REST APIs, guidance to deploy OpenLDAP, Remote Search and Database transfer processes, and more. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. Content Composer Content Composer delivers simplified processes for creating and managing Digital Experience site content. Users can work with Content Composer features in HCL Digital Experience 9.5 Container Update CF181 and higher releases. See HCL Digital Experience 9.5 Content Composer for details. Digital Asset Management Digital Asset Management delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access the Digital Asset Management features in HCL Digital Experience 9.5 Container Update CF181 and higher releases. See HCL Digital Experience 9.5 Digital Asset Management for details. Experience API The HCL Experience API is a set of OpenAPI compliant REST APIs available for customers deploying HCL Digital Experience 9.5 containers on supported Kubernetes platforms. It supports the integration and management of HCL Digital Experience content and functionality to any digital channel using any front-end development framework. The HCL Experience API includes REST APIs that serve as a wrapper around previously published HCL Digital Experience HTTP based APIs. See HCL Experience API for details. OpenLDAP Container integration OpenLDAP Software is an open source implementation of the Lightweight Directory Access Protocol. The HCL Digital Experience 9.5 Container Update release CF181 and higher includes an OpenLDAP container and a customization of the operator to deploy and configure the LDAP container to the HCL Digital Experience 9.5 container deployment. See Configure the OpenLDAP container image to the HCL Digital Experience 9.5 Container Deployment for details. Transfer default Container database to IBM DB2 HCL Digital Experience 9.5 installs a copy of Derby as the default database. HCL Digital Experience administrators can apply guidance to transfer the default database configuration to IBM DB2, if preferred for use as the relational database for Digital Experience 9.5 Container deployment data. See Transfer Digital Experience 9.5 Container default database to IBM DB2 for details. Remote Search services Docker deployment To support search services when deployed to Docker, Digital Experience administrators can configure Remote search services. This will require some different setup and configuration steps than used to set up remote search on a non-Docker container platform. See Deploy Remote Search services on Docker for details. New Digital Experience WCM Workflow REST APIs Two new WCM REST APIs are added to handle Process Now and Remove Workflow from an item functionality. See How to use REST with drafts and workflows for details. New Web Content Manager Reference REST API The new WCM Content Manager Reference REST API can be used by developers to find references to a Web Content or Digital Asset Management item identified by its UUID. See How to use REST with content items for details. New Web Content Text Search REST API The Text Search REST API Content Authors search for free form text in the Web Content Manager JCR. It is equivalent to the functionality in the Web Content Manager user interface. See REST Query service for web content - Query parameters for details. New Digital Experience Core Configuration REST API The Digital Experience Core Configuration API allows developers to retrieve Digital Experience deployment configuration settings. See Generic reading by using REST services for Web Content Manager for details. Web Developer Toolkit The Web Developer toolkit for HCL Digital Experience provides the ability to sync themes, content and script portlets (also known as Script Applications). It is available on the HCL Digital Experience GitHub repository. See Web Developer Toolkit for details. Parent topic: Container Update releases","title":"What's new in CF181? Containers"},{"location":"what%27s-new/new_cf181/#whats-new-in-cf181-containers","text":"This HCL Digital Experience 9.5 Container Update release includes new Production releases of Content Composer, Digital Asset Management, and Experience API, new WCM REST APIs, guidance to deploy OpenLDAP, Remote Search and Database transfer processes, and more. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information.","title":"What's new in CF181? Containers"},{"location":"what%27s-new/new_cf181/#content-composer","text":"Content Composer delivers simplified processes for creating and managing Digital Experience site content. Users can work with Content Composer features in HCL Digital Experience 9.5 Container Update CF181 and higher releases. See HCL Digital Experience 9.5 Content Composer for details.","title":"Content Composer"},{"location":"what%27s-new/new_cf181/#digital-asset-management","text":"Digital Asset Management delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access the Digital Asset Management features in HCL Digital Experience 9.5 Container Update CF181 and higher releases. See HCL Digital Experience 9.5 Digital Asset Management for details.","title":"Digital Asset Management"},{"location":"what%27s-new/new_cf181/#experience-api","text":"The HCL Experience API is a set of OpenAPI compliant REST APIs available for customers deploying HCL Digital Experience 9.5 containers on supported Kubernetes platforms. It supports the integration and management of HCL Digital Experience content and functionality to any digital channel using any front-end development framework. The HCL Experience API includes REST APIs that serve as a wrapper around previously published HCL Digital Experience HTTP based APIs. See HCL Experience API for details.","title":"Experience API"},{"location":"what%27s-new/new_cf181/#openldap-container-integration","text":"OpenLDAP Software is an open source implementation of the Lightweight Directory Access Protocol. The HCL Digital Experience 9.5 Container Update release CF181 and higher includes an OpenLDAP container and a customization of the operator to deploy and configure the LDAP container to the HCL Digital Experience 9.5 container deployment. See Configure the OpenLDAP container image to the HCL Digital Experience 9.5 Container Deployment for details.","title":"OpenLDAP Container integration"},{"location":"what%27s-new/new_cf181/#transfer-default-container-database-to-ibm-db2","text":"HCL Digital Experience 9.5 installs a copy of Derby as the default database. HCL Digital Experience administrators can apply guidance to transfer the default database configuration to IBM DB2, if preferred for use as the relational database for Digital Experience 9.5 Container deployment data. See Transfer Digital Experience 9.5 Container default database to IBM DB2 for details.","title":"Transfer default Container database to IBM DB2"},{"location":"what%27s-new/new_cf181/#remote-search-services-docker-deployment","text":"To support search services when deployed to Docker, Digital Experience administrators can configure Remote search services. This will require some different setup and configuration steps than used to set up remote search on a non-Docker container platform. See Deploy Remote Search services on Docker for details.","title":"Remote Search services Docker deployment"},{"location":"what%27s-new/new_cf181/#new-digital-experience-wcm-workflow-rest-apis","text":"Two new WCM REST APIs are added to handle Process Now and Remove Workflow from an item functionality. See How to use REST with drafts and workflows for details.","title":"New Digital Experience WCM Workflow REST APIs"},{"location":"what%27s-new/new_cf181/#new-web-content-manager-reference-rest-api","text":"The new WCM Content Manager Reference REST API can be used by developers to find references to a Web Content or Digital Asset Management item identified by its UUID. See How to use REST with content items for details.","title":"New Web Content Manager Reference REST API"},{"location":"what%27s-new/new_cf181/#new-web-content-text-search-rest-api","text":"The Text Search REST API Content Authors search for free form text in the Web Content Manager JCR. It is equivalent to the functionality in the Web Content Manager user interface. See REST Query service for web content - Query parameters for details.","title":"New Web Content Text Search REST API"},{"location":"what%27s-new/new_cf181/#new-digital-experience-core-configuration-rest-api","text":"The Digital Experience Core Configuration API allows developers to retrieve Digital Experience deployment configuration settings. See Generic reading by using REST services for Web Content Manager for details.","title":"New Digital Experience Core Configuration REST API"},{"location":"what%27s-new/new_cf181/#web-developer-toolkit","text":"The Web Developer toolkit for HCL Digital Experience provides the ability to sync themes, content and script portlets (also known as Script Applications). It is available on the HCL Digital Experience GitHub repository. See Web Developer Toolkit for details. Parent topic: Container Update releases","title":"Web Developer Toolkit"},{"location":"what%27s-new/new_noncf18/","text":"What's new in CF18? This HCL Digital Experience 9.5 CF18 release includes new WCM REST APIs, updated releases of Content Composer and Digital Asset Management Tech Preview releases, Rich Text Editor and JavaServer Faces bridge updates, and more. Go to the HCL Software Support Site for the list of software fixes. See What's New in CF18 for HCL Digital Experience for a list of updates in this release. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. Web Content Manager (WCM) Support Tools The HCL WCM Support Tools portlet is provided to investigate and analyze information related to the WCM Java Content Repository (JCR) nodes. The WCM Support Tools Portlet is provided in the HCL Digital Experience 9.5 CF18 release, and is accessible from the standard Digital Experience administration panel. See HCL Web Content Manager Support Tools for details. JavaServer Faces (JSF) Bridge With the HCL Digital Experience CF18 Container and CF update, an updated JSF Portlet Bridge is introduced and installed in the HCL Digital Experience software. Customers installing HCL Digital Experience 8.5, 9.0, or 9.5 non-container CF18 or Container Update release CF18, and using an IBM WebSphere Application Server Network Deployment version later than 8.5.5.17 or 9.0.5.2 can utilize the new JSF Portlet Bridge. See JavaServer Faces implementation for more information. Apply Content Template REST API The Apply Content Template API allows a developer to apply a content template to a set of specific content items or all content items of a certain template. It is equivalent to the functionality in the user interface. See How to set default content values for content templates by using REST for details. Enhanced Content Template API The Enhanced Web Content Manager Content Template API adds the ability to retrieve the configuration details of content template elements. See How to retrieve the settings of the elements for content templates by using REST for details. Rich Text Editor Textbox I/O Updates Updates are provided for the Textbox I/O Rich Text Editor, requiring use of Java 8. See Rich text editor toolbar configuration options for details. Sample Guidance to set Storage Class and Volume to deploy HCL Digital Experience 9.5 Containers to Amazon Elastic Kubernetes Service (EKS) and Red Hat OpenShift Additional guidance for \"Sample Storage Class and Volume\" is available to HCL Digital Experience 9.5 container administrators. See Sample Storage Class and Volume topic for details. HCL Content Composer \u2013 Tech Preview for HCL Digital Experience 9.5 CF173 or higher release Content Composer delivers simplified processes for creating and managing Digital Experience site content. Users can access a Tech Preview of the Content Composer features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 Content Composer \u2013 Tech Preview for details. Digital Asset Management \u2013 Tech Preview for HCL Digital Experience 9.5 CF173 or higher release Digital Asset Management (DAM) delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access a Tech Preview of the Digital Asset Management features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 Digital Asset Management \u2013 Tech Preview for details. Updated HCL Digital Experience 9.5 platform support statements See the updates to the HCL Digital Experience 9.5 detailed system support statements published to the HCL Digital Experience Support site. Parent topic: Latest Combined CF and Container updates","title":"What's new in CF18?"},{"location":"what%27s-new/new_noncf18/#whats-new-in-cf18","text":"This HCL Digital Experience 9.5 CF18 release includes new WCM REST APIs, updated releases of Content Composer and Digital Asset Management Tech Preview releases, Rich Text Editor and JavaServer Faces bridge updates, and more. Go to the HCL Software Support Site for the list of software fixes. See What's New in CF18 for HCL Digital Experience for a list of updates in this release. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information.","title":"What's new in CF18?"},{"location":"what%27s-new/new_noncf18/#web-content-manager-wcm-support-tools","text":"The HCL WCM Support Tools portlet is provided to investigate and analyze information related to the WCM Java Content Repository (JCR) nodes. The WCM Support Tools Portlet is provided in the HCL Digital Experience 9.5 CF18 release, and is accessible from the standard Digital Experience administration panel. See HCL Web Content Manager Support Tools for details.","title":"Web Content Manager (WCM) Support Tools"},{"location":"what%27s-new/new_noncf18/#javaserver-faces-jsf-bridge","text":"With the HCL Digital Experience CF18 Container and CF update, an updated JSF Portlet Bridge is introduced and installed in the HCL Digital Experience software. Customers installing HCL Digital Experience 8.5, 9.0, or 9.5 non-container CF18 or Container Update release CF18, and using an IBM WebSphere Application Server Network Deployment version later than 8.5.5.17 or 9.0.5.2 can utilize the new JSF Portlet Bridge. See JavaServer Faces implementation for more information.","title":"JavaServer Faces (JSF) Bridge"},{"location":"what%27s-new/new_noncf18/#apply-content-template-rest-api","text":"The Apply Content Template API allows a developer to apply a content template to a set of specific content items or all content items of a certain template. It is equivalent to the functionality in the user interface. See How to set default content values for content templates by using REST for details.","title":"Apply Content Template REST API"},{"location":"what%27s-new/new_noncf18/#enhanced-content-template-api","text":"The Enhanced Web Content Manager Content Template API adds the ability to retrieve the configuration details of content template elements. See How to retrieve the settings of the elements for content templates by using REST for details.","title":"Enhanced Content Template API"},{"location":"what%27s-new/new_noncf18/#rich-text-editor-textbox-io-updates","text":"Updates are provided for the Textbox I/O Rich Text Editor, requiring use of Java 8. See Rich text editor toolbar configuration options for details.","title":"Rich Text Editor Textbox I/O Updates"},{"location":"what%27s-new/new_noncf18/#sample-guidance-to-set-storage-class-and-volume-to-deploy-hcl-digital-experience-95-containers-to-amazon-elastic-kubernetes-service-eks-and-red-hat-openshift","text":"Additional guidance for \"Sample Storage Class and Volume\" is available to HCL Digital Experience 9.5 container administrators. See Sample Storage Class and Volume topic for details.","title":"Sample Guidance to set Storage Class and Volume to deploy HCL Digital Experience 9.5 Containers to Amazon Elastic Kubernetes Service (EKS) and Red Hat OpenShift"},{"location":"what%27s-new/new_noncf18/#hcl-content-composer-tech-preview-for-hcl-digital-experience-95-cf173-or-higher-release","text":"Content Composer delivers simplified processes for creating and managing Digital Experience site content. Users can access a Tech Preview of the Content Composer features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 Content Composer \u2013 Tech Preview for details.","title":"HCL Content Composer \u2013 Tech Preview for HCL Digital Experience 9.5 CF173 or higher release"},{"location":"what%27s-new/new_noncf18/#digital-asset-management-tech-preview-for-hcl-digital-experience-95-cf173-or-higher-release","text":"Digital Asset Management (DAM) delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access a Tech Preview of the Digital Asset Management features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 Digital Asset Management \u2013 Tech Preview for details.","title":"Digital Asset Management \u2013 Tech Preview for HCL Digital Experience 9.5 CF173 or higher release"},{"location":"what%27s-new/new_noncf18/#updated-hcl-digital-experience-95-platform-support-statements","text":"See the updates to the HCL Digital Experience 9.5 detailed system support statements published to the HCL Digital Experience Support site. Parent topic: Latest Combined CF and Container updates","title":"Updated HCL Digital Experience 9.5 platform support statements"},{"location":"what%27s-new/new_noncf19/","text":"What's new with CF19? Combined Cumulative Fix (CF19) includes new features and software fixes for the latest version of HCL Digital Experience. This HCL Digital Experience 9.5 CF19 release includes new WCM REST APIs, Web Developer Toolkit, updated releases of Content Composer, Digital Asset Management and Experience API, Hybrid deployment support, Progressive Web Application delivery, Google Analytics integration, Mobile Preview, DXClient and DXConnect tooling supporting CICD release processes, new HCL Digital Experience \u2018How To\u2019 videos and more. Go to the HCL Software Support Site for the list of software fixes. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. Web Developer Toolkit The Web Developer toolkit for HCL Digital Experience provides the ability to sync themes, content and script portlets (also known as Script Applications). It is available on the HCL Digital Experience Github repository. See Web Developer Toolkit for details. Hybrid Deployment The HCL Digital Experience 9.5 Hybrid deployment and topics deliver capability to deploy and manage HCL Digital Experience 9.5 core Portal Server and Web Content Manager services on premises, and connect to cloud native components Digital Asset Management, Content Composer, Experience API and related services in a production environment. See the Hybrid Deployment topic for more information. Progressive Web Application support Develop support that adds native mobile application experience and performance to your web site using browser-based functionality. See the Progressive Web Application topic for more information. Google Analytics integration Learn how to set up integration of Digital Experience sites with Google Analytics and view the resulting web analytics tracking to assess the effectiveness of your DX site pages with end user audiences. See the Integrate Google Analytics with HCL Digital Experience topic for more information. Mobile Preview Use the Mobile Preview simulator to view the presentation of Digital Experience site page components on select mobile devices. See the Mobile Preview topic for more information. DXClient and DXConnect tooling supporting CICD release processes HCL Digital Experience 9.5 CF19 and higher includes a DXClient toolset, and DX Connect servlet that provides developers and administrators with an approach to deploy changes or improvements to the HCL Digital Experience platform,and automate processes in the development and delivery process. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. New Digital Experience REST APIs New HCL DX APIs are available with the HCL DX CF19 release: Web content image renditions interactions REST API Web Content Library Locale Query Workflow Comments API Using XML Access to export and import Digital Asset Management assets Web Content Manager Syndication REST APIs Process Now and Remove Workflow REST APIs Web Content Manager References REST API Web Content Text Search REST API Digital Experience Core Configuration REST API Web Content Manager Lock/Unlock API Create or update an Option Selection Element Search Component Results Display New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step by step guidance for HCL Digital Experience practitioners presented in several new videos. See the following HCL Digital Experience Help Center topics: Remote Search Service Transfer HCL Digital Experience 9.5 Container default database to IBM DB2 Create a web content library Virtual portals Backup and restore Configuration Wizard Combined Cumulative Fix Install Rich text editor toolbar configuration options Data collection and symptom analysis HCL Digital Experience Combined Cumulative Fix (CF) Installation How to manage syndicators and subscribers Deploy HCL Digital Experience 9.5 to Red Hat OpenShift Deploy Digital Asset Management, Content Composer and Experience API to Red Hat OpenShift Configure the OpenLDAP container image to the HCL DX 9.5 Container Deployment Parent topic: Latest Combined CF and Container updates","title":"What's new with CF19?"},{"location":"what%27s-new/new_noncf19/#whats-new-with-cf19","text":"Combined Cumulative Fix (CF19) includes new features and software fixes for the latest version of HCL Digital Experience. This HCL Digital Experience 9.5 CF19 release includes new WCM REST APIs, Web Developer Toolkit, updated releases of Content Composer, Digital Asset Management and Experience API, Hybrid deployment support, Progressive Web Application delivery, Google Analytics integration, Mobile Preview, DXClient and DXConnect tooling supporting CICD release processes, new HCL Digital Experience \u2018How To\u2019 videos and more. Go to the HCL Software Support Site for the list of software fixes. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information.","title":"What's new with CF19?"},{"location":"what%27s-new/new_noncf19/#web-developer-toolkit","text":"The Web Developer toolkit for HCL Digital Experience provides the ability to sync themes, content and script portlets (also known as Script Applications). It is available on the HCL Digital Experience Github repository. See Web Developer Toolkit for details.","title":"Web Developer Toolkit"},{"location":"what%27s-new/new_noncf19/#hybrid-deployment","text":"The HCL Digital Experience 9.5 Hybrid deployment and topics deliver capability to deploy and manage HCL Digital Experience 9.5 core Portal Server and Web Content Manager services on premises, and connect to cloud native components Digital Asset Management, Content Composer, Experience API and related services in a production environment. See the Hybrid Deployment topic for more information.","title":"Hybrid Deployment"},{"location":"what%27s-new/new_noncf19/#progressive-web-application-support","text":"Develop support that adds native mobile application experience and performance to your web site using browser-based functionality. See the Progressive Web Application topic for more information.","title":"Progressive Web Application support"},{"location":"what%27s-new/new_noncf19/#google-analytics-integration","text":"Learn how to set up integration of Digital Experience sites with Google Analytics and view the resulting web analytics tracking to assess the effectiveness of your DX site pages with end user audiences. See the Integrate Google Analytics with HCL Digital Experience topic for more information.","title":"Google Analytics integration"},{"location":"what%27s-new/new_noncf19/#mobile-preview","text":"Use the Mobile Preview simulator to view the presentation of Digital Experience site page components on select mobile devices. See the Mobile Preview topic for more information.","title":"Mobile Preview"},{"location":"what%27s-new/new_noncf19/#dxclient-and-dxconnect-tooling-supporting-cicd-release-processes","text":"HCL Digital Experience 9.5 CF19 and higher includes a DXClient toolset, and DX Connect servlet that provides developers and administrators with an approach to deploy changes or improvements to the HCL Digital Experience platform,and automate processes in the development and delivery process. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"DXClient and DXConnect tooling supporting CICD release processes"},{"location":"what%27s-new/new_noncf19/#new-digital-experience-rest-apis","text":"New HCL DX APIs are available with the HCL DX CF19 release: Web content image renditions interactions REST API Web Content Library Locale Query Workflow Comments API Using XML Access to export and import Digital Asset Management assets Web Content Manager Syndication REST APIs Process Now and Remove Workflow REST APIs Web Content Manager References REST API Web Content Text Search REST API Digital Experience Core Configuration REST API Web Content Manager Lock/Unlock API Create or update an Option Selection Element Search Component Results Display","title":"New Digital Experience REST APIs"},{"location":"what%27s-new/new_noncf19/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step by step guidance for HCL Digital Experience practitioners presented in several new videos. See the following HCL Digital Experience Help Center topics: Remote Search Service Transfer HCL Digital Experience 9.5 Container default database to IBM DB2 Create a web content library Virtual portals Backup and restore Configuration Wizard Combined Cumulative Fix Install Rich text editor toolbar configuration options Data collection and symptom analysis HCL Digital Experience Combined Cumulative Fix (CF) Installation How to manage syndicators and subscribers Deploy HCL Digital Experience 9.5 to Red Hat OpenShift Deploy Digital Asset Management, Content Composer and Experience API to Red Hat OpenShift Configure the OpenLDAP container image to the HCL DX 9.5 Container Deployment Parent topic: Latest Combined CF and Container updates","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"what%27s-new/new_noncf196/","text":"What's new in CF196? Combined Cumulative Fix (CF196) includes new features and software fixes for the latest version of HCL Digital Experience. Beginning with CF19 and Container Update release CF196, release updates for both on\u2013premises platforms and container deployments will be available. This HCL Digital Experience 9.5 CF196 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management and Experience API, Design Studio (Beta) for Container deployments, Theme Editor Portlet, Content Security Policy support, DXClient and DXConnect tooling supporting CICD release processes, Multilingual enhancements, HCL Unica Discover enablement, new HCL Digital Experience \u2018How To\u2019 videos and more. Go to the HCL Software Support Site for the list of software fixes. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. Note: For new capabilities that are available for HCL DX 9.5 CF196 Container Update deployments, see What's new in the CF196 Container Update release topic. Theme Editor Portlet The Theme Editor portlet is a new addition to HCL Digital Experience CF196 and higher release capabilities. The portlet allows an administrator to edit static theme resources in WebDAV without the use of a WebDAV client or tool. See the Theme Editor Portlet topic for more information. Web Content Manager Multilingual Solution Enhancements The HCL Web Content Manager Multilingual Solution is a set of tools used to manage translated versions Web Content Manager content for localized and regionalized websites. Beginning with the HCL Digital Experience 9.5 CF196 release, a new export and import capability allows you to support translation of the content of a library by exporting it into a format supported by a translation service and importing the translated content back into the content library using the DXClient tool. Support is also added to define a maximum field length, export changed contents from a library, and export to projects. A new option to switch the language in an HCL DX 9.5 Mobile view is also available. See the How to export and import WCM library content using DXClient and The Woodburn Studio demo site Help Center topics for more information. Enable Presentation of Locales in Friendly URLs Beginning with the HCL Digital Experience 9.5 Container Update CF196 release, enablement is provided to present friendly URLs with locale specific definitions when multi-lingual page versions are requested. This enablement can also improve SEO results when users search for language-specific DX page topics. See the Enabling presentation of locales in friendly URLs topic for more information. Change language presented in the HCL Digital Experience Theme Beginning with HCL DX CF196 release, you can switch the language presented in your Digital Experience theme. An example is provided in the Woodburn Studio demo site supporting presentations for French, Spanish, and English languages. See the How to switch the languages in the Digital Experience theme topic for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include a new DXClient Docker image, and new release artifact types supporting Shared Libraries, Obtain failed Syndication reports, Undeploy Themes, and Export/Import Web Content Manager Library, Deploy DX Core, Manage Syndicator, and Manage Subscriber tasks are provided in CF196. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. Web Content Manager Advanced Cache Options New options are available to flush the Web Content Manager Advanced cache, as a result of syndication operations, to help improve performance and reliability. See the Web Content Manager Cache Parameters topic for more information. Content Security Policy The Content-Security-Policy header is used by modern browsers to enhance the security of HCL Digital Experience site documents or web pages by allowing HCL Digital Experience administrators or developers declare which dynamic resources are allowed to load. With HCL Digital Experience Container Update CF192 and later releases, developers can apply platform support and guidance to update their DX sites to validate trusted sources before rendering pages to end users. See the Content Security Policy Help Center topic for more information. Enhanced Cross Origin Resource Sharing Configuration Enhanced Cross Origin Resource Sharing Configuration adds new options for HCL Digital Experience administrators to set configuration for CORS using a WP configuration service in the IBM WebSphere Application Server resource environment provider. See Enhanced Cross Origin Resource Sharing Configuration for more information. HCL Digital Experience 9.5 Integration with HCL Unica Discover Integration of HCL Digital Experience 9.5 sites with HCL Unica Discover enables DX site managers and marketers to access deep insight analytics and session replay services to assess the effectiveness of DX site pages with end user audiences. See the Integrate HCL Unica Discover with HCL Digital Experience topic for more information. Deploy HCL DX 9.5 using Docker Compose Beginning with HCL DX 9.5 CF196, administrators and developers can deploy HCL DX 9.5 using Docker Compose, for non-production use. Docker Compose scripts for HCL DX 9.5, installation and configuration instructions for non-production use are available in the HCL Software Github . See the Docker image deployment using Docker Compose topic for more information. Deploy HCL Digital Experience 9.5 on HCL Solution Factory (SoFy) The HCL Solution Factory (SoFy) platform offers the ability for organizations to quickly prototype and test assets and can enable organizations to speed cloud-native adoption. Visit HCL SoFy to access HCL Digital Experience 9.5 and other HCL software offerings to quickly assess and test cloud-native strategies, gain hands-on experience working with demonstration assets, to see what best fits adoption plans. View this online tutorial: Deploy HCL Digital Experience in Minutes with HCL SoFy HCL Digital Experience 9.5 Integration with HCL Commerce HCL Commerce integration with HCL Digital Experience allows content and digital assets managed within HCL Digital Experience to be utilized in any Commerce store. Visit the HCL Commerce - Digital Experience integration resource for more information and pre-requisites. New Digital Experience REST APIs New HCL DX APIs are available with the HCL DX CF196 release: Web Content Manager Multilingual Solution APIs Web Content Manager Comments API Web Content Manager Find Rendering and Script Portlet References Web Content Manager Access Control Filter REST API Clear Theme Cache API Work with Deleted Web Content Items New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in videos in following HCL Digital Experience Help Center topics: Configuring user access permissions to Digital Asset Management assets Deep Dive: Progressive Web Applications with HCL DX 9.5 Content Security Policy with HCL Digital 9.5 Getting Started with DXClient on Red Hat OpenShift using HCL Digital Experience CF194 Understanding the Core Persistent Volumes in HCL Digital Experience Container Update CF194 Create a WebDAV Theme copy using HCL Digital Experience 9.5 Create and apply A/B personalized scenarios with HCL Digital Experience Deploy HCL Digital Experience in Minutes using HCL Solution Factory Using the dxctl tool to Update Digital Experience 9.5 on Red Hat OpenShift How to Upload HCL Digital Experience 9.5 CF Container Images to a Private Repository HCL Digital Experience 9.5 Container Deployment Using the dxctl tool to deploy Digital Experience 9.5 on Red Hat OpenShift Create a WAR-based theme copy on HCL Digital Experience 9.5 Parent topic: Latest Combined CF and Container updates","title":"What's new in CF196?"},{"location":"what%27s-new/new_noncf196/#whats-new-in-cf196","text":"Combined Cumulative Fix (CF196) includes new features and software fixes for the latest version of HCL Digital Experience. Beginning with CF19 and Container Update release CF196, release updates for both on\u2013premises platforms and container deployments will be available. This HCL Digital Experience 9.5 CF196 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management and Experience API, Design Studio (Beta) for Container deployments, Theme Editor Portlet, Content Security Policy support, DXClient and DXConnect tooling supporting CICD release processes, Multilingual enhancements, HCL Unica Discover enablement, new HCL Digital Experience \u2018How To\u2019 videos and more. Go to the HCL Software Support Site for the list of software fixes. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. Note: For new capabilities that are available for HCL DX 9.5 CF196 Container Update deployments, see What's new in the CF196 Container Update release topic.","title":"What's new in CF196?"},{"location":"what%27s-new/new_noncf196/#theme-editor-portlet","text":"The Theme Editor portlet is a new addition to HCL Digital Experience CF196 and higher release capabilities. The portlet allows an administrator to edit static theme resources in WebDAV without the use of a WebDAV client or tool. See the Theme Editor Portlet topic for more information.","title":"Theme Editor Portlet"},{"location":"what%27s-new/new_noncf196/#web-content-manager-multilingual-solution-enhancements","text":"The HCL Web Content Manager Multilingual Solution is a set of tools used to manage translated versions Web Content Manager content for localized and regionalized websites. Beginning with the HCL Digital Experience 9.5 CF196 release, a new export and import capability allows you to support translation of the content of a library by exporting it into a format supported by a translation service and importing the translated content back into the content library using the DXClient tool. Support is also added to define a maximum field length, export changed contents from a library, and export to projects. A new option to switch the language in an HCL DX 9.5 Mobile view is also available. See the How to export and import WCM library content using DXClient and The Woodburn Studio demo site Help Center topics for more information.","title":"Web Content Manager Multilingual Solution Enhancements"},{"location":"what%27s-new/new_noncf196/#enable-presentation-of-locales-in-friendly-urls","text":"Beginning with the HCL Digital Experience 9.5 Container Update CF196 release, enablement is provided to present friendly URLs with locale specific definitions when multi-lingual page versions are requested. This enablement can also improve SEO results when users search for language-specific DX page topics. See the Enabling presentation of locales in friendly URLs topic for more information.","title":"Enable Presentation of Locales in Friendly URLs"},{"location":"what%27s-new/new_noncf196/#change-language-presented-in-the-hcl-digital-experience-theme","text":"Beginning with HCL DX CF196 release, you can switch the language presented in your Digital Experience theme. An example is provided in the Woodburn Studio demo site supporting presentations for French, Spanish, and English languages. See the How to switch the languages in the Digital Experience theme topic for more information.","title":"Change language presented in the HCL Digital Experience Theme"},{"location":"what%27s-new/new_noncf196/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include a new DXClient Docker image, and new release artifact types supporting Shared Libraries, Obtain failed Syndication reports, Undeploy Themes, and Export/Import Web Content Manager Library, Deploy DX Core, Manage Syndicator, and Manage Subscriber tasks are provided in CF196. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"what%27s-new/new_noncf196/#web-content-manager-advanced-cache-options","text":"New options are available to flush the Web Content Manager Advanced cache, as a result of syndication operations, to help improve performance and reliability. See the Web Content Manager Cache Parameters topic for more information.","title":"Web Content Manager Advanced Cache Options"},{"location":"what%27s-new/new_noncf196/#content-security-policy","text":"The Content-Security-Policy header is used by modern browsers to enhance the security of HCL Digital Experience site documents or web pages by allowing HCL Digital Experience administrators or developers declare which dynamic resources are allowed to load. With HCL Digital Experience Container Update CF192 and later releases, developers can apply platform support and guidance to update their DX sites to validate trusted sources before rendering pages to end users. See the Content Security Policy Help Center topic for more information.","title":"Content Security Policy"},{"location":"what%27s-new/new_noncf196/#enhanced-cross-origin-resource-sharing-configuration","text":"Enhanced Cross Origin Resource Sharing Configuration adds new options for HCL Digital Experience administrators to set configuration for CORS using a WP configuration service in the IBM WebSphere Application Server resource environment provider. See Enhanced Cross Origin Resource Sharing Configuration for more information.","title":"Enhanced Cross Origin Resource Sharing Configuration"},{"location":"what%27s-new/new_noncf196/#hcl-digital-experience-95-integration-with-hcl-unica-discover","text":"Integration of HCL Digital Experience 9.5 sites with HCL Unica Discover enables DX site managers and marketers to access deep insight analytics and session replay services to assess the effectiveness of DX site pages with end user audiences. See the Integrate HCL Unica Discover with HCL Digital Experience topic for more information.","title":"HCL Digital Experience 9.5 Integration with HCL Unica Discover"},{"location":"what%27s-new/new_noncf196/#deploy-hcl-dx-95-using-docker-compose","text":"Beginning with HCL DX 9.5 CF196, administrators and developers can deploy HCL DX 9.5 using Docker Compose, for non-production use. Docker Compose scripts for HCL DX 9.5, installation and configuration instructions for non-production use are available in the HCL Software Github . See the Docker image deployment using Docker Compose topic for more information.","title":"Deploy HCL DX 9.5 using Docker Compose"},{"location":"what%27s-new/new_noncf196/#deploy-hcl-digital-experience-95-on-hcl-solution-factory-sofy","text":"The HCL Solution Factory (SoFy) platform offers the ability for organizations to quickly prototype and test assets and can enable organizations to speed cloud-native adoption. Visit HCL SoFy to access HCL Digital Experience 9.5 and other HCL software offerings to quickly assess and test cloud-native strategies, gain hands-on experience working with demonstration assets, to see what best fits adoption plans. View this online tutorial: Deploy HCL Digital Experience in Minutes with HCL SoFy","title":"Deploy HCL Digital Experience 9.5 on HCL Solution Factory (SoFy)"},{"location":"what%27s-new/new_noncf196/#hcl-digital-experience-95-integration-with-hcl-commerce","text":"HCL Commerce integration with HCL Digital Experience allows content and digital assets managed within HCL Digital Experience to be utilized in any Commerce store. Visit the HCL Commerce - Digital Experience integration resource for more information and pre-requisites.","title":"HCL Digital Experience 9.5 Integration with HCL Commerce"},{"location":"what%27s-new/new_noncf196/#new-digital-experience-rest-apis","text":"New HCL DX APIs are available with the HCL DX CF196 release: Web Content Manager Multilingual Solution APIs Web Content Manager Comments API Web Content Manager Find Rendering and Script Portlet References Web Content Manager Access Control Filter REST API Clear Theme Cache API Work with Deleted Web Content Items","title":"New Digital Experience REST APIs"},{"location":"what%27s-new/new_noncf196/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in videos in following HCL Digital Experience Help Center topics: Configuring user access permissions to Digital Asset Management assets Deep Dive: Progressive Web Applications with HCL DX 9.5 Content Security Policy with HCL Digital 9.5 Getting Started with DXClient on Red Hat OpenShift using HCL Digital Experience CF194 Understanding the Core Persistent Volumes in HCL Digital Experience Container Update CF194 Create a WebDAV Theme copy using HCL Digital Experience 9.5 Create and apply A/B personalized scenarios with HCL Digital Experience Deploy HCL Digital Experience in Minutes using HCL Solution Factory Using the dxctl tool to Update Digital Experience 9.5 on Red Hat OpenShift How to Upload HCL Digital Experience 9.5 CF Container Images to a Private Repository HCL Digital Experience 9.5 Container Deployment Using the dxctl tool to deploy Digital Experience 9.5 on Red Hat OpenShift Create a WAR-based theme copy on HCL Digital Experience 9.5 Parent topic: Latest Combined CF and Container updates","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"what%27s-new/new_noncf197/","text":"What's new with CF197? Combined Cumulative Fix (CF197) includes new features and software fixes for the latest version of HCL Digital Experience. Release updates for both on \u2013 premises platforms and container deployments are available. This HCL Digital Experience 9.5 Container Update and CF197 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management and Experience API, Design Studio (Beta) for Container deployments, New CICD release process artifacts, new HCL Digital Experience \u2018How To\u2019 videos and more. Go to the HCL Software Support Site for the latest list of software fixes. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include new release artifact types supporting Obtain failed Syndication reports for single or multiple items, and delete Digital Asset Management inactive schema from Persistence are provided in CF197. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in videos in following HCL Digital Experience Help Center topics: Updating the HCL DX 9.5 Portal & IBM WebSphere Application Server Administrator Secrets in OpenShift and Kubernetes Use Docker Compose to install HCL DX 9.5 with Cloud Components for Developer Use Parent topic: Latest Combined CF and Container updates","title":"What's new with CF197?"},{"location":"what%27s-new/new_noncf197/#whats-new-with-cf197","text":"Combined Cumulative Fix (CF197) includes new features and software fixes for the latest version of HCL Digital Experience. Release updates for both on \u2013 premises platforms and container deployments are available. This HCL Digital Experience 9.5 Container Update and CF197 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management and Experience API, Design Studio (Beta) for Container deployments, New CICD release process artifacts, new HCL Digital Experience \u2018How To\u2019 videos and more. Go to the HCL Software Support Site for the latest list of software fixes. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information.","title":"What's new with CF197?"},{"location":"what%27s-new/new_noncf197/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include new release artifact types supporting Obtain failed Syndication reports for single or multiple items, and delete Digital Asset Management inactive schema from Persistence are provided in CF197. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"what%27s-new/new_noncf197/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in videos in following HCL Digital Experience Help Center topics: Updating the HCL DX 9.5 Portal & IBM WebSphere Application Server Administrator Secrets in OpenShift and Kubernetes Use Docker Compose to install HCL DX 9.5 with Cloud Components for Developer Use Parent topic: Latest Combined CF and Container updates","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"what%27s-new/new_noncf198/","text":"What's new with CF198? Combined Cumulative Fix (CF198) includes new features and software fixes for the latest version of HCL Digital Experience. Release updates for both on-premises platforms and container deployments are available. This HCL Digital Experience 9.5 Container Update and CF198 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, the new Site Manager Custom Layout Editor, HCL Digital Experience Technical Articles, \u2018How To\u2019 videos, and more. Go to the HCL Software Support Site for the latest list of software fixes. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include process definitions to automate select IBM WebSphere Application Server settings during deployment, generate import and export lists of virtual portals, and import and export personalization rules. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. New HCL Digital Experience Site Manager Custom Layout Editor Beginning with HCL Digital Experience CF198, a Custom Layout Editor is available for use with HCL DX Site Manager capabilities, reducing custom development requirements for site designers implementing custom layouts in DX site pages. See the Using the Digital Experience Custom Layout Editor topic for more information. New Experience APIs New HCL Experience Web Content Manager REST APIs are available for new menu component update, collection responses conversion utilities, the ability to use the page editor to edit elements inline and update metadata, set locations for sites, set HTML tags for text elements, and more. See the HCL DX Experience API topic for more information Rationalized CF release versioning Beginning with HCL Digital Experience CF196, single versioning is used for both container and on-premise Combined Cumulative Fixes (CFs). This means that fixes for both deployments are included into one CF deliverable. And although the versioning is the same, CFs are packaged separately for on-premises and container deployments. See the CF release versioning and update path topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in articles and videos from the following HCL Digital Experience Help Center topics: Video: Creating an HCL Digital Experience 9.5 CF196 cluster Step-by-step guide: How to deploy HCL DX Container update CF197 and later to Microsoft Azure Kubernetes Service Parent topic: Latest Combined CF and Container updates","title":"What's new with CF198?"},{"location":"what%27s-new/new_noncf198/#whats-new-with-cf198","text":"Combined Cumulative Fix (CF198) includes new features and software fixes for the latest version of HCL Digital Experience. Release updates for both on-premises platforms and container deployments are available. This HCL Digital Experience 9.5 Container Update and CF198 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, the new Site Manager Custom Layout Editor, HCL Digital Experience Technical Articles, \u2018How To\u2019 videos, and more. Go to the HCL Software Support Site for the latest list of software fixes. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information.","title":"What's new with CF198?"},{"location":"what%27s-new/new_noncf198/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include process definitions to automate select IBM WebSphere Application Server settings during deployment, generate import and export lists of virtual portals, and import and export personalization rules. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"what%27s-new/new_noncf198/#new-hcl-digital-experience-site-manager-custom-layout-editor","text":"Beginning with HCL Digital Experience CF198, a Custom Layout Editor is available for use with HCL DX Site Manager capabilities, reducing custom development requirements for site designers implementing custom layouts in DX site pages. See the Using the Digital Experience Custom Layout Editor topic for more information.","title":"New HCL Digital Experience Site Manager Custom Layout Editor"},{"location":"what%27s-new/new_noncf198/#new-experience-apis","text":"New HCL Experience Web Content Manager REST APIs are available for new menu component update, collection responses conversion utilities, the ability to use the page editor to edit elements inline and update metadata, set locations for sites, set HTML tags for text elements, and more. See the HCL DX Experience API topic for more information","title":"New Experience APIs"},{"location":"what%27s-new/new_noncf198/#rationalized-cf-release-versioning","text":"Beginning with HCL Digital Experience CF196, single versioning is used for both container and on-premise Combined Cumulative Fixes (CFs). This means that fixes for both deployments are included into one CF deliverable. And although the versioning is the same, CFs are packaged separately for on-premises and container deployments. See the CF release versioning and update path topic for more information.","title":"Rationalized CF release versioning"},{"location":"what%27s-new/new_noncf198/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in articles and videos from the following HCL Digital Experience Help Center topics: Video: Creating an HCL Digital Experience 9.5 CF196 cluster Step-by-step guide: How to deploy HCL DX Container update CF197 and later to Microsoft Azure Kubernetes Service Parent topic: Latest Combined CF and Container updates","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"what%27s-new/new_noncf199/","text":"What's new with CF199? Combined Cumulative Fix (CF199) includes new features and software fixes for the latest version of HCL Digital Experience. Release updates for both on-premises platforms and container deployments are available. This HCL Digital Experience 9.5 Container Update and CF199 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, new \u201cHow To\u201d videos, and more. Go to the HCL Software Support Site for the latest list of software fixes. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include process definitions to automate Export and Import of select IBM WebSphere Application Server Resource Provider settings during deployment, and to create Syndication relationships and credential vault settings. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. End of Support for HCL Digital Experience Deprecated Features The following list of HCL Digital Experience deprecated features will reach end of support beginning with Container update and CF200 release. Refer to the Deprecated features and themes for HCL Digital Experience 9.5 topic in the Help Center. New Experience API V2 Web Content Manager REST APIs video See the HCL Experience API topic for the video. New Experience APIs New HCL Experience APIs are available for creating, updating and deleting Design Studio (Beta) sites, pages, and containers. New Web Content Manager REST V2 APIs are available for creating content templates, categories, and more. See the HCL Experience API topic for more information. New REST APIs to Configure Remote Search Beginning with HCL Digital Experience CF and Container Update CF199, additional REST services enable administrators and developers to programatically configure remote search in on-premises and container-based Digital Experience deployments. See the Configure Remote Search using REST APIs topic for more information. Parent topic: Latest Combined CF and Container updates","title":"What's new with CF199?"},{"location":"what%27s-new/new_noncf199/#whats-new-with-cf199","text":"Combined Cumulative Fix (CF199) includes new features and software fixes for the latest version of HCL Digital Experience. Release updates for both on-premises platforms and container deployments are available. This HCL Digital Experience 9.5 Container Update and CF199 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, new \u201cHow To\u201d videos, and more. Go to the HCL Software Support Site for the latest list of software fixes. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information.","title":"What's new with CF199?"},{"location":"what%27s-new/new_noncf199/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include process definitions to automate Export and Import of select IBM WebSphere Application Server Resource Provider settings during deployment, and to create Syndication relationships and credential vault settings. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"what%27s-new/new_noncf199/#end-of-support-for-hcl-digital-experience-deprecated-features","text":"The following list of HCL Digital Experience deprecated features will reach end of support beginning with Container update and CF200 release. Refer to the Deprecated features and themes for HCL Digital Experience 9.5 topic in the Help Center.","title":"End of Support for HCL Digital Experience Deprecated Features"},{"location":"what%27s-new/new_noncf199/#new-experience-api-v2-web-content-manager-rest-apis-video","text":"See the HCL Experience API topic for the video.","title":"New Experience API V2 Web Content Manager REST APIs video"},{"location":"what%27s-new/new_noncf199/#new-experience-apis","text":"New HCL Experience APIs are available for creating, updating and deleting Design Studio (Beta) sites, pages, and containers. New Web Content Manager REST V2 APIs are available for creating content templates, categories, and more. See the HCL Experience API topic for more information.","title":"New Experience APIs"},{"location":"what%27s-new/new_noncf199/#new-rest-apis-to-configure-remote-search","text":"Beginning with HCL Digital Experience CF and Container Update CF199, additional REST services enable administrators and developers to programatically configure remote search in on-premises and container-based Digital Experience deployments. See the Configure Remote Search using REST APIs topic for more information. Parent topic: Latest Combined CF and Container updates","title":"New REST APIs to Configure Remote Search"},{"location":"what%27s-new/newcf182/","text":"What's new in CF182? Containers This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. This update adds new guidance to minimize downtime when updating container-based deployments with new artifacts, configuration options for Digital Asset Management storage, and more. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. Deploy HCL DX 9.5 Container updates with minimal operations downtime This topic provides guidance to update artifacts in HCL Digital Experience 9.5 container deployments while minimizing operations downtime, and notes how processes and tools to support these efforts differ across Kubernetes container-based and non-Kubernetes HCL Digital Experience platform deployments. See Deploying HCL DX 9.5 Container artifact updates with minimal operations downtime for details. Optional Digital Asset Management Storage Configuration Settings This topic outlines optional configuration steps to tune Digital Asset Management storage services Storage Class and Volume. See Optional Digital Asset Management Storage Configuration Settings for details. Parent topic: Container Update releases","title":"What's new in CF182? Containers"},{"location":"what%27s-new/newcf182/#whats-new-in-cf182-containers","text":"This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. This update adds new guidance to minimize downtime when updating container-based deployments with new artifacts, configuration options for Digital Asset Management storage, and more. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information.","title":"What's new in CF182? Containers"},{"location":"what%27s-new/newcf182/#deploy-hcl-dx-95-container-updates-with-minimal-operations-downtime","text":"This topic provides guidance to update artifacts in HCL Digital Experience 9.5 container deployments while minimizing operations downtime, and notes how processes and tools to support these efforts differ across Kubernetes container-based and non-Kubernetes HCL Digital Experience platform deployments. See Deploying HCL DX 9.5 Container artifact updates with minimal operations downtime for details.","title":"Deploy HCL DX 9.5 Container updates with minimal operations downtime"},{"location":"what%27s-new/newcf182/#optional-digital-asset-management-storage-configuration-settings","text":"This topic outlines optional configuration steps to tune Digital Asset Management storage services Storage Class and Volume. See Optional Digital Asset Management Storage Configuration Settings for details. Parent topic: Container Update releases","title":"Optional Digital Asset Management Storage Configuration Settings"},{"location":"what%27s-new/newcf183/","text":"What's new in CF183? Containers This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include guidance to deploy to Microsoft Azure Kubernetes Service (AKS), new Web Content Manager REST APIs and Search Query examples, guidance to deploy the Remote Search image on Red Hat OpenShift, new HCL Digital Experience \u2018How To\u2019 videos, and more. Go to the HCL Software Support Site for the list of software fixes, including Container Update release CF183. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Deploy HCL DX 9.5 Container CF182 or higher to Microsoft Azure Kubernetes Service (AKS) Learn how to deploy HCL Digital Experience (DX) 9.5 CF182 and higher container releases along with Ambassador to Kubernetes, as verified in Microsoft Azure Kubernetes Service (AKS). See the HCL Digital Experience 9.5 Deployment and Deploy HCL Digital Experience 9.5 Container to Microsoft Azure Kubernetes Service (AKS) topics for more information. Web Content Manager Lock/Unlock API The Web Content Manager Lock/Unlock API lets you lock and unlock WCM content components, authoring templates, and item. It can also extend the WCM Query API. See the Web Content Manager Lock/Unlock AP I topic for more information. Content Template Create/Update Option Element Selection API The Web Content Manager Create/Update Option Element Selection API lets you create or update an Option Selection Element in a Content Template. See the Create or update an Option Selection Element topic for more information. Search Component Results Display examples A search element defines the layout of a form that is used to display search results. See the Search Component Results Display topic for examples of how to design your search results. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step by step guidance for HCL Digital Experience practitioners presented in several new videos. See the following HCL Digital Experience Help Center topics: Remote Search Service Transfer HCL Digital Experience 9.5 Container default database to IBM DB2 Create a web content library Virtual portals Backup and restore Configuration Wizard Combined Cumulative Fix Install Rich text editor toolbar configuration options Data collection and symptom analysis Parent topic: Container Update releases","title":"What's new in CF183? Containers"},{"location":"what%27s-new/newcf183/#whats-new-in-cf183-containers","text":"This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include guidance to deploy to Microsoft Azure Kubernetes Service (AKS), new Web Content Manager REST APIs and Search Query examples, guidance to deploy the Remote Search image on Red Hat OpenShift, new HCL Digital Experience \u2018How To\u2019 videos, and more. Go to the HCL Software Support Site for the list of software fixes, including Container Update release CF183. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in CF183? Containers"},{"location":"what%27s-new/newcf183/#deploy-hcl-dx-95-container-cf182-or-higher-to-microsoft-azure-kubernetes-service-aks","text":"Learn how to deploy HCL Digital Experience (DX) 9.5 CF182 and higher container releases along with Ambassador to Kubernetes, as verified in Microsoft Azure Kubernetes Service (AKS). See the HCL Digital Experience 9.5 Deployment and Deploy HCL Digital Experience 9.5 Container to Microsoft Azure Kubernetes Service (AKS) topics for more information.","title":"Deploy HCL DX 9.5 Container CF182 or higher to Microsoft Azure Kubernetes Service (AKS)"},{"location":"what%27s-new/newcf183/#web-content-manager-lockunlock-api","text":"The Web Content Manager Lock/Unlock API lets you lock and unlock WCM content components, authoring templates, and item. It can also extend the WCM Query API. See the Web Content Manager Lock/Unlock AP I topic for more information.","title":"Web Content Manager Lock/Unlock API"},{"location":"what%27s-new/newcf183/#content-template-createupdate-option-element-selection-api","text":"The Web Content Manager Create/Update Option Element Selection API lets you create or update an Option Selection Element in a Content Template. See the Create or update an Option Selection Element topic for more information.","title":"Content Template Create/Update Option Element Selection API"},{"location":"what%27s-new/newcf183/#search-component-results-display-examples","text":"A search element defines the layout of a form that is used to display search results. See the Search Component Results Display topic for examples of how to design your search results.","title":"Search Component Results Display examples"},{"location":"what%27s-new/newcf183/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step by step guidance for HCL Digital Experience practitioners presented in several new videos. See the following HCL Digital Experience Help Center topics: Remote Search Service Transfer HCL Digital Experience 9.5 Container default database to IBM DB2 Create a web content library Virtual portals Backup and restore Configuration Wizard Combined Cumulative Fix Install Rich text editor toolbar configuration options Data collection and symptom analysis Parent topic: Container Update releases","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"what%27s-new/newcf184/","text":"What's new in CF184? Containers This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include additional guidance to deploy to Microsoft Azure Kubernetes Service (AKS), new Web Content Manager REST APIs supporting Syndication options, HCL Digital Experience API published to HCL DX GitHub, new HCL Digital Experience \u2018How To\u2019 videos, and more. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Web Content Manager Syndication REST APIs The Web Content Manager Syndication REST APIs let you control syndication processes. See the Web Content Manager Syndication REST APIs topic for more information. Access the HCL Experience API in HCL DX GitHub The HCL Experience API is a set of OpenAPI-compliant REST APIs available for customers deploying HCL Digital Experience 9.5 containers on supported Kubernetes platforms. It supports the integration and management of HCL Digital Experience content and functionality to any digital channel using any front-end development framework. Developers may also now access this API published to the HCL DX GitHub repository. See the Experience API topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step by step guidance for HCL Digital Experience practitioners presented in several new videos. See the following HCL Digital Experience Help Center topics: HCL Digital Experience Combined Cumulative Fix (CF) Installation Deploy HCL Digital Experience 9.5 to Red Hat OpenShift Deploy Digital Asset Management, Content Composer and Experience API to Red Hat OpenShift Parent topic: Container Update releases","title":"What's new in CF184? Containers"},{"location":"what%27s-new/newcf184/#whats-new-in-cf184-containers","text":"This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include additional guidance to deploy to Microsoft Azure Kubernetes Service (AKS), new Web Content Manager REST APIs supporting Syndication options, HCL Digital Experience API published to HCL DX GitHub, new HCL Digital Experience \u2018How To\u2019 videos, and more. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in CF184? Containers"},{"location":"what%27s-new/newcf184/#web-content-manager-syndication-rest-apis","text":"The Web Content Manager Syndication REST APIs let you control syndication processes. See the Web Content Manager Syndication REST APIs topic for more information.","title":"Web Content Manager Syndication REST APIs"},{"location":"what%27s-new/newcf184/#access-the-hcl-experience-api-in-hcl-dx-github","text":"The HCL Experience API is a set of OpenAPI-compliant REST APIs available for customers deploying HCL Digital Experience 9.5 containers on supported Kubernetes platforms. It supports the integration and management of HCL Digital Experience content and functionality to any digital channel using any front-end development framework. Developers may also now access this API published to the HCL DX GitHub repository. See the Experience API topic for more information.","title":"Access the HCL Experience API in HCL DX GitHub"},{"location":"what%27s-new/newcf184/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step by step guidance for HCL Digital Experience practitioners presented in several new videos. See the following HCL Digital Experience Help Center topics: HCL Digital Experience Combined Cumulative Fix (CF) Installation Deploy HCL Digital Experience 9.5 to Red Hat OpenShift Deploy Digital Asset Management, Content Composer and Experience API to Red Hat OpenShift Parent topic: Container Update releases","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"what%27s-new/newcf19/","text":"What's new in CF19? Containers This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include additional guidance to deploy to Google Kubernetes Engine (GKE), Hybrid deployment support, Progressive Web Application delivery, Google Analytics integration, Mobile Preview, DXClient and DXConnect tooling supporting CICD release processes, Kaltura video support, new Web Content Manager REST APIs, new HCL Digital Experience \u2018How To\u2019 videos, and more. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal. Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Deploy HCL Digital Experience 9.5 Container to Google Kubernetes Engine (GKE) Learn how to deploy HCL Digital Experience (DX) 9.5 CF19 and higher container release along with Ambassador to Kubernetes, as verified in Google Kubernetes Engine (GKE) . See the Deploy HCL Digital Experience 9.5 Container to Google Kubernetes Engine (GKE) topic for more information. Hybrid Deployment The HCL Digital Experience 9.5 Hybrid deployment and topics deliver capability to deploy and manage HCL Digital Experience 9.5 core Portal Server and Web Content Manager services on premises, and connect to cloud native components Digital Asset Management, Content Composer, Experience API and related services in a production environment. See the Hybrid Deployment topic for more information. Progressive Web Application support Develop support that adds native mobile application experience and performance to your web site using browser-based functionality. See the Progressive Web Application topic for more information. Google Analytics integration Learn how to set up integration of Digital Experience sites with Google Analytics and view the resulting web analytics tracking to assess the effectiveness of your DX site pages with end user audiences. See the Integrate Google Analytics with HCL Digital Experience topic for more information. Mobile Preview Use the Mobile Preview simulator to view the presentation of Digital Experience site page components on select mobile devices. See the Mobile Preview topic for more information. DXClient and DXConnect tooling supporting CICD release processes HCL Digital Experience 9.5 CF19 and higher includes a DXClient toolset, and DX Connect servlet that provides developers and administrators with an approach to deploy changes or improvements to the HCL Digital Experience platform,and automate processes in the development and delivery process. Note: The DXClient tool is not supported for use with HCL DX 9.5 deployments in Red Hat OpenShift or supported Kubernetes platforms. Use of the DXClient tool with those platforms will be available in future HCL DX 9.5 update releases. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. Digital Asset Management and Kaltura Integration Learn how to configure Kaltura Video Content Management System integration to accelerate HCL Digital Asset Management rich media integration to HCL Digital Experience site pages and content. See the Configure DAM - Kaltura integration topic for more information. New Digital Experience REST APIs New HCL DX APIs are introduced with the HCL DX CF19 release: Web content image renditions interactions REST API Web Content Library Locale Query Workflow Comments API Using XML Access to export and import Digital Asset Management assets New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step by step guidance for HCL Digital Experience practitioners presented in several new videos. See the following HCL Digital Experience Help Center topics: Configure the OpenLDAP container image to the HCL DX 9.5 Container Deployment How to manage syndicators and subscribers Parent topic: Container Update releases","title":"What's new in CF19? Containers"},{"location":"what%27s-new/newcf19/#whats-new-in-cf19-containers","text":"This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include additional guidance to deploy to Google Kubernetes Engine (GKE), Hybrid deployment support, Progressive Web Application delivery, Google Analytics integration, Mobile Preview, DXClient and DXConnect tooling supporting CICD release processes, Kaltura video support, new Web Content Manager REST APIs, new HCL Digital Experience \u2018How To\u2019 videos, and more. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal. Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in CF19? Containers"},{"location":"what%27s-new/newcf19/#deploy-hcl-digital-experience-95-container-to-google-kubernetes-engine-gke","text":"Learn how to deploy HCL Digital Experience (DX) 9.5 CF19 and higher container release along with Ambassador to Kubernetes, as verified in Google Kubernetes Engine (GKE) . See the Deploy HCL Digital Experience 9.5 Container to Google Kubernetes Engine (GKE) topic for more information.","title":"Deploy HCL Digital Experience 9.5 Container to Google Kubernetes Engine (GKE)"},{"location":"what%27s-new/newcf19/#hybrid-deployment","text":"The HCL Digital Experience 9.5 Hybrid deployment and topics deliver capability to deploy and manage HCL Digital Experience 9.5 core Portal Server and Web Content Manager services on premises, and connect to cloud native components Digital Asset Management, Content Composer, Experience API and related services in a production environment. See the Hybrid Deployment topic for more information.","title":"Hybrid Deployment"},{"location":"what%27s-new/newcf19/#progressive-web-application-support","text":"Develop support that adds native mobile application experience and performance to your web site using browser-based functionality. See the Progressive Web Application topic for more information.","title":"Progressive Web Application support"},{"location":"what%27s-new/newcf19/#google-analytics-integration","text":"Learn how to set up integration of Digital Experience sites with Google Analytics and view the resulting web analytics tracking to assess the effectiveness of your DX site pages with end user audiences. See the Integrate Google Analytics with HCL Digital Experience topic for more information.","title":"Google Analytics integration"},{"location":"what%27s-new/newcf19/#mobile-preview","text":"Use the Mobile Preview simulator to view the presentation of Digital Experience site page components on select mobile devices. See the Mobile Preview topic for more information.","title":"Mobile Preview"},{"location":"what%27s-new/newcf19/#dxclient-and-dxconnect-tooling-supporting-cicd-release-processes","text":"HCL Digital Experience 9.5 CF19 and higher includes a DXClient toolset, and DX Connect servlet that provides developers and administrators with an approach to deploy changes or improvements to the HCL Digital Experience platform,and automate processes in the development and delivery process. Note: The DXClient tool is not supported for use with HCL DX 9.5 deployments in Red Hat OpenShift or supported Kubernetes platforms. Use of the DXClient tool with those platforms will be available in future HCL DX 9.5 update releases. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"DXClient and DXConnect tooling supporting CICD release processes"},{"location":"what%27s-new/newcf19/#digital-asset-management-and-kaltura-integration","text":"Learn how to configure Kaltura Video Content Management System integration to accelerate HCL Digital Asset Management rich media integration to HCL Digital Experience site pages and content. See the Configure DAM - Kaltura integration topic for more information.","title":"Digital Asset Management and Kaltura Integration"},{"location":"what%27s-new/newcf19/#new-digital-experience-rest-apis","text":"New HCL DX APIs are introduced with the HCL DX CF19 release: Web content image renditions interactions REST API Web Content Library Locale Query Workflow Comments API Using XML Access to export and import Digital Asset Management assets","title":"New Digital Experience REST APIs"},{"location":"what%27s-new/newcf19/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step by step guidance for HCL Digital Experience practitioners presented in several new videos. See the following HCL Digital Experience Help Center topics: Configure the OpenLDAP container image to the HCL DX 9.5 Container Deployment How to manage syndicators and subscribers Parent topic: Container Update releases","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"what%27s-new/newcf191/","text":"What's new in CF191? Containers This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images, that may be used for new deployments only. Upgrading from a previous DX 9.5 Container Release to CF191 is not supported. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Deploy HCL Digital Experience 9.5 on HCL Solution Factory The HCL Solution Factory platform offers the ability for organizations to quickly prototype and test assets and can enable organizations to speed cloud-native adoption. Visit HCL Solution Factory to access HCL Digital Experience 9.5 and other HCL software offerings to quickly assess and test cloud-native strategies to see what best fits adoption plans. View this online tutorial \u201c Deploy HCL Digital Experience in Minutes with HCL SoFy \u201d HCL Digital Experience 9.5 Integration with HCL Commerce HCL Commerce integration with HCL Digital Experience allows content and digital assets managed within HCL Digital Experience to be utilized in any Commerce store. Visit the HCL Commerce Help Center Digital Experience integration for more information and pre-requisites. Parent topic: Container Update releases","title":"What's new in CF191? Containers"},{"location":"what%27s-new/newcf191/#whats-new-in-cf191-containers","text":"This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images, that may be used for new deployments only. Upgrading from a previous DX 9.5 Container Release to CF191 is not supported. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in CF191? Containers"},{"location":"what%27s-new/newcf191/#deploy-hcl-digital-experience-95-on-hcl-solution-factory","text":"The HCL Solution Factory platform offers the ability for organizations to quickly prototype and test assets and can enable organizations to speed cloud-native adoption. Visit HCL Solution Factory to access HCL Digital Experience 9.5 and other HCL software offerings to quickly assess and test cloud-native strategies to see what best fits adoption plans. View this online tutorial \u201c Deploy HCL Digital Experience in Minutes with HCL SoFy \u201d","title":"Deploy HCL Digital Experience 9.5 on HCL Solution Factory"},{"location":"what%27s-new/newcf191/#hcl-digital-experience-95-integration-with-hcl-commerce","text":"HCL Commerce integration with HCL Digital Experience allows content and digital assets managed within HCL Digital Experience to be utilized in any Commerce store. Visit the HCL Commerce Help Center Digital Experience integration for more information and pre-requisites. Parent topic: Container Update releases","title":"HCL Digital Experience 9.5 Integration with HCL Commerce"},{"location":"what%27s-new/newcf192/","text":"What's new in CF192? Containers This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. HCL Digital Experience 9.5 Docker and Container Initialization Performance Beginning with the HCL Digital Experience 9.5 Container Update CF192 release, DX 9.5 Docker and container initialization performance is improved. See the HCL Digital Experience 9.5 Docker and Container Initialization Performance Help Center topic for more information. HCL Digital Experience 9.5 Container Core Transaction Logging Beginning with the HCL Digital Experience 9.5 Container Update CF192 release, transaction logging for the DX Docker Core image is updated to improve performance. See the Logging and tracing for Containers and new services Help Center topic for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform,and automate processes in the development and delivery process. New release artifact types supporting Script Application Undeploy and Restore, and Deploy Theme. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. New Content Composer Features New Content Composer features are added with HCL Digital Experience Container Update CF192, including a new Version Comparison interface and capabilities to View and Filter Workflow comments, and more. See the HCL Content Composer Help Center topic for additional information. New Digital Asset Management Features New Digital Asset Management Features are added with HCL Digital Experience Container Update CF192, including enhanced crop functionality, Kaltura video player support, thumbnail preview support, asset size filter, Renditions and Versioning support, and more. See the HCL Digital Asset Management Help Center topic for additional information. HCL Digital Experience 9.5 Integration with HCL Unica Discover Integration of HCL Digital Experience 9.5 sites with HCL Unica Discover enables DX site managers and marketers to access deep insight analytics and session replay services to assess the effectiveness of DX site pages with end user audiences. See the Integrate HCL Unica Discover with HCL Digital Experience topic for more information. Content Security Policy The Content-Security-Policy header is used by modern browsers to enhance the security of HCL Digital Experience site documents or web pages by allowing HCL Digital Experience administrators or developers declare which dynamic resources are allowed to load. With HCL Digital Experience Container Update CF192 and later releases, developers can apply platform support and guidance to update their DX sites to validate trusted sources before rendering pages to end users. See the Content Security Policy Help Center topic for more information. New Digital Experience REST APIs New HCL DX APIs are introduced with the HCL DX CF192 Container Update release: Using the WCM Add Comment API Web Content Manager Find Rendering and Script Portlet References Web Content Manager Access Control Filter REST API Clear Theme Cache API Work with Deleted Web Content Items Web Content Manager Multilingual Solution APIs New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: HCL Digital Experience 9.5 Container Deployment Using the dxctl tool to deploy Digital Experience 9.5 on Red Hat OpenShift Create a WAR-based theme copy on HCL Digital Experience 9.5 Parent topic: Container Update releases","title":"What's new in CF192? Containers"},{"location":"what%27s-new/newcf192/#whats-new-in-cf192-containers","text":"This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in CF192? Containers"},{"location":"what%27s-new/newcf192/#hcl-digital-experience-95-docker-and-container-initialization-performance","text":"Beginning with the HCL Digital Experience 9.5 Container Update CF192 release, DX 9.5 Docker and container initialization performance is improved. See the HCL Digital Experience 9.5 Docker and Container Initialization Performance Help Center topic for more information.","title":"HCL Digital Experience 9.5 Docker and Container Initialization Performance"},{"location":"what%27s-new/newcf192/#hcl-digital-experience-95-container-core-transaction-logging","text":"Beginning with the HCL Digital Experience 9.5 Container Update CF192 release, transaction logging for the DX Docker Core image is updated to improve performance. See the Logging and tracing for Containers and new services Help Center topic for more information.","title":"HCL Digital Experience 9.5 Container Core Transaction Logging"},{"location":"what%27s-new/newcf192/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform,and automate processes in the development and delivery process. New release artifact types supporting Script Application Undeploy and Restore, and Deploy Theme. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"what%27s-new/newcf192/#new-content-composer-features","text":"New Content Composer features are added with HCL Digital Experience Container Update CF192, including a new Version Comparison interface and capabilities to View and Filter Workflow comments, and more. See the HCL Content Composer Help Center topic for additional information.","title":"New Content Composer Features"},{"location":"what%27s-new/newcf192/#new-digital-asset-management-features","text":"New Digital Asset Management Features are added with HCL Digital Experience Container Update CF192, including enhanced crop functionality, Kaltura video player support, thumbnail preview support, asset size filter, Renditions and Versioning support, and more. See the HCL Digital Asset Management Help Center topic for additional information.","title":"New Digital Asset Management Features"},{"location":"what%27s-new/newcf192/#hcl-digital-experience-95-integration-with-hcl-unica-discover","text":"Integration of HCL Digital Experience 9.5 sites with HCL Unica Discover enables DX site managers and marketers to access deep insight analytics and session replay services to assess the effectiveness of DX site pages with end user audiences. See the Integrate HCL Unica Discover with HCL Digital Experience topic for more information.","title":"HCL Digital Experience 9.5 Integration with HCL Unica Discover"},{"location":"what%27s-new/newcf192/#content-security-policy","text":"The Content-Security-Policy header is used by modern browsers to enhance the security of HCL Digital Experience site documents or web pages by allowing HCL Digital Experience administrators or developers declare which dynamic resources are allowed to load. With HCL Digital Experience Container Update CF192 and later releases, developers can apply platform support and guidance to update their DX sites to validate trusted sources before rendering pages to end users. See the Content Security Policy Help Center topic for more information.","title":"Content Security Policy"},{"location":"what%27s-new/newcf192/#new-digital-experience-rest-apis","text":"New HCL DX APIs are introduced with the HCL DX CF192 Container Update release: Using the WCM Add Comment API Web Content Manager Find Rendering and Script Portlet References Web Content Manager Access Control Filter REST API Clear Theme Cache API Work with Deleted Web Content Items Web Content Manager Multilingual Solution APIs","title":"New Digital Experience REST APIs"},{"location":"what%27s-new/newcf192/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: HCL Digital Experience 9.5 Container Deployment Using the dxctl tool to deploy Digital Experience 9.5 on Red Hat OpenShift Create a WAR-based theme copy on HCL Digital Experience 9.5 Parent topic: Container Update releases","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"what%27s-new/newcf193/","text":"What's new in CF193? Containers This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New and updated feature references are detailed here. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Enable Presentation of Locales in Friendly URLs Beginning with the HCL Digital Experience 9.5 Container Update CF193 release, enablement is provided to present friendly URLs with locale specific definitions when multi-lingual page versions are requested. This enablement can also improve SEO results when users search for language-specific DX page topics. See the Enabling presentation of locales in friendly URLs topic for more information. Theme Editor Portlet The Theme Editor portlet is a new addition to HCL Digital Experience Container Update CF193 and higher release capabilities. The portlet allows an administrator to edit static theme resources in WebDAV without the use of a WebDAV client or tool. See the Theme Editor Portlet topic for more information. HCL Digital Experience 9.5 Container Custom Context Root URL Beginning with HCL DX 9.5 Container Update CF193 release, you can define the custom context root URLs when deploying your DX 9.5 software to the supported container platforms. See the Customizing the Digital Experience Portal URL when deployed to Container platforms topic for more information. New Digital Asset Management Features New Digital Asset Management Features are added with HCL Digital Experience Container Update CF193, and include the ability to filter Digital Assets by size. See the HCL Digital Asset Management Help Center topic for additional information. Change language presented in the HCL Digital Experience Theme Beginning with HCL DX 9.5 Container Update CF193 release, you can switch the language presented in your Digital Experience theme. An example is provided in the Woodburn Studio demo site supporting presentations for French, Spanish, and English languages. See the How to switch the languages in the Digital Experience theme topic for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. New release artifact types supporting Deploy DX Core, Manage Syndicator, and Manage Subscriber tasks are provided in Container Update CF193. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. New Digital Experience REST APIs New and updated HCL DX APIs are introduced with the HCL DX CF193 Container Update release: Web Content Manager Multilingual Solution APIs HCL Digital Experience 9.5 Container Platform Support Matrix View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix Help Center topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Deploy HCL Digital Experience in Minutes using HCL Solution Factory Using the dxctl tool to Update Digital Experience 9.5 on Red Hat OpenShift How to Upload HCL Digital Experience 9.5 CF Container Images to a Private Repository Parent topic: Container Update releases","title":"What's new in CF193? Containers"},{"location":"what%27s-new/newcf193/#whats-new-in-cf193-containers","text":"This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New and updated feature references are detailed here. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in CF193? Containers"},{"location":"what%27s-new/newcf193/#enable-presentation-of-locales-in-friendly-urls","text":"Beginning with the HCL Digital Experience 9.5 Container Update CF193 release, enablement is provided to present friendly URLs with locale specific definitions when multi-lingual page versions are requested. This enablement can also improve SEO results when users search for language-specific DX page topics. See the Enabling presentation of locales in friendly URLs topic for more information.","title":"Enable Presentation of Locales in Friendly URLs"},{"location":"what%27s-new/newcf193/#theme-editor-portlet","text":"The Theme Editor portlet is a new addition to HCL Digital Experience Container Update CF193 and higher release capabilities. The portlet allows an administrator to edit static theme resources in WebDAV without the use of a WebDAV client or tool. See the Theme Editor Portlet topic for more information.","title":"Theme Editor Portlet"},{"location":"what%27s-new/newcf193/#hcl-digital-experience-95-container-custom-context-root-url","text":"Beginning with HCL DX 9.5 Container Update CF193 release, you can define the custom context root URLs when deploying your DX 9.5 software to the supported container platforms. See the Customizing the Digital Experience Portal URL when deployed to Container platforms topic for more information.","title":"HCL Digital Experience 9.5 Container Custom Context Root URL"},{"location":"what%27s-new/newcf193/#new-digital-asset-management-features","text":"New Digital Asset Management Features are added with HCL Digital Experience Container Update CF193, and include the ability to filter Digital Assets by size. See the HCL Digital Asset Management Help Center topic for additional information.","title":"New Digital Asset Management Features"},{"location":"what%27s-new/newcf193/#change-language-presented-in-the-hcl-digital-experience-theme","text":"Beginning with HCL DX 9.5 Container Update CF193 release, you can switch the language presented in your Digital Experience theme. An example is provided in the Woodburn Studio demo site supporting presentations for French, Spanish, and English languages. See the How to switch the languages in the Digital Experience theme topic for more information.","title":"Change language presented in the HCL Digital Experience Theme"},{"location":"what%27s-new/newcf193/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. New release artifact types supporting Deploy DX Core, Manage Syndicator, and Manage Subscriber tasks are provided in Container Update CF193. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"what%27s-new/newcf193/#new-digital-experience-rest-apis","text":"New and updated HCL DX APIs are introduced with the HCL DX CF193 Container Update release: Web Content Manager Multilingual Solution APIs","title":"New Digital Experience REST APIs"},{"location":"what%27s-new/newcf193/#hcl-digital-experience-95-container-platform-support-matrix","text":"View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix Help Center topic for more information.","title":"HCL Digital Experience 9.5 Container Platform Support Matrix"},{"location":"what%27s-new/newcf193/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Deploy HCL Digital Experience in Minutes using HCL Solution Factory Using the dxctl tool to Update Digital Experience 9.5 on Red Hat OpenShift How to Upload HCL Digital Experience 9.5 CF Container Images to a Private Repository Parent topic: Container Update releases","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"what%27s-new/newcf194/","text":"What's new in CF194? Containers This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. No new features were added in this update. Important note: The default IBM WebSphere Application Server certificate that ships with HCL Digital Experience 9.5 Docker images expires on April 26, 2021. Access to HCL Digital Experience 9.5 container deployments is not adversely affected. However, scripts executed against the DX 9.5 deployed servers, like stopServer or some ConfigEngine tasks, will fail. To address this, HCL Digital Experience 9.5 customers deploying to container platforms can use either of the following options to update the certificate: Apply the HCL Digital Experience 9.5 Container Update CF194, available from the HCL Software Licensing Portal on April 19, 2021. Renew the certificate on your DX 9.5 Container Deployment by following the steps outlined in the following HCL DX Support Knowledge Base article: Manual Steps to Apply the Digital Experience 9.5 Container Deployment Core Certificate Update . Go to the HCL Software Support Site for the list of software fixes, including Container Update release CF194. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Parent topic: Container Update releases","title":"What's new in CF194? Containers"},{"location":"what%27s-new/newcf194/#whats-new-in-cf194-containers","text":"This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. No new features were added in this update. Important note: The default IBM WebSphere Application Server certificate that ships with HCL Digital Experience 9.5 Docker images expires on April 26, 2021. Access to HCL Digital Experience 9.5 container deployments is not adversely affected. However, scripts executed against the DX 9.5 deployed servers, like stopServer or some ConfigEngine tasks, will fail. To address this, HCL Digital Experience 9.5 customers deploying to container platforms can use either of the following options to update the certificate: Apply the HCL Digital Experience 9.5 Container Update CF194, available from the HCL Software Licensing Portal on April 19, 2021. Renew the certificate on your DX 9.5 Container Deployment by following the steps outlined in the following HCL DX Support Knowledge Base article: Manual Steps to Apply the Digital Experience 9.5 Container Deployment Core Certificate Update . Go to the HCL Software Support Site for the list of software fixes, including Container Update release CF194. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Parent topic: Container Update releases","title":"What's new in CF194? Containers"},{"location":"what%27s-new/newcf195/","text":"What's new in CF195? Containers This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New and updated feature references are detailed here. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Web Content Manager Multilingual Solution Library Export and Import The HCL Web Content Manager Multilingual Solution is a set of tools used to manage translated versions Web Content Manager content for localized and regionalized websites. Beginning with the HCL Digital Experience 9.5 Container Update CF195 release, a new export and import capability allows you to support translation of the content of a library by exporting it into a format supported by a translation service and importing the translated content back into the content library using the DXClient tool. See the How to export and import WCM library content using DXClient topic for more information. Web Content Manager Advanced Cache Options New options are available to flush the Web Content Manager Advanced cache, as a result of syndication operations, to help improve performance and reliability. See the Web Content Manager Cache Parameters topic for more information. Enhanced Cross Origin Resource Sharing Configuration Enhanced Cross Origin Resource Sharing Configuration adds new options for HCL Digital Experience administrators to set configuration for CORS using a WP configuration service in the IBM WebSphere Application Server resource environment provider. See the Enhanced Cross Origin Resource Sharing Configuration for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. New release artifact types supporting Undeploy Themes, and Export/Import Web Content Manager Library content are provided in Container Update CF195. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. Remote Search Configuration for HCL Digital Experience 9.5 deployments on Kubernetes platforms Beginning with HCL DX 9.5 Container Update CF195 release, Remote Search can be configured for deployment on supported Kubernetes platforms. See the Configure Remote Search in Red Hat OpenShift and Kubernetes topic for more information. Define No Context Root in for HCL Digital Experience 9.5 container deployments Beginning with HCL DX 9.5 Container Update CF195 release, administrators can define custom context root URLs, or no context root URL, when deploying HCL DX 9.5 software to the supported container platforms. See the Customizing the Digital Experience URL when deployed to Container platforms topic for more information. HCL Digital Experience 9.5 Container Platform Support Matrix View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix Help Center topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Getting Started with DXClient on Red Hat OpenShift using HCL Digital Experience CF194 Understanding the Core Persistent Volumes in HCL Digital Experience Container Update CF194 Create a WebDAV Theme copy using HCL Digital Experience 9.5 Create and apply A/B personalized scenarios with HCL Digital Experience Parent topic: Container Update releases","title":"What's new in CF195? Containers"},{"location":"what%27s-new/newcf195/#whats-new-in-cf195-containers","text":"This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New and updated feature references are detailed here. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in CF195? Containers"},{"location":"what%27s-new/newcf195/#web-content-manager-multilingual-solution-library-export-and-import","text":"The HCL Web Content Manager Multilingual Solution is a set of tools used to manage translated versions Web Content Manager content for localized and regionalized websites. Beginning with the HCL Digital Experience 9.5 Container Update CF195 release, a new export and import capability allows you to support translation of the content of a library by exporting it into a format supported by a translation service and importing the translated content back into the content library using the DXClient tool. See the How to export and import WCM library content using DXClient topic for more information.","title":"Web Content Manager Multilingual Solution Library Export and Import"},{"location":"what%27s-new/newcf195/#web-content-manager-advanced-cache-options","text":"New options are available to flush the Web Content Manager Advanced cache, as a result of syndication operations, to help improve performance and reliability. See the Web Content Manager Cache Parameters topic for more information.","title":"Web Content Manager Advanced Cache Options"},{"location":"what%27s-new/newcf195/#enhanced-cross-origin-resource-sharing-configuration","text":"Enhanced Cross Origin Resource Sharing Configuration adds new options for HCL Digital Experience administrators to set configuration for CORS using a WP configuration service in the IBM WebSphere Application Server resource environment provider. See the Enhanced Cross Origin Resource Sharing Configuration for more information.","title":"Enhanced Cross Origin Resource Sharing Configuration"},{"location":"what%27s-new/newcf195/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. New release artifact types supporting Undeploy Themes, and Export/Import Web Content Manager Library content are provided in Container Update CF195. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"what%27s-new/newcf195/#remote-search-configuration-for-hcl-digital-experience-95-deployments-on-kubernetes-platforms","text":"Beginning with HCL DX 9.5 Container Update CF195 release, Remote Search can be configured for deployment on supported Kubernetes platforms. See the Configure Remote Search in Red Hat OpenShift and Kubernetes topic for more information.","title":"Remote Search Configuration for HCL Digital Experience 9.5 deployments on Kubernetes platforms"},{"location":"what%27s-new/newcf195/#define-no-context-root-in-for-hcl-digital-experience-95-container-deployments","text":"Beginning with HCL DX 9.5 Container Update CF195 release, administrators can define custom context root URLs, or no context root URL, when deploying HCL DX 9.5 software to the supported container platforms. See the Customizing the Digital Experience URL when deployed to Container platforms topic for more information.","title":"Define No Context Root in for HCL Digital Experience 9.5 container deployments"},{"location":"what%27s-new/newcf195/#hcl-digital-experience-95-container-platform-support-matrix","text":"View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix Help Center topic for more information.","title":"HCL Digital Experience 9.5 Container Platform Support Matrix"},{"location":"what%27s-new/newcf195/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Getting Started with DXClient on Red Hat OpenShift using HCL Digital Experience CF194 Understanding the Core Persistent Volumes in HCL Digital Experience Container Update CF194 Create a WebDAV Theme copy using HCL Digital Experience 9.5 Create and apply A/B personalized scenarios with HCL Digital Experience Parent topic: Container Update releases","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"what%27s-new/newcf196/","text":"What's new in CF196? Containers This HCL Digital Experience 9.5 Container Update release includes new releases of HCL Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images, and a beta preview release of Design Studio. New and updated feature references are detailed here. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Note: For new capabilities that are available for HCL DX on-premise deployments, see What's new in the CF196 topic. Design Studio (Beta) Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. Note: Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF196. It is not yet supported for use in production deployments . See the Design Studio (Beta) topic for more information. Deploy HCL DX CF196 to container platforms using Helm Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators a larger degree of transparency and control in deployment operations, and is available for use with the Google Kubernetes Engine (GKE) platform with Container Update CF196. See the HCL DX 9.5 Helm deployment topic for more information. Deploy HCL DX 9.5 using Docker Compose Beginning with HCL DX 9.5 Container Update CF196, administrators and developers can deploy HCL DX 9.5 using Docker Compose, for non-production use. Docker Compose scripts for HCL DX 9.5, installation, and configuration instructions for non-production use are available in the HCL Software Github page. See the Docker image deployment using Docker Compose topic for more information. Web Content Manager Multilingual Solution Enhancements The HCL Web Content Manager Multilingual Solution is a set of tools used to manage translated versions Web Content Manager content for localized and regionalized websites. Beginning with the HCL Digital Experience 9.5 Container Update CF196 release, support is added to import and export multiple libraries to a format supported by a translation service, support a maximum field length, export changed contents from a library, and export to projects. A new option to switch the language in an HCL DX 9.5 mobile view is also available. See the How to export and import WCM library content using DXClient and The Woodburn Studio demo site topics for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. Updates include a new DXClient Docker image, and new release artifact types supporting shared libraries, obtain failed syndication reports are provided in Container Update CF196. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. HCL Digital Experience 9.5 Container Platform Support Matrix View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix Help Center topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Configuring user access permissions to Digital Asset Management assets Deep dive: Progressive Web Applications with HCL DX 9.5 Content Security Policy with HCL DX 9.5 Parent topic: Container Update releases","title":"What's new in CF196? Containers"},{"location":"what%27s-new/newcf196/#whats-new-in-cf196-containers","text":"This HCL Digital Experience 9.5 Container Update release includes new releases of HCL Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images, and a beta preview release of Design Studio. New and updated feature references are detailed here. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Note: For new capabilities that are available for HCL DX on-premise deployments, see What's new in the CF196 topic.","title":"What's new in CF196? Containers"},{"location":"what%27s-new/newcf196/#design-studio-beta","text":"Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. Note: Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF196. It is not yet supported for use in production deployments . See the Design Studio (Beta) topic for more information.","title":"Design Studio (Beta)"},{"location":"what%27s-new/newcf196/#deploy-hcl-dx-cf196-to-container-platforms-using-helm","text":"Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators a larger degree of transparency and control in deployment operations, and is available for use with the Google Kubernetes Engine (GKE) platform with Container Update CF196. See the HCL DX 9.5 Helm deployment topic for more information.","title":"Deploy HCL DX CF196 to container platforms using Helm"},{"location":"what%27s-new/newcf196/#deploy-hcl-dx-95-using-docker-compose","text":"Beginning with HCL DX 9.5 Container Update CF196, administrators and developers can deploy HCL DX 9.5 using Docker Compose, for non-production use. Docker Compose scripts for HCL DX 9.5, installation, and configuration instructions for non-production use are available in the HCL Software Github page. See the Docker image deployment using Docker Compose topic for more information.","title":"Deploy HCL DX 9.5 using Docker Compose"},{"location":"what%27s-new/newcf196/#web-content-manager-multilingual-solution-enhancements","text":"The HCL Web Content Manager Multilingual Solution is a set of tools used to manage translated versions Web Content Manager content for localized and regionalized websites. Beginning with the HCL Digital Experience 9.5 Container Update CF196 release, support is added to import and export multiple libraries to a format supported by a translation service, support a maximum field length, export changed contents from a library, and export to projects. A new option to switch the language in an HCL DX 9.5 mobile view is also available. See the How to export and import WCM library content using DXClient and The Woodburn Studio demo site topics for more information.","title":"Web Content Manager Multilingual Solution Enhancements"},{"location":"what%27s-new/newcf196/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. Updates include a new DXClient Docker image, and new release artifact types supporting shared libraries, obtain failed syndication reports are provided in Container Update CF196. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"what%27s-new/newcf196/#hcl-digital-experience-95-container-platform-support-matrix","text":"View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix Help Center topic for more information.","title":"HCL Digital Experience 9.5 Container Platform Support Matrix"},{"location":"what%27s-new/newcf196/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Configuring user access permissions to Digital Asset Management assets Deep dive: Progressive Web Applications with HCL DX 9.5 Content Security Policy with HCL DX 9.5 Parent topic: Container Update releases","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"what%27s-new/newcf197/","text":"What's new in CF197? Containers This HCL Digital Experience 9.5 Container Update and CF197 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management and Experience API images, and an updated beta preview release of Design Studio. New and updated feature references are detailed here. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Design Studio (Beta) Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New services available with the Container Update CF197 release include ability to render DX site pages and updates using the sample site, Ability to use the page editor to edit elements inline and update metadata, set locations for sites, set html tags for text elements, and more. Note: Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF197, and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) topic for more information. Deploy HCL DX CF197 to container platforms using Helm Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators a larger degree of transparency and control in deployment operations. Support for new HCL DX 9.5 CF197 deployments to Red Hat OpenShift, Microsoft Azure Elastic Kubernetes Service (AKS) and Amazon Elastic Kubernetes Service (EKS),and ability to update from HCL DX 9.5 version CF196 to CF197 is supported with the Google Kubernetes Engine (GKE) platform. See the HCL DX 9.5 Helm deployment topic for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. Updates include new release artifact types supporting, Obtain failed Syndication reports for single or multiple items, and delete Digital Asset Management inactive schema from Persistence are provided in CF197. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. HCL Digital Experience 9.5 Container Platform Support Matrix View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Updating the HCL DX 9.5 Portal & IBM WebSphere Application Server Administrator Secrets in OpenShift and Kubernetes Use Docker Compose to install HCL DX 9.5 with Cloud Components for Developer Use Parent topic: Container Update releases","title":"What's new in CF197? Containers"},{"location":"what%27s-new/newcf197/#whats-new-in-cf197-containers","text":"This HCL Digital Experience 9.5 Container Update and CF197 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management and Experience API images, and an updated beta preview release of Design Studio. New and updated feature references are detailed here. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in CF197? Containers"},{"location":"what%27s-new/newcf197/#design-studio-beta","text":"Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New services available with the Container Update CF197 release include ability to render DX site pages and updates using the sample site, Ability to use the page editor to edit elements inline and update metadata, set locations for sites, set html tags for text elements, and more. Note: Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF197, and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) topic for more information.","title":"Design Studio (Beta)"},{"location":"what%27s-new/newcf197/#deploy-hcl-dx-cf197-to-container-platforms-using-helm","text":"Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators a larger degree of transparency and control in deployment operations. Support for new HCL DX 9.5 CF197 deployments to Red Hat OpenShift, Microsoft Azure Elastic Kubernetes Service (AKS) and Amazon Elastic Kubernetes Service (EKS),and ability to update from HCL DX 9.5 version CF196 to CF197 is supported with the Google Kubernetes Engine (GKE) platform. See the HCL DX 9.5 Helm deployment topic for more information.","title":"Deploy HCL DX CF197 to container platforms using Helm"},{"location":"what%27s-new/newcf197/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. Updates include new release artifact types supporting, Obtain failed Syndication reports for single or multiple items, and delete Digital Asset Management inactive schema from Persistence are provided in CF197. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"what%27s-new/newcf197/#hcl-digital-experience-95-container-platform-support-matrix","text":"View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix topic for more information.","title":"HCL Digital Experience 9.5 Container Platform Support Matrix"},{"location":"what%27s-new/newcf197/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Updating the HCL DX 9.5 Portal & IBM WebSphere Application Server Administrator Secrets in OpenShift and Kubernetes Use Docker Compose to install HCL DX 9.5 with Cloud Components for Developer Use Parent topic: Container Update releases","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"what%27s-new/newcf198/","text":"What's new in CF198? Containers This HCL Digital Experience 9.5 Container Update and CF198 release includes new releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, the new Site Manager Custom Layout Editor, HCL Digital Experience Technical Articles, \u2018How To\u2019 videos, and more. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Design Studio (Beta) Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New features available with the Container Update CF198 release include the ability to create new DX sites, reading and updating site metadata, accessing site and page UUID and URLs, and client-side logging services. Note: Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF198, and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) topic for more information. Deploy HCL DX CF198 to container platforms using Helm Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators more transparency and control in deployment operations. Support for hybrid deployments is provided, enabling to update from HCL DX 9.5 CF197 to CF198 in the Red Hat OpenShift, Microsoft Azure Elastic Kubernetes Service (AKS) and Amazon Elastic Kubernetes Service (EKS) platforms. See the HCL DX 9.5 Helm deployment topic for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. Updates delivered in CF198 include process definitions to automate select IBM WebSphere Application Server settings during deployment, generate import and export lists of virtual portals, and import and export personalization rules. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. New HCL Digital Experience Site Manager Custom Layout Editor Beginning with HCL Digital Experience CF198, a Custom Layout Editor is available for use with HCL DX Site Manager capabilities, reducing custom development requirements for site designers implementing custom layouts in DX site pages. See the Using the Digital Experience Custom Layout Editor topic for more information. New Experience APIs New HCL Experience Web Content Manager REST APIs are available for new menu component update, collection responses conversion utilities, the ability to use the page editor to edit elements inline and update metadata, set locations for sites, set HTML tags for text elements, and more. See the HCL DX Experience API topic for more information Rationalized CF release versioning Beginning with HCL Digital Experience CF196, single versioning is used for both container and on-premise Combined Cumulative Fixes (CFs). This means that fixes for both deployments are included into one CF deliverable. And although the versioning is the same, CFs are packaged separately for on-premises and container deployments. See the CF release versioning and update path topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in articles and videos from the following HCL Digital Experience Help Center topics: Video: Creating an HCL Digital Experience 9.5 CF196 cluster Step-by-step guide: How to deploy HCL DX Container update CF197 and later to Microsoft Azure Kubernetes Service HCL Digital Experience 9.5 Container Platform Support Matrix View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix topic for more information. Parent topic: Container Update releases","title":"What's new in CF198? Containers"},{"location":"what%27s-new/newcf198/#whats-new-in-cf198-containers","text":"This HCL Digital Experience 9.5 Container Update and CF198 release includes new releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, the new Site Manager Custom Layout Editor, HCL Digital Experience Technical Articles, \u2018How To\u2019 videos, and more. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in CF198? Containers"},{"location":"what%27s-new/newcf198/#design-studio-beta","text":"Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New features available with the Container Update CF198 release include the ability to create new DX sites, reading and updating site metadata, accessing site and page UUID and URLs, and client-side logging services. Note: Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF198, and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) topic for more information.","title":"Design Studio (Beta)"},{"location":"what%27s-new/newcf198/#deploy-hcl-dx-cf198-to-container-platforms-using-helm","text":"Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators more transparency and control in deployment operations. Support for hybrid deployments is provided, enabling to update from HCL DX 9.5 CF197 to CF198 in the Red Hat OpenShift, Microsoft Azure Elastic Kubernetes Service (AKS) and Amazon Elastic Kubernetes Service (EKS) platforms. See the HCL DX 9.5 Helm deployment topic for more information.","title":"Deploy HCL DX CF198 to container platforms using Helm"},{"location":"what%27s-new/newcf198/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. Updates delivered in CF198 include process definitions to automate select IBM WebSphere Application Server settings during deployment, generate import and export lists of virtual portals, and import and export personalization rules. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"what%27s-new/newcf198/#new-hcl-digital-experience-site-manager-custom-layout-editor","text":"Beginning with HCL Digital Experience CF198, a Custom Layout Editor is available for use with HCL DX Site Manager capabilities, reducing custom development requirements for site designers implementing custom layouts in DX site pages. See the Using the Digital Experience Custom Layout Editor topic for more information.","title":"New HCL Digital Experience Site Manager Custom Layout Editor"},{"location":"what%27s-new/newcf198/#new-experience-apis","text":"New HCL Experience Web Content Manager REST APIs are available for new menu component update, collection responses conversion utilities, the ability to use the page editor to edit elements inline and update metadata, set locations for sites, set HTML tags for text elements, and more. See the HCL DX Experience API topic for more information","title":"New Experience APIs"},{"location":"what%27s-new/newcf198/#rationalized-cf-release-versioning","text":"Beginning with HCL Digital Experience CF196, single versioning is used for both container and on-premise Combined Cumulative Fixes (CFs). This means that fixes for both deployments are included into one CF deliverable. And although the versioning is the same, CFs are packaged separately for on-premises and container deployments. See the CF release versioning and update path topic for more information.","title":"Rationalized CF release versioning"},{"location":"what%27s-new/newcf198/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in articles and videos from the following HCL Digital Experience Help Center topics: Video: Creating an HCL Digital Experience 9.5 CF196 cluster Step-by-step guide: How to deploy HCL DX Container update CF197 and later to Microsoft Azure Kubernetes Service","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"what%27s-new/newcf198/#hcl-digital-experience-95-container-platform-support-matrix","text":"View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix topic for more information. Parent topic: Container Update releases","title":"HCL Digital Experience 9.5 Container Platform Support Matrix"},{"location":"what%27s-new/newcf199/","text":"What's new in CF199? Containers This HCL Digital Experience 9.5 Container Update and CF199 release includes new releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, \u201cHow To\u201d videos, and more. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Design Studio (Beta) Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New features available with the Container Update CF199 release include the ability to select Web Content Manager library assets when creating sites, UI globalization, support for alternate and no context root when defining sites, and more. Note: Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF196, and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) topic for more information. Migrate from HCL DX 9.5 Operator to Helm Deployments Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators more transparency and control in deployment operations. Beginning with Container Update CF199, support for migration from Operator-based (dxctl) to Helm-based deployments is provided. See the HCL DX 9.5 Helm deployment topic for more information. Digital Asset Management Staging New Digital Asset Management (DAM) staging support enables administrators to stage and synchronize DAM assets from an authoring environment (source environment/publisher) to multiple rendering environments (target environment/subscriber), using DXClient. See the DAM staging topic for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include process definitions to automate Export and Import of select IBM WebSphere Application Server Resource Provider settings during deployment, and to create Syndication relationships and credential vault settings. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. End of Support for HCL Digital Experience Deprecated Features The following list of HCL Digital Experience deprecated features will reach end of support beginning with Container update and CF200 release. Refer to the Deprecated features and themes for HCL Digital Experience 9.5 topic in the Help Center. New Experience APIs New HCL Experience APIs are available for creating, updating and deleting Design Studio (Beta) sites, pages, and containers. New Web Content Manager REST V2 APIs are available for creating content templates, categories, and more. See the HCL Experience API topic for more information. New REST APIs to Configure Remote Search Beginning with HCL Digital Experience CF and Container Update CF199, additional REST services enable administrators and developers to programatically configure remote search in on-premises and container-based Digital Experience deployments. See the Configure Remote Search using REST APIs topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in articles and videos from the following HCL Digital Experience Help Center topics: Video: Deploy HCL DX 9.5 Container Update using Helm Video: Experience API V2 Web Content Manager REST APIs Parent topic: Container Update releases","title":"What's new in CF199? Containers"},{"location":"what%27s-new/newcf199/#whats-new-in-cf199-containers","text":"This HCL Digital Experience 9.5 Container Update and CF199 release includes new releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, \u201cHow To\u201d videos, and more. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in CF199? Containers"},{"location":"what%27s-new/newcf199/#design-studio-beta","text":"Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New features available with the Container Update CF199 release include the ability to select Web Content Manager library assets when creating sites, UI globalization, support for alternate and no context root when defining sites, and more. Note: Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF196, and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) topic for more information.","title":"Design Studio (Beta)"},{"location":"what%27s-new/newcf199/#migrate-from-hcl-dx-95-operator-to-helm-deployments","text":"Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators more transparency and control in deployment operations. Beginning with Container Update CF199, support for migration from Operator-based (dxctl) to Helm-based deployments is provided. See the HCL DX 9.5 Helm deployment topic for more information.","title":"Migrate from HCL DX 9.5 Operator to Helm Deployments"},{"location":"what%27s-new/newcf199/#digital-asset-management-staging","text":"New Digital Asset Management (DAM) staging support enables administrators to stage and synchronize DAM assets from an authoring environment (source environment/publisher) to multiple rendering environments (target environment/subscriber), using DXClient. See the DAM staging topic for more information.","title":"Digital Asset Management Staging"},{"location":"what%27s-new/newcf199/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include process definitions to automate Export and Import of select IBM WebSphere Application Server Resource Provider settings during deployment, and to create Syndication relationships and credential vault settings. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"what%27s-new/newcf199/#end-of-support-for-hcl-digital-experience-deprecated-features","text":"The following list of HCL Digital Experience deprecated features will reach end of support beginning with Container update and CF200 release. Refer to the Deprecated features and themes for HCL Digital Experience 9.5 topic in the Help Center.","title":"End of Support for HCL Digital Experience Deprecated Features"},{"location":"what%27s-new/newcf199/#new-experience-apis","text":"New HCL Experience APIs are available for creating, updating and deleting Design Studio (Beta) sites, pages, and containers. New Web Content Manager REST V2 APIs are available for creating content templates, categories, and more. See the HCL Experience API topic for more information.","title":"New Experience APIs"},{"location":"what%27s-new/newcf199/#new-rest-apis-to-configure-remote-search","text":"Beginning with HCL Digital Experience CF and Container Update CF199, additional REST services enable administrators and developers to programatically configure remote search in on-premises and container-based Digital Experience deployments. See the Configure Remote Search using REST APIs topic for more information.","title":"New REST APIs to Configure Remote Search"},{"location":"what%27s-new/newcf199/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in articles and videos from the following HCL Digital Experience Help Center topics: Video: Deploy HCL DX 9.5 Container Update using Helm Video: Experience API V2 Web Content Manager REST APIs Parent topic: Container Update releases","title":"New HCL Digital Experience \u2018How To\u2019 Videos"}]}