{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"containerization/azure_aks/","text":"Deploy DX Container to Microsoft Azure Kubernetes Service (AKS) Learn how to deploy HCL Digital Experience (DX) 9.5 CF182 and later container release along with Ambassador to Kubernetes, as verified in Microsoft Azure Kubernetes Service (AKS). About this task Follow these steps to deploy HCL Digital Experience 9.5 CF182 and later container release along with Ambassador to Kubernetes, as verified in Microsoft Azure Kubernetes Service (AKS) . This deployment relies heavily on Kubernetes Operators for full functionality. If deploying HCL DX 9.5 Container Update CF191 and earlier, view the instructions to deploy using script commands instead of the dxctl tool as described below in this Help Center section. Note: Reference the latest HCL DX 9.5 Container Release and Update file list in the Docker deployment topic. Prerequisites Prior to using the procedure below, it is assumed that the HCL DX Administrator is generally experienced in using Kubernetes. Additionally, the DX Administrator must have the appropriate access to the target environment. If not, following are some preliminary steps that must be taken. Setup KUBECONFIG to refer to the target server. This ensures any kubectl commands executed locally affect the target environment. Example: Use kubectl get {pods, pv, storageclass} to get appropriate information from the artifacts running in the target Kubernetes environment. The following tools must be installed on a machine other than the Portal server: Docker Microsoft Azure CLI If deploying Digital Experience Container Update CF192 and later, the dxctl tool is used to install and configure the deployment Volume requirement: It requires an AccessMode of ReadWriteMany . It requires a minimum of 40 GB , with the default request set to 100 GB . RECLAIM POLICY = Retain Note: HCL Digital Experience is input-output (I/O) intensive and requires a high performing file system for optimization. Azure container registry (For tagging and pushing). Deploying HCL Digital Experience (DX) 9.5 CF192 and later version Follow these steps to deploy the HCL Digital Experience (DX) 9.5 CF192 and later container release to the Microsoft Azure AKS platform: Download the HCL Digital Experience Container Update CF192 and later release container product and extract it to your local file system. The file system can be on a local workstation or cloud platform. If deploying HCL DX 9.5 Container Update CF192 release, the image and package names are as follows: CF192-core.zip files: ``` HCL DX notices V9.5 CF192.txt - ``` dxclient_v1.2.0_20210305-1758.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF192_20210305-2309.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF192_20210305-2309.zip - ``` hcl-dx-content-composer-image-v1.6.0_20210305-1756.tar.gz - ``` hcl-dx-core-image-v95_CF192_20210305-1758.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF192_20210305-1757.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.6.0_20210305-1802.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210305-1805.zip - ``` hcl-dx-image-processor-image-v1.6.0_20210305-1758.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20210305_1614986151.tar.gz - ``` hcl-dx-postgres-image-v1.6.0_20210305-1800.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF192_20210305-1758.tar.gz - ``` hcl-dx-ringapi-image-v1.6.0_20210305-1802.tar.gz Log in to your Microsoft Azure AKS platform. For more information, refer to the Microsoft Azure documentation if the Microsoft Azure CLI needs to be installed. Example log in command: az login Example: Create a resource group in Microsoft Azure using the following command: az group create --name <resourceGroupName> --location <region> Example: Azure Console Example: For more information, refer to the Microsoft Azure documentation on Resource . Create a Microsoft Azure Container Registry (ACR) to push the HCL DX 9.5 CF192 and later container images to. Azure Console Example: Once the ACR gets created, log in using the following command: az acr login --name <containerRegistry> Example: For more information, refer to the Microsoft Azure documentation on Container Registry . Set up the NFS server. Provide the HCL DX 9.5 CF192 and later Docker image access to the volume mount created in order to copy the profile. There are various ways to do this, and NFS is one option. If NFS is used, here are the parameters that have been tested to work: rw Default. sync Default after NFS 1.0, means that the server does not reply until after the commit. insecure Requires requests originate on ports less than 1024. ** root_squash Map requests to the nobody user.** Hard Required because this means the system keeps trying to write until it works.** nfsvers=4.1 rsize=8388608 Avoids dropped packages, default 8192. wsize=8388608 Avoids dropped packages, default 8192 timeo=600 60 seconds. retrans=2 Number of retries after a time out. noresvport ** Tells the NFS client to use a new Transmission Control Protocol (TCP) source port when a network connection is reestablished. Doing this helps make sure that the EFS file system has uninterrupted availability after a network recovery event. Note: Those marked with (**) are critical and, in many cases, it is recommended to have the rsize and wsize set to 8388608. For more information, refer to the Microsoft Azure documentation on Storage . Configure the Microsoft Azure Kubernetes cluster. To configure kubectl to connect to your Kubernetes cluster, use the az aks get-credentials command. Example: az aks get-credentials --resource-group <resourcegroup> --name <clusterName> For more information, refer to the Microsoft Azure documentation on Cluster . DX-Container Image Management Change directory. Open a terminal window and change to the root directory of the extracted package. Docker load, tag and push by using the following commands: List Docker images docker images Docker load Load the containers into your Docker repository: docker load -i hcl-dx-core-image-v95_CF192_20210225-035822.tar.gz docker load -i hcl-dx-ambassador-image-154.tar.gz docker load -i hcl-dx-cloud-operator-image-v95_CF192_20210225-0546.tar.gz docker load -i hcl-dx-redis-image-5.0.1.tar.gz ACR details To tag and push the images to ACR, obtain login server details: az acr list --resource-group <resourceGroup> --query \"[].{acrLoginServer:loginServer}\" --output table Docker tag Tag your images using the tag command as shown in the examples below: docker tag SOURCE_IMAGE[:TAG] TARGET_IMAGE[:TAG] Example: docker tag hcl/dx/core:v95_CF192_20210225-035822 YOUR_CONTAINER.azurecr.io/hcl/dx/core:v95_CF192_20210225-035822 docker tag hcl/dx/cloud-operator:v95_CF192_20210225-0546-YOUR_CONTAINER.azurecr.io/hcl/dx/cloud-operator:v95_CF192_20210225-0546 docker tag hcl/dx/ambassador:YOUR_CONTAINER.azurecr.io/hcl/dx/ambassador:154 docker tag hcl/dx/redis:5.0.1 YOUR_CONTAINER.azurecr.io/hcl/dx/redis:5.0.1 Docker push Push the images to ACR using the following push command: docker push [OPTIONS] NAME[:TAG] Example commands: docker push YOUR_CONTAINER.azurecr.io/hcl/dx/core:v95_CF192_20210225-035822 docker push YOUR_CONTAINER.azurecr.io/hcl/dx/cloud-operator:v95_CF192_20210225-0546 docker push YOUR_CONTAINER.azurecr.io/hcl/dx/ambassador:154 docker push YOUR_CONTAINER.azurecr.io/hcl/dx/redis:5.0.1 Once the images are pushed, they can be listed using the commands below, or through use of the Microsoft Azure Kubernetes platform console. Command Example: az acr repository list --name <acrName> --output table Microsoft Azure AKS Console - DX 9.5 example: DX-Deployment using dxctl Create a StorageClass . Sample StorageClass YAML: ``` kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: dx-deploy-stg provisioner: example.com/nfs ``` Create a Persistence Volume (pv) with AccessMode as ReadWriteMany and reclaim policy as Retain. Sample PV YAML: apiVersion: v1 kind: PersistentVolume metadata: name: blrcaps-core-3 spec: capacity: storage: 100Gi accessModes: - ReadWriteMany nfs: path: NFS_PATH server: NFS_SERVER persistentVolumeReclaimPolicy: Retain storageClassName: dx-deploy-stg mountOptions: - hard - nfsvers=4.1 - rsize=10485760 - wsize=10485760 - timeo=600 - retrans=2 - noresvport Note: Make sure the PV is available. If it is not, remove claimRef: from the YAML file. Log in to the cluster. Before using the dxctl tool to deploy, you must be logged in to the targeted cluster using the cloud platform specific CLI (Red Hat OpenShift, Amazon EKS, Microsoft Azure AKS, Google GKE). Example: az login Download dxctl . Instructions for downloading the latest packages are available here . Once downloaded and extracted, the hcl-dx-cloud-scripts directory structure is as follows: For more information about dxctl , visit the following documentation here . Configure and deploy using the HCL DX dcxtl tool. To start, change to the extracted files directory using the following command: cd hcl-dx-cloud-scripts Using DX Container Update CF192 and later, the directory structure appears as follows: Configure the dxctl properties for the DX 9.5 Container CF192 and later deployment. Copy one of the provided properties files to further modify for your deployment. The modified properties file can be used for the deployment and the same file must be used for further updates. Example: mkdir -p /home/$USER/deployments/ cp dxctl/properties/full-deployment.properties /home/$USER/deployments/myfirst_deployment.properties Then, update the dxctl properties file values. Sample values: dx.namespace: endgametest-jeet1 dx.image: dxen dx.tag: v95_CF192_20210225-035822_rohan_release_95_CF192_60374773 dx.storageclass:dx-deploy-stg dx.volume: jeet3 dx.volume.size:100 remote.search.enabled:false openldap.enabled:false api.enabled: false composer.enabled: false dam.enabled: false ingress.image:dx-build-output/common/ambassador ingress.tag:1.5.4 ingress.redis.image:redis ingress.redis.tag:5.0.1 dx.operator.image: dx-build-output/hcldx-cloud-operator/hcldx-cloud-operator dx.operator.tag: v95_CF192_20210225-0546_xxxxxxxxx_95_CF192 Important: With HCL DX 9.5 Container Update CF197 and later, dam.features in full-deployment properties is added for use in a future container update release, and should not be modified except with direct guidance from HCL Support. Note: With HCL DX 9.5 Container Update CF193 and later, persist.force.read in full-deployment properties is added to enable a read-only Postgres pod for Digital Asset Management. This enables a failover capability for the Postgres service supporting DAM. Another option to enable a read-only pod is to set the persist.minreplicas: option set to greater than 1. Example: Deploy using dxctl . Run the following command to deploy HCL DX 9.5 Container Update CF192 and later to Microsoft Azure AKS: ./mac/dxctl --deploy -p /home/$USER/deployments/myfirst_deployment.properties Example: Note: This set of steps result in a deployment being created. Validate the deployment. Make sure all the pods are \"Running\" and in \"Ready\" state on your Microsoft Azure AKS platform, as shown in the example below: Generate TLS Certificate Create a TLS certification to be used by the deployment. Prior to this step, create a self-signed certificate to enable HTTPS using the following command: openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -subj '/CN=ambassador-cert' -nodes Then, store the Certificate and Key in a Kubernetes Secret using the following command: kubectl create secret tls dx-tls-cert --cert=cert.pem --key=key.pem -n <YourNamespace> Afterward, access the HCL DX 9.5 CF192 and later container deployment. To do so, obtain the external IP from the container platform Load balancer to access the HCL DX 9.5 deployment, as shown in the example below: $ kubectl get all -n NAMESPACE Then run the next command: https://EXTERNAL_IP/wps/portal Note: It is required to ensure the Microsoft Azure AKS load balancer configured permits external access. Consult the Microsoft Azure documentation for Load Balancer setup and default configuration details . Update the HCL Digital Experience (DX) 9.5 Azure AKS deployment to later HCL DX 9.5 Container Update releases To update the deployment to later HCL DX 9.5 Container Update releases, follow these steps: Update the deployment properties file with new image values, and run the Update command. Note: If the properties file is not available, then execute the following command to generate one. ./win/dxctl --getproperties --dx.namespace <Your Namespace> Example: On Mac: ./mac/dxctl --update -p properties/myfirst_deployment.properties Example: On Windows: .\\win\\dxctl.exe --update -p properties\\myfirst_deployment.properties On Linux: ./linux/dxctl -\u2013update -p properties/myfirst_deployment.properties Additional considerations: For example, once the database is transferred, the DBTYPE must be updated so you can scale the instances higher. Additionally, once the database is transferred, the number of replicas could be increased. There are additional options to customize the deployment . Delete the HCL Digital Experience (DX) 9.5 CF192 and later release Azure AKS deployment To delete the deployment, follow one of two methods: Method 1: Remove the deployment but allow for redeployment with the same volumes using the following command: ./linux/dxctl --destroy -p properties/myfirst_deployment.properties Method 2: Remove the entire namespace/project using the following command: ./linux/dxctl --destroy -p properties/myfirst_deployment.properties -all true Example: If some resources like services are still not deleted, run the following command: kubectl patch services $(kubectl get services -n $NAMESPACE | grep -v \"NAME\" |awk '{print $1}') -p '{\"metadata\":{\"finalizers\":null}}' -n $NAMESPACE Deploying HCL Digital Experience (DX) 9.5 CF191 and earlier version Follow these steps to deploy the HCL Digital Experience (DX) 9.5 CF191 and earlier container version to the Microsoft Azure AKS platform: Download and extract the contents of the HCL DX 9.5 CF182 package to the local file system. In Microsoft Azure Kubernetes Service (AKS), load, tag, and push the HCL Digital Experience images into your MS Azure Container Registries. Note: In Microsoft Azure, when using AKS a single Container Registry, or multiple Container Registries may be used. See the Microsoft Azure Container Registry documentation for additional information about this topic. In this example, 10 Container Registries are created: As an alternative, DX Administrators can use a single or fewer registries and create 'Repositories' within. In this example, a Container Registry named azambassador with a repository 'ambassador' is shown: Administrators can tag and push another image into this Container Registry to get a second repository. In the following example, the Ambassador Redis image is added: The HCL DX 9.5 Container deployment does not assume 1, or many registries are defined, and either definition setup works. In the following example, the HCL DX 9.5 Redis 5.0.1 image is added to the azambassador Container Registry. This example shows loading the HCL DX 9.5 CF181 and earlier container into a local repository, tagging it and pushing it to the azuredxen Container Registry in the dxen \u2018Repository\u2019. Install the HCL Digital Experience (DX) 9.5 CF182 and later core images Load the HCL DX 9.5 CF182 and later images to your deployment. The following example uses the CF183 version in the load command: Docker load -I hcl-dx-core-image-v95_CF183_20200819-1159.tar.gz Docker tag and docker push to the Azure environment: Once complete, the image is viewable in the Microsoft Azure repository: Reminder : Consult the HCL Digital Experience 9.5 Deployment \u2013 Docker topic for the latest list of HCL DX 9.5 container files that are available. HCL DX 9.5 Container Update CF183 files are used in these examples: CF183-core.zip files HCL DX notices V9.5 CF183.txt hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF183_20200818-1852.tar.gz hcl-dx-cloud-scripts-v95_CF183_20200818-1852.zip hcl-dx-core-image-v95_CF183_20200818-1342.tar.gz hcl-dx-redis-image-5.0.1.tar.gz CF183-other.zip files HCL DX notices V9.5 CF183.txt hcl-dx-content-composer-image-v1.2.0_20200818-1343.tar.gz hcl-dx-digital-asset-management-operator-image-v95_CF183_20200818-1344.tar.gz hcl-dx-digital-asset-manager-image-v1.2.0_20200818-1346.tar.gz hcl-dx-image-processor-image-v1.2.0_20200818-1345.tar.gz hcl-dx-openldap-image-v1.0.0-master_20200818_1597758965.tar.gz hcl-dx-postgres-image-v1.2.0_20200818-1349.tar.gz hcl-dx-remote-search-image-v95_CF183_20200818-1342.tar.gz hcl-dx-ringapi-image-v1.2.0_20200818-1351.tar.gz To install HCL Digital Experience 9.5 core software to Microsoft Azure AKS, the following images are required: hcl-dx-cloud-operator-image-v95 hcl-dx-core-image-v95 hcl-dx-ambassador-image hcl-dx-redis-image Images included in the \u2018other\u2019 package are optional and used to support use of OpenLDAP, Remote Search, the Experience API, Content Composer, and Digital Asset Management components and services. See examples that show how to load HCL DX 9.5 images to MS Azure below. In the following example, the items are loaded into the azuredxen Content Registry and multiple repositories are created. Images are tagged with dx-183 reflecting the HCL DX 9.5 Container Update CF183 version images used in this deployment. At this stage, the ./deploy/operator.yaml needs to be properly updated and the operator, and Redis image details need to be provided: First, replace the line: From: \u2018image: REPOSITORY_NAME /hcldx-cloud-operator:9.5.next\u2019 To: Add the proper value for the deployment, as in the following example: \u2018image: azuredxen.azurecr.io/hcldx-cloud-operator:v95_CF183_20200819-1711\u2019 Next, replace the values: \"REDIS_REPO\",\"REDIS_IMG_ENV\",\u201cREDIS_TAG_ENV\u201d with proper values. See the following example: Reviewing the Azure dashboard, administrators can see the following for redis: Deploy the Custom Resource Definition using the scripts/deployCrd.sh file. See the following example: Important : Ensure there is an available persistent volume for the deployment or a self-provisioning storage class. The HCL DX Help Center topic ( Sample Storage Class and Volume for HCL Digital Experience 9.5 Container Deployments ) can be referenced for related guidance. In this example, a storage class named dx-deploy-stg and a volume dxdeployhave been created: Run the deployment scripts as follows: ./scripts/deployDx.sh az-demo 1 azuredxen.azurecr.io dxen v95_CF183_20200819-1159 dxeploy dx-deploy-stg derby ambassador 154 NAMESPACE - the project or the namespace to create or use for deployment. REPLICAS - the number of initial instances for the deployment. REPOSITORY - your local repository, the repository used by Kubernetes. IMAGENAME - the name of the dxCore image, as added to the repository above. IMAGETAG - the tag for the target image as added to the repository above. VOLUMENAME - the volume to be used by the deployment for persistence, this must use AccessMode ReadWriteMany . STORAGECLASS - the storage class name used to create the persistent volume. DBTYPE - the database type. By default, and initially, this is Derby. HCL DX 9.5 uses Apache Derby, Oracle Database, IBM DB2, or Microsoft SQL Server. Acceptable values are derby , oracle , db2 , or msSql . INGRESSIMAGE - The image name to use for Ambassador. INGRESSTAG - The image tag to use for Ambassador. The command output shows the values as they align with the deployment, and the result of each step. DX Administrators can use \u2018kubectl get pods -n az-emo\u2019 to check the pods as they are starting. See the following example: While waiting for the pods to start up DX Administrators must create a tls secret for ambassador as follows: kubectl create secret tls dx-tls-cert --cert=my-cert.pem --key=my-key.pem -n aws-mynamespace In this example, an existing key and certification created using OpenSSL was used. Using SSL, administrators can create a private key: 'openssl genrsa -out my-key.pem 2048' Using OpenSSL, administrators can create a certificate signed by the private key: 'openssl req -x509 -key my-key.pem -out my-cert.pem -days 365 -subj '/CN=my-cert' At this stage, the deployment writes out the wp_profile into the persistent volume, and configure HCL DX 9.5 a minimum default configuration. See the HCL DX 9.5 Container Requirements and Customization topics for additional information. Once the HCL DX 9.5 dx-deployment-0 pod is running, administrators can access the HCL DX 9.5 deployment by obtaining the ambassador service details. Command examples to obtain this information: \u2018kubectl get svc -n az-demo\u2019 or \u2018kubectl get svc ambassador -n az-demo\u2019 Using the external IP address obtained via the kubectl get command ( https://external-ip/wps/portal ), select the resulting URL obtained to access your HCL DX 9.5 deployment. Note: It is required to ensure the MS Azure AKS load balancer configured permits external access. For more information, refer to the MS Azure documentation for Load Balancer setup for the default configuration details. (Optional) Deploy the OpenLDAP, Experience API, Content Composer, and Digital Asset Management components to Microsoft AKS Create a config map with the same name as the DX statefulset used to deploy the HCL DX 9.5 CF182 and later Core image software. By default, the DX statefulset is dx-deployment, as shown in this example: kubectl create configmap dx-deployment -n az-demo Once created, populate it with the following data: ``` data: dx.deploy.openldap.enabled: 'true' dx.deploy.openldap.tag: dx-183 dx.deploy.openldap.image: dx-openldap dx.deploy.experienceapi.enabled: 'true' dx.deploy.experienceapi.tag: dx-183 dx.deploy.experienceapi.image: ring-api dx.deploy.contentui.enabled: 'true' dx.deploy.contentui.tag: dx-183 dx.deploy.contentui.image: content-ui dx.deploy.dam.enabled: 'true' dx.deploy.dam.volume: releaseml dx.deploy.dam.storageclass: dx-deploy-stg dx.deploy.dam.persistence.tag: dx-183 dx.deploy.dam.persistence.image: persist dx.deploy.dam.imgprocessor.tag: dx-183 dx.deploy.dam.imgprocessor.image: image-processor dx.deploy.dam.tag: dx-183 dx.deploy.dam.image: dam dx.deploy.dam.operator.tag: dx-183 dx.deploy.dam.operator.image: hcl-dam-operator dx.deploy.host.override: \u201cfalse\u201d ``` Administrators can also create the config map in a YAML file and deploy it with the following instructions (example): kubectl create -f my_config_map.yaml -n az-demo . After creating the config map, the HCL DX 9.5 CF182 and later deployment goes into \u2018 init\u2019 mode, and restart a couple of times after the new options are configured. Administrators can check the status via the command line using the command (example) kubectl get pods -n az-demo : As an alternative approach, administrators can check the status of the deployment progress through the MS Azure AKS dashboard: In this deployment of HCL DX 9.5 core and optional images, the DX core image is the last container to start successfully. Note that it restarts twice. Once restarts are complete, administrators can confirm the deployment and configuration of the DX core and OpenLDAP, Experience API, Content Composer, and Digital Asset Management images as follows: OpenLDAP image deployment validation: Navigate to Practitioner Studio > Administration > Security > Users and Groups , and search for all available groups: The group ldap_test_users should appear in this listing. To validate the Content Composer and Experience API image deployments, navigate to Practitioner Studio > Web Content > Content Composer : To validate the Digital Asset Management and Experience API image deployments, navigate to Practitioner Studio > Digital Assets : To validate access to the Experience API, administrators and developers should be able to access the Experience API at the following URL: https://external-ip/dx/api/core/v1/explorer/ See the following section for additional information: Install Experience API, Content Composer, and Digital Asset Management Update the HCL Digital Experience (DX) 9.5 Azure AKS deployment To update the deployment to later HCL DX 9.5 Container Update releases, follow these steps: Note: If using HCL DX 9.5 Container Update CF192 and later, the dxctl tool can be used to Update the deployment. The dxctl tool does not deploy or update the DxDeployment custom resource definition. Prior to running an Update process, administrators should check the DxDeployment custom resource definition ( hcl-dx-cloud-scripts/deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml ) for changes and update accordingly: Kubernetes command: kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com CAUTION: Since crd is a cluster-wide resource, the use of kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com causes a service outage for all the dx-deployment across the cluster. Kubernetes command: kubectl create -f deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml To update the deployment, run the updateDx.sh script with updated values: ./scripts/updateDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE INGRESSIMAGE INGRESSTAG NAMESPACE - the project or the namespace to create or use for deployment. REPLICAS - the number of initial instances for the deployment. REPOSITORY - your local repository, the repository used by Kubernetes. IMAGENAME - the name of the dxCore image, as added to the repository above. IMAGETAG - the tag for the target image as added to the repository above. VOLUMENAME - the volume to be used by the deployment for persistence, this must use AccessMode ReadWriteMany . STORAGECLASS - the storage class name used to create the persistent volume. DBTYPE - the database type. By default, and initially, this is Derby. HCL DX 9.5 uses Apache Derby, Oracle Database, IBM DB2, or Microsoft SQL Server. Acceptable values are derby , oracle , db2 , or msSql . INGRESSIMAGE - The image name to use for Ambassador (Native K8s). INGRESSTAG - The image tag to use for Ambassador (Native K8s). For example: ./scripts/UpdateDx.sh az-demo 1 azuredxen.azurecr.io dxen v95_CF183_20200819-1159 dxeploy dx-deploy-stg derby ambassador 154 Once the database is transferred, the DBTYPE needs to be updated so you can scale the instances higher. Additionally, once the database is transferred, the number of replicas could be increased. See Customizing your Container deployment for more information on customizing your deployment. Delete the HCL Digital Experience (DX) 9.5 Azure AKS deployment Removing the entire deployment requires several steps, this is by design. To remove the deployment in a specific namespace, run the removeDx.sh script: ./scripts/removeDx.sh **NAMESPACE** NAMESPACE - the project or the namespace created or used for deployment. To remove a namespace, use any of the following commands: Kubernetes command: 'kubectl delete -f dxNameSpace_**NAMESPACE**.yaml' where NAMESPACE is the namespace to be removed The persistent volume associated to the deployment needs to be cleaned up by your Administrator. To reuse a persistent volume, see the following steps: Open the persistent volume in a visual editor (vi) using the Kubernetes command: kubectl edit pv <pv_name> Remove the claimRef section: claimRef: apiVersion: v1 kind: PersistentVolumeClaim name: dx-deploy-pvc namespace: az-demo resourceVersion: \"488931120\" uid: ebd58361-0e2a-11ea-b02e-02f8fe687954 Ensure you get the 'persistentvolume/<pv_name> edited' message. You may need to manually remove any data remaining from the previous deployment.","title":"Deploy DX Container to Microsoft Azure Kubernetes Service \\(AKS\\)"},{"location":"containerization/azure_aks/#deploy-dx-container-to-microsoft-azure-kubernetes-service-aks","text":"Learn how to deploy HCL Digital Experience (DX) 9.5 CF182 and later container release along with Ambassador to Kubernetes, as verified in Microsoft Azure Kubernetes Service (AKS).","title":"Deploy DX Container to Microsoft Azure Kubernetes Service (AKS)"},{"location":"containerization/azure_aks/#about-this-task","text":"Follow these steps to deploy HCL Digital Experience 9.5 CF182 and later container release along with Ambassador to Kubernetes, as verified in Microsoft Azure Kubernetes Service (AKS) . This deployment relies heavily on Kubernetes Operators for full functionality. If deploying HCL DX 9.5 Container Update CF191 and earlier, view the instructions to deploy using script commands instead of the dxctl tool as described below in this Help Center section. Note: Reference the latest HCL DX 9.5 Container Release and Update file list in the Docker deployment topic.","title":"About this task"},{"location":"containerization/azure_aks/#prerequisites","text":"Prior to using the procedure below, it is assumed that the HCL DX Administrator is generally experienced in using Kubernetes. Additionally, the DX Administrator must have the appropriate access to the target environment. If not, following are some preliminary steps that must be taken. Setup KUBECONFIG to refer to the target server. This ensures any kubectl commands executed locally affect the target environment. Example: Use kubectl get {pods, pv, storageclass} to get appropriate information from the artifacts running in the target Kubernetes environment. The following tools must be installed on a machine other than the Portal server: Docker Microsoft Azure CLI If deploying Digital Experience Container Update CF192 and later, the dxctl tool is used to install and configure the deployment Volume requirement: It requires an AccessMode of ReadWriteMany . It requires a minimum of 40 GB , with the default request set to 100 GB . RECLAIM POLICY = Retain Note: HCL Digital Experience is input-output (I/O) intensive and requires a high performing file system for optimization. Azure container registry (For tagging and pushing).","title":"Prerequisites"},{"location":"containerization/azure_aks/#deploying-hcl-digital-experience-dx-95-cf192-and-later-version","text":"Follow these steps to deploy the HCL Digital Experience (DX) 9.5 CF192 and later container release to the Microsoft Azure AKS platform: Download the HCL Digital Experience Container Update CF192 and later release container product and extract it to your local file system. The file system can be on a local workstation or cloud platform. If deploying HCL DX 9.5 Container Update CF192 release, the image and package names are as follows: CF192-core.zip files: ``` HCL DX notices V9.5 CF192.txt - ``` dxclient_v1.2.0_20210305-1758.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF192_20210305-2309.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF192_20210305-2309.zip - ``` hcl-dx-content-composer-image-v1.6.0_20210305-1756.tar.gz - ``` hcl-dx-core-image-v95_CF192_20210305-1758.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF192_20210305-1757.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.6.0_20210305-1802.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210305-1805.zip - ``` hcl-dx-image-processor-image-v1.6.0_20210305-1758.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20210305_1614986151.tar.gz - ``` hcl-dx-postgres-image-v1.6.0_20210305-1800.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF192_20210305-1758.tar.gz - ``` hcl-dx-ringapi-image-v1.6.0_20210305-1802.tar.gz Log in to your Microsoft Azure AKS platform. For more information, refer to the Microsoft Azure documentation if the Microsoft Azure CLI needs to be installed. Example log in command: az login Example: Create a resource group in Microsoft Azure using the following command: az group create --name <resourceGroupName> --location <region> Example: Azure Console Example: For more information, refer to the Microsoft Azure documentation on Resource . Create a Microsoft Azure Container Registry (ACR) to push the HCL DX 9.5 CF192 and later container images to. Azure Console Example: Once the ACR gets created, log in using the following command: az acr login --name <containerRegistry> Example: For more information, refer to the Microsoft Azure documentation on Container Registry . Set up the NFS server. Provide the HCL DX 9.5 CF192 and later Docker image access to the volume mount created in order to copy the profile. There are various ways to do this, and NFS is one option. If NFS is used, here are the parameters that have been tested to work: rw Default. sync Default after NFS 1.0, means that the server does not reply until after the commit. insecure Requires requests originate on ports less than 1024. ** root_squash Map requests to the nobody user.** Hard Required because this means the system keeps trying to write until it works.** nfsvers=4.1 rsize=8388608 Avoids dropped packages, default 8192. wsize=8388608 Avoids dropped packages, default 8192 timeo=600 60 seconds. retrans=2 Number of retries after a time out. noresvport ** Tells the NFS client to use a new Transmission Control Protocol (TCP) source port when a network connection is reestablished. Doing this helps make sure that the EFS file system has uninterrupted availability after a network recovery event. Note: Those marked with (**) are critical and, in many cases, it is recommended to have the rsize and wsize set to 8388608. For more information, refer to the Microsoft Azure documentation on Storage . Configure the Microsoft Azure Kubernetes cluster. To configure kubectl to connect to your Kubernetes cluster, use the az aks get-credentials command. Example: az aks get-credentials --resource-group <resourcegroup> --name <clusterName> For more information, refer to the Microsoft Azure documentation on Cluster .","title":"Deploying HCL Digital Experience (DX) 9.5 CF192 and later version"},{"location":"containerization/azure_aks/#dx-container-image-management","text":"Change directory. Open a terminal window and change to the root directory of the extracted package. Docker load, tag and push by using the following commands: List Docker images docker images Docker load Load the containers into your Docker repository: docker load -i hcl-dx-core-image-v95_CF192_20210225-035822.tar.gz docker load -i hcl-dx-ambassador-image-154.tar.gz docker load -i hcl-dx-cloud-operator-image-v95_CF192_20210225-0546.tar.gz docker load -i hcl-dx-redis-image-5.0.1.tar.gz ACR details To tag and push the images to ACR, obtain login server details: az acr list --resource-group <resourceGroup> --query \"[].{acrLoginServer:loginServer}\" --output table Docker tag Tag your images using the tag command as shown in the examples below: docker tag SOURCE_IMAGE[:TAG] TARGET_IMAGE[:TAG] Example: docker tag hcl/dx/core:v95_CF192_20210225-035822 YOUR_CONTAINER.azurecr.io/hcl/dx/core:v95_CF192_20210225-035822 docker tag hcl/dx/cloud-operator:v95_CF192_20210225-0546-YOUR_CONTAINER.azurecr.io/hcl/dx/cloud-operator:v95_CF192_20210225-0546 docker tag hcl/dx/ambassador:YOUR_CONTAINER.azurecr.io/hcl/dx/ambassador:154 docker tag hcl/dx/redis:5.0.1 YOUR_CONTAINER.azurecr.io/hcl/dx/redis:5.0.1 Docker push Push the images to ACR using the following push command: docker push [OPTIONS] NAME[:TAG] Example commands: docker push YOUR_CONTAINER.azurecr.io/hcl/dx/core:v95_CF192_20210225-035822 docker push YOUR_CONTAINER.azurecr.io/hcl/dx/cloud-operator:v95_CF192_20210225-0546 docker push YOUR_CONTAINER.azurecr.io/hcl/dx/ambassador:154 docker push YOUR_CONTAINER.azurecr.io/hcl/dx/redis:5.0.1 Once the images are pushed, they can be listed using the commands below, or through use of the Microsoft Azure Kubernetes platform console. Command Example: az acr repository list --name <acrName> --output table Microsoft Azure AKS Console - DX 9.5 example:","title":"DX-Container Image Management"},{"location":"containerization/azure_aks/#dx-deployment-using-dxctl","text":"Create a StorageClass . Sample StorageClass YAML: ``` kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: dx-deploy-stg provisioner: example.com/nfs ``` Create a Persistence Volume (pv) with AccessMode as ReadWriteMany and reclaim policy as Retain. Sample PV YAML: apiVersion: v1 kind: PersistentVolume metadata: name: blrcaps-core-3 spec: capacity: storage: 100Gi accessModes: - ReadWriteMany nfs: path: NFS_PATH server: NFS_SERVER persistentVolumeReclaimPolicy: Retain storageClassName: dx-deploy-stg mountOptions: - hard - nfsvers=4.1 - rsize=10485760 - wsize=10485760 - timeo=600 - retrans=2 - noresvport Note: Make sure the PV is available. If it is not, remove claimRef: from the YAML file. Log in to the cluster. Before using the dxctl tool to deploy, you must be logged in to the targeted cluster using the cloud platform specific CLI (Red Hat OpenShift, Amazon EKS, Microsoft Azure AKS, Google GKE). Example: az login Download dxctl . Instructions for downloading the latest packages are available here . Once downloaded and extracted, the hcl-dx-cloud-scripts directory structure is as follows: For more information about dxctl , visit the following documentation here . Configure and deploy using the HCL DX dcxtl tool. To start, change to the extracted files directory using the following command: cd hcl-dx-cloud-scripts Using DX Container Update CF192 and later, the directory structure appears as follows: Configure the dxctl properties for the DX 9.5 Container CF192 and later deployment. Copy one of the provided properties files to further modify for your deployment. The modified properties file can be used for the deployment and the same file must be used for further updates. Example: mkdir -p /home/$USER/deployments/ cp dxctl/properties/full-deployment.properties /home/$USER/deployments/myfirst_deployment.properties Then, update the dxctl properties file values. Sample values: dx.namespace: endgametest-jeet1 dx.image: dxen dx.tag: v95_CF192_20210225-035822_rohan_release_95_CF192_60374773 dx.storageclass:dx-deploy-stg dx.volume: jeet3 dx.volume.size:100 remote.search.enabled:false openldap.enabled:false api.enabled: false composer.enabled: false dam.enabled: false ingress.image:dx-build-output/common/ambassador ingress.tag:1.5.4 ingress.redis.image:redis ingress.redis.tag:5.0.1 dx.operator.image: dx-build-output/hcldx-cloud-operator/hcldx-cloud-operator dx.operator.tag: v95_CF192_20210225-0546_xxxxxxxxx_95_CF192 Important: With HCL DX 9.5 Container Update CF197 and later, dam.features in full-deployment properties is added for use in a future container update release, and should not be modified except with direct guidance from HCL Support. Note: With HCL DX 9.5 Container Update CF193 and later, persist.force.read in full-deployment properties is added to enable a read-only Postgres pod for Digital Asset Management. This enables a failover capability for the Postgres service supporting DAM. Another option to enable a read-only pod is to set the persist.minreplicas: option set to greater than 1. Example: Deploy using dxctl . Run the following command to deploy HCL DX 9.5 Container Update CF192 and later to Microsoft Azure AKS: ./mac/dxctl --deploy -p /home/$USER/deployments/myfirst_deployment.properties Example: Note: This set of steps result in a deployment being created. Validate the deployment. Make sure all the pods are \"Running\" and in \"Ready\" state on your Microsoft Azure AKS platform, as shown in the example below:","title":"DX-Deployment using dxctl"},{"location":"containerization/azure_aks/#generate-tls-certificate","text":"Create a TLS certification to be used by the deployment. Prior to this step, create a self-signed certificate to enable HTTPS using the following command: openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -subj '/CN=ambassador-cert' -nodes Then, store the Certificate and Key in a Kubernetes Secret using the following command: kubectl create secret tls dx-tls-cert --cert=cert.pem --key=key.pem -n <YourNamespace> Afterward, access the HCL DX 9.5 CF192 and later container deployment. To do so, obtain the external IP from the container platform Load balancer to access the HCL DX 9.5 deployment, as shown in the example below: $ kubectl get all -n NAMESPACE Then run the next command: https://EXTERNAL_IP/wps/portal Note: It is required to ensure the Microsoft Azure AKS load balancer configured permits external access. Consult the Microsoft Azure documentation for Load Balancer setup and default configuration details .","title":"Generate TLS Certificate"},{"location":"containerization/azure_aks/#update-the-hcl-digital-experience-dx-95-azure-aks-deployment-to-later-hcl-dx-95-container-update-releases","text":"To update the deployment to later HCL DX 9.5 Container Update releases, follow these steps: Update the deployment properties file with new image values, and run the Update command. Note: If the properties file is not available, then execute the following command to generate one. ./win/dxctl --getproperties --dx.namespace <Your Namespace> Example: On Mac: ./mac/dxctl --update -p properties/myfirst_deployment.properties Example: On Windows: .\\win\\dxctl.exe --update -p properties\\myfirst_deployment.properties On Linux: ./linux/dxctl -\u2013update -p properties/myfirst_deployment.properties Additional considerations: For example, once the database is transferred, the DBTYPE must be updated so you can scale the instances higher. Additionally, once the database is transferred, the number of replicas could be increased. There are additional options to customize the deployment .","title":"Update the HCL Digital Experience (DX) 9.5 Azure AKS deployment to later HCL DX 9.5 Container Update releases"},{"location":"containerization/azure_aks/#delete-the-hcl-digital-experience-dx-95-cf192-and-later-release-azure-aks-deployment","text":"To delete the deployment, follow one of two methods: Method 1: Remove the deployment but allow for redeployment with the same volumes using the following command: ./linux/dxctl --destroy -p properties/myfirst_deployment.properties Method 2: Remove the entire namespace/project using the following command: ./linux/dxctl --destroy -p properties/myfirst_deployment.properties -all true Example: If some resources like services are still not deleted, run the following command: kubectl patch services $(kubectl get services -n $NAMESPACE | grep -v \"NAME\" |awk '{print $1}') -p '{\"metadata\":{\"finalizers\":null}}' -n $NAMESPACE","title":"Delete the HCL Digital Experience (DX) 9.5 CF192 and later release Azure AKS deployment"},{"location":"containerization/azure_aks/#deploying-hcl-digital-experience-dx-95-cf191-and-earlier-version","text":"Follow these steps to deploy the HCL Digital Experience (DX) 9.5 CF191 and earlier container version to the Microsoft Azure AKS platform: Download and extract the contents of the HCL DX 9.5 CF182 package to the local file system. In Microsoft Azure Kubernetes Service (AKS), load, tag, and push the HCL Digital Experience images into your MS Azure Container Registries. Note: In Microsoft Azure, when using AKS a single Container Registry, or multiple Container Registries may be used. See the Microsoft Azure Container Registry documentation for additional information about this topic. In this example, 10 Container Registries are created: As an alternative, DX Administrators can use a single or fewer registries and create 'Repositories' within. In this example, a Container Registry named azambassador with a repository 'ambassador' is shown: Administrators can tag and push another image into this Container Registry to get a second repository. In the following example, the Ambassador Redis image is added: The HCL DX 9.5 Container deployment does not assume 1, or many registries are defined, and either definition setup works. In the following example, the HCL DX 9.5 Redis 5.0.1 image is added to the azambassador Container Registry. This example shows loading the HCL DX 9.5 CF181 and earlier container into a local repository, tagging it and pushing it to the azuredxen Container Registry in the dxen \u2018Repository\u2019.","title":"Deploying HCL Digital Experience (DX) 9.5 CF191 and earlier version"},{"location":"containerization/azure_aks/#install-the-hcl-digital-experience-dx-95-cf182-and-later-core-images","text":"Load the HCL DX 9.5 CF182 and later images to your deployment. The following example uses the CF183 version in the load command: Docker load -I hcl-dx-core-image-v95_CF183_20200819-1159.tar.gz Docker tag and docker push to the Azure environment: Once complete, the image is viewable in the Microsoft Azure repository: Reminder : Consult the HCL Digital Experience 9.5 Deployment \u2013 Docker topic for the latest list of HCL DX 9.5 container files that are available. HCL DX 9.5 Container Update CF183 files are used in these examples: CF183-core.zip files HCL DX notices V9.5 CF183.txt hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF183_20200818-1852.tar.gz hcl-dx-cloud-scripts-v95_CF183_20200818-1852.zip hcl-dx-core-image-v95_CF183_20200818-1342.tar.gz hcl-dx-redis-image-5.0.1.tar.gz CF183-other.zip files HCL DX notices V9.5 CF183.txt hcl-dx-content-composer-image-v1.2.0_20200818-1343.tar.gz hcl-dx-digital-asset-management-operator-image-v95_CF183_20200818-1344.tar.gz hcl-dx-digital-asset-manager-image-v1.2.0_20200818-1346.tar.gz hcl-dx-image-processor-image-v1.2.0_20200818-1345.tar.gz hcl-dx-openldap-image-v1.0.0-master_20200818_1597758965.tar.gz hcl-dx-postgres-image-v1.2.0_20200818-1349.tar.gz hcl-dx-remote-search-image-v95_CF183_20200818-1342.tar.gz hcl-dx-ringapi-image-v1.2.0_20200818-1351.tar.gz To install HCL Digital Experience 9.5 core software to Microsoft Azure AKS, the following images are required: hcl-dx-cloud-operator-image-v95 hcl-dx-core-image-v95 hcl-dx-ambassador-image hcl-dx-redis-image Images included in the \u2018other\u2019 package are optional and used to support use of OpenLDAP, Remote Search, the Experience API, Content Composer, and Digital Asset Management components and services. See examples that show how to load HCL DX 9.5 images to MS Azure below. In the following example, the items are loaded into the azuredxen Content Registry and multiple repositories are created. Images are tagged with dx-183 reflecting the HCL DX 9.5 Container Update CF183 version images used in this deployment. At this stage, the ./deploy/operator.yaml needs to be properly updated and the operator, and Redis image details need to be provided: First, replace the line: From: \u2018image: REPOSITORY_NAME /hcldx-cloud-operator:9.5.next\u2019 To: Add the proper value for the deployment, as in the following example: \u2018image: azuredxen.azurecr.io/hcldx-cloud-operator:v95_CF183_20200819-1711\u2019 Next, replace the values: \"REDIS_REPO\",\"REDIS_IMG_ENV\",\u201cREDIS_TAG_ENV\u201d with proper values. See the following example: Reviewing the Azure dashboard, administrators can see the following for redis: Deploy the Custom Resource Definition using the scripts/deployCrd.sh file. See the following example: Important : Ensure there is an available persistent volume for the deployment or a self-provisioning storage class. The HCL DX Help Center topic ( Sample Storage Class and Volume for HCL Digital Experience 9.5 Container Deployments ) can be referenced for related guidance. In this example, a storage class named dx-deploy-stg and a volume dxdeployhave been created: Run the deployment scripts as follows: ./scripts/deployDx.sh az-demo 1 azuredxen.azurecr.io dxen v95_CF183_20200819-1159 dxeploy dx-deploy-stg derby ambassador 154 NAMESPACE - the project or the namespace to create or use for deployment. REPLICAS - the number of initial instances for the deployment. REPOSITORY - your local repository, the repository used by Kubernetes. IMAGENAME - the name of the dxCore image, as added to the repository above. IMAGETAG - the tag for the target image as added to the repository above. VOLUMENAME - the volume to be used by the deployment for persistence, this must use AccessMode ReadWriteMany . STORAGECLASS - the storage class name used to create the persistent volume. DBTYPE - the database type. By default, and initially, this is Derby. HCL DX 9.5 uses Apache Derby, Oracle Database, IBM DB2, or Microsoft SQL Server. Acceptable values are derby , oracle , db2 , or msSql . INGRESSIMAGE - The image name to use for Ambassador. INGRESSTAG - The image tag to use for Ambassador. The command output shows the values as they align with the deployment, and the result of each step. DX Administrators can use \u2018kubectl get pods -n az-emo\u2019 to check the pods as they are starting. See the following example: While waiting for the pods to start up DX Administrators must create a tls secret for ambassador as follows: kubectl create secret tls dx-tls-cert --cert=my-cert.pem --key=my-key.pem -n aws-mynamespace In this example, an existing key and certification created using OpenSSL was used. Using SSL, administrators can create a private key: 'openssl genrsa -out my-key.pem 2048' Using OpenSSL, administrators can create a certificate signed by the private key: 'openssl req -x509 -key my-key.pem -out my-cert.pem -days 365 -subj '/CN=my-cert' At this stage, the deployment writes out the wp_profile into the persistent volume, and configure HCL DX 9.5 a minimum default configuration. See the HCL DX 9.5 Container Requirements and Customization topics for additional information. Once the HCL DX 9.5 dx-deployment-0 pod is running, administrators can access the HCL DX 9.5 deployment by obtaining the ambassador service details. Command examples to obtain this information: \u2018kubectl get svc -n az-demo\u2019 or \u2018kubectl get svc ambassador -n az-demo\u2019 Using the external IP address obtained via the kubectl get command ( https://external-ip/wps/portal ), select the resulting URL obtained to access your HCL DX 9.5 deployment. Note: It is required to ensure the MS Azure AKS load balancer configured permits external access. For more information, refer to the MS Azure documentation for Load Balancer setup for the default configuration details.","title":"Install the HCL Digital Experience (DX) 9.5 CF182 and later core images"},{"location":"containerization/azure_aks/#optional-deploy-the-openldap-experience-api-content-composer-and-digital-asset-management-components-to-microsoft-aks","text":"Create a config map with the same name as the DX statefulset used to deploy the HCL DX 9.5 CF182 and later Core image software. By default, the DX statefulset is dx-deployment, as shown in this example: kubectl create configmap dx-deployment -n az-demo Once created, populate it with the following data: ``` data: dx.deploy.openldap.enabled: 'true' dx.deploy.openldap.tag: dx-183 dx.deploy.openldap.image: dx-openldap dx.deploy.experienceapi.enabled: 'true' dx.deploy.experienceapi.tag: dx-183 dx.deploy.experienceapi.image: ring-api dx.deploy.contentui.enabled: 'true' dx.deploy.contentui.tag: dx-183 dx.deploy.contentui.image: content-ui dx.deploy.dam.enabled: 'true' dx.deploy.dam.volume: releaseml dx.deploy.dam.storageclass: dx-deploy-stg dx.deploy.dam.persistence.tag: dx-183 dx.deploy.dam.persistence.image: persist dx.deploy.dam.imgprocessor.tag: dx-183 dx.deploy.dam.imgprocessor.image: image-processor dx.deploy.dam.tag: dx-183 dx.deploy.dam.image: dam dx.deploy.dam.operator.tag: dx-183 dx.deploy.dam.operator.image: hcl-dam-operator dx.deploy.host.override: \u201cfalse\u201d ``` Administrators can also create the config map in a YAML file and deploy it with the following instructions (example): kubectl create -f my_config_map.yaml -n az-demo . After creating the config map, the HCL DX 9.5 CF182 and later deployment goes into \u2018 init\u2019 mode, and restart a couple of times after the new options are configured. Administrators can check the status via the command line using the command (example) kubectl get pods -n az-demo : As an alternative approach, administrators can check the status of the deployment progress through the MS Azure AKS dashboard: In this deployment of HCL DX 9.5 core and optional images, the DX core image is the last container to start successfully. Note that it restarts twice. Once restarts are complete, administrators can confirm the deployment and configuration of the DX core and OpenLDAP, Experience API, Content Composer, and Digital Asset Management images as follows: OpenLDAP image deployment validation: Navigate to Practitioner Studio > Administration > Security > Users and Groups , and search for all available groups: The group ldap_test_users should appear in this listing. To validate the Content Composer and Experience API image deployments, navigate to Practitioner Studio > Web Content > Content Composer : To validate the Digital Asset Management and Experience API image deployments, navigate to Practitioner Studio > Digital Assets : To validate access to the Experience API, administrators and developers should be able to access the Experience API at the following URL: https://external-ip/dx/api/core/v1/explorer/ See the following section for additional information: Install Experience API, Content Composer, and Digital Asset Management","title":"(Optional) Deploy the OpenLDAP, Experience API, Content Composer, and Digital Asset Management components to Microsoft AKS"},{"location":"containerization/azure_aks/#update-the-hcl-digital-experience-dx-95-azure-aks-deployment","text":"To update the deployment to later HCL DX 9.5 Container Update releases, follow these steps: Note: If using HCL DX 9.5 Container Update CF192 and later, the dxctl tool can be used to Update the deployment. The dxctl tool does not deploy or update the DxDeployment custom resource definition. Prior to running an Update process, administrators should check the DxDeployment custom resource definition ( hcl-dx-cloud-scripts/deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml ) for changes and update accordingly: Kubernetes command: kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com CAUTION: Since crd is a cluster-wide resource, the use of kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com causes a service outage for all the dx-deployment across the cluster. Kubernetes command: kubectl create -f deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml To update the deployment, run the updateDx.sh script with updated values: ./scripts/updateDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE INGRESSIMAGE INGRESSTAG NAMESPACE - the project or the namespace to create or use for deployment. REPLICAS - the number of initial instances for the deployment. REPOSITORY - your local repository, the repository used by Kubernetes. IMAGENAME - the name of the dxCore image, as added to the repository above. IMAGETAG - the tag for the target image as added to the repository above. VOLUMENAME - the volume to be used by the deployment for persistence, this must use AccessMode ReadWriteMany . STORAGECLASS - the storage class name used to create the persistent volume. DBTYPE - the database type. By default, and initially, this is Derby. HCL DX 9.5 uses Apache Derby, Oracle Database, IBM DB2, or Microsoft SQL Server. Acceptable values are derby , oracle , db2 , or msSql . INGRESSIMAGE - The image name to use for Ambassador (Native K8s). INGRESSTAG - The image tag to use for Ambassador (Native K8s). For example: ./scripts/UpdateDx.sh az-demo 1 azuredxen.azurecr.io dxen v95_CF183_20200819-1159 dxeploy dx-deploy-stg derby ambassador 154 Once the database is transferred, the DBTYPE needs to be updated so you can scale the instances higher. Additionally, once the database is transferred, the number of replicas could be increased. See Customizing your Container deployment for more information on customizing your deployment.","title":"Update the HCL Digital Experience (DX) 9.5 Azure AKS deployment"},{"location":"containerization/azure_aks/#delete-the-hcl-digital-experience-dx-95-azure-aks-deployment","text":"Removing the entire deployment requires several steps, this is by design. To remove the deployment in a specific namespace, run the removeDx.sh script: ./scripts/removeDx.sh **NAMESPACE** NAMESPACE - the project or the namespace created or used for deployment. To remove a namespace, use any of the following commands: Kubernetes command: 'kubectl delete -f dxNameSpace_**NAMESPACE**.yaml' where NAMESPACE is the namespace to be removed The persistent volume associated to the deployment needs to be cleaned up by your Administrator. To reuse a persistent volume, see the following steps: Open the persistent volume in a visual editor (vi) using the Kubernetes command: kubectl edit pv <pv_name> Remove the claimRef section: claimRef: apiVersion: v1 kind: PersistentVolumeClaim name: dx-deploy-pvc namespace: az-demo resourceVersion: \"488931120\" uid: ebd58361-0e2a-11ea-b02e-02f8fe687954 Ensure you get the 'persistentvolume/<pv_name> edited' message. You may need to manually remove any data remaining from the previous deployment.","title":"Delete the HCL Digital Experience (DX) 9.5 Azure AKS deployment"},{"location":"containerization/c_kubesupportstatement/","text":"Platform support matrix View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. HCL encourages customers to remain up-to-date on the latest DX and Kubernetes releases. As a result, DX will provide all fixes on the latest release. Customers may be asked to upgrade to the latest DX release to assist with problem determination. General Kubernetes Support Policy: For the platforms below, HCL will provide support for the Kubernetes releases with the tested versions of DX that are listed in the table. HCL continues to track the Kubernetes release schedules of the platform providers below, with the intent of staying as up-to-date as possible. The specific combinations tested and supported are listed in the following table. From time-to-time, platform providers may release previews of upcoming Kubernetes versions. We will not provide support for those versions. If you encounter any issue on an unsupported or untested Kubernetes or Cloud Platform version, you may be asked to install a supported level. DX CF Releases Red Hat OpenShift Amazon EKS Azure AKS Google GKE CF199 OS 4.7- Kubernetes 1.20 |EKS 1.19|AKS 1.20|GKE 1.21| |CF198|OS 4.7- Kubernetes 1.20 |EKS 1.19|AKS 1.19AKS 1.20 |GKE 1.20| |CF197|OS 4.6- Kubernetes 1.19 OS 4.5 Kubernetes 1.18 |EKS 1.19|AKS 1.19|GKE 1.20| |CF196|OS 4.6- Kubernetes 1.19 OS 4.5 Kubernetes 1.18 |EKS 1.19|AKS 1.19|GKE 1.18| |CF195|OS 4.6- Kubernetes 1.19 OS 4.5 Kubernetes 1.18 |EKS 1.19|AKS 1.19|GKE 1.18| |CF194|OS 4.5- Kubernetes 1.18 OS 4.6 Kubernetes 1.19 |EKS 1.19|AKS 1.19|GKE 1.18| |CF193|OS 4.5- Kubernetes 1.18 OS 4.6 Kubernetes 1.19 |EKS 1.19|AKS 1.19AKS 1.17.7 |GKE 1.18| |CF192|OS 4.4Kubernetes 1.17 |EKS 1.17EKS 1.18 |AKS 1.17.7 AKS 1.18.4 |GKE 1.16 GKE 1.17 GKE 1.18 | |CF191|OS 4.4 Kubernetes 1.17 |EKS 1.17|AKS 1.17.7|GKE 1.16| |CF19|OS 4.3Kubernetes 1.16 |EKS 1.17|AKS 1.17.7|GKE 1.16| |CF184|OS 4.3Kubernetes 1.16 |EKS 1.17|AKS 1.17.7|GKE 1.16| DX CF Releases Red Hat OpenShift CF199 OS 4.7Kubernetes 1.20 | |CF198|OS 4.7Kubernetes 1.20 | |CF197|OS 4.5Kubernetes 1.18 | |CF196|OS 4.5Kubernetes 1.18 | |CF195|OS 4.5Kubernetes 1.18 | |CF194|OS 4.5Kubernetes 1.18 | |CF193|OS 4.5Kubernetes 1.18 | |CF192|OS 4.4Kubernetes 1.17 | |CF191|OS 4.4Kubernetes 1.17 | |CF19|OS 4.3Kubernetes 1.16 | |CF184|OS 4.3Kubernetes 1.16 |","title":"Platform support matrix"},{"location":"containerization/c_kubesupportstatement/#platform-support-matrix","text":"View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. HCL encourages customers to remain up-to-date on the latest DX and Kubernetes releases. As a result, DX will provide all fixes on the latest release. Customers may be asked to upgrade to the latest DX release to assist with problem determination.","title":"Platform support matrix"},{"location":"containerization/c_kubesupportstatement/#general-kubernetes-support-policy","text":"For the platforms below, HCL will provide support for the Kubernetes releases with the tested versions of DX that are listed in the table. HCL continues to track the Kubernetes release schedules of the platform providers below, with the intent of staying as up-to-date as possible. The specific combinations tested and supported are listed in the following table. From time-to-time, platform providers may release previews of upcoming Kubernetes versions. We will not provide support for those versions. If you encounter any issue on an unsupported or untested Kubernetes or Cloud Platform version, you may be asked to install a supported level. DX CF Releases Red Hat OpenShift Amazon EKS Azure AKS Google GKE CF199 OS 4.7- Kubernetes 1.20 |EKS 1.19|AKS 1.20|GKE 1.21| |CF198|OS 4.7- Kubernetes 1.20 |EKS 1.19|AKS 1.19AKS 1.20 |GKE 1.20| |CF197|OS 4.6- Kubernetes 1.19 OS 4.5 Kubernetes 1.18 |EKS 1.19|AKS 1.19|GKE 1.20| |CF196|OS 4.6- Kubernetes 1.19 OS 4.5 Kubernetes 1.18 |EKS 1.19|AKS 1.19|GKE 1.18| |CF195|OS 4.6- Kubernetes 1.19 OS 4.5 Kubernetes 1.18 |EKS 1.19|AKS 1.19|GKE 1.18| |CF194|OS 4.5- Kubernetes 1.18 OS 4.6 Kubernetes 1.19 |EKS 1.19|AKS 1.19|GKE 1.18| |CF193|OS 4.5- Kubernetes 1.18 OS 4.6 Kubernetes 1.19 |EKS 1.19|AKS 1.19AKS 1.17.7 |GKE 1.18| |CF192|OS 4.4Kubernetes 1.17 |EKS 1.17EKS 1.18 |AKS 1.17.7 AKS 1.18.4 |GKE 1.16 GKE 1.17 GKE 1.18 | |CF191|OS 4.4 Kubernetes 1.17 |EKS 1.17|AKS 1.17.7|GKE 1.16| |CF19|OS 4.3Kubernetes 1.16 |EKS 1.17|AKS 1.17.7|GKE 1.16| |CF184|OS 4.3Kubernetes 1.16 |EKS 1.17|AKS 1.17.7|GKE 1.16| DX CF Releases Red Hat OpenShift CF199 OS 4.7Kubernetes 1.20 | |CF198|OS 4.7Kubernetes 1.20 | |CF197|OS 4.5Kubernetes 1.18 | |CF196|OS 4.5Kubernetes 1.18 | |CF195|OS 4.5Kubernetes 1.18 | |CF194|OS 4.5Kubernetes 1.18 | |CF193|OS 4.5Kubernetes 1.18 | |CF192|OS 4.4Kubernetes 1.17 | |CF191|OS 4.4Kubernetes 1.17 | |CF19|OS 4.3Kubernetes 1.16 | |CF184|OS 4.3Kubernetes 1.16 |","title":"General Kubernetes Support Policy:"},{"location":"containerization/container_init_performance/","text":"HCL Digital Experience 9.5 Docker and Container initialization performance Beginning with from HCL Digital Experience 9.5 Container Update CF192 release, container DX applications initialization performance is improved. Review the following guidance for information, defaults, and options to manage container applications initialization performance when deployed to Docker, Red Hat OpenShift, and Kubernetes platforms. Introduction When deployed on the supported Red Hat OpenShift and Kubernetes environments, the HCL Digital Experience core platform \"Pod\" must be started before it can start serving requests. Furthermore, once the Pod is started, HCL Digital Experience Portal and Web Content Manager core must be initialized before the OpenShift or Kubernetes \u201creadiness\u201d probes can determine that the Pod is able to serve requests. The OpenShift or Kubernetes readiness probe functions to execute an HTTP request to the \"/wps/portal\" or \"/ibm/console\" page and ensures that it responds.The HCL DX core must be initialized by execution of the \"startServer.sh WebSphere_Portal\" process before the readiness probes completes successfully. This starts the IBM WebSphere Application Server profile containing HCL DX Portal and Web Content Manager. This initialization process includes several IBM WebSphere and HCL DX applications including select portlets that must initialize before the HCL DX Core can respond to the readiness probe. In DX 9.5 Container Update CF192 and higher, to support a faster initialization of DX core Portal and Web Content Manager in Docker, Red Hat OpenShift, and supported Kubernetes platforms, most DX portlets not required for initial operations will default to a \"lazy load\" initialization. Using this means of initialization, an HCL DX portlet application is not started by a user request, but by the first standard HTTP request that occurs and renders a DX portal page that contains the portlet application on the server. Direct access to the portlet, for example an Ajax request, does not start the portlet. In addition, some IBM WebSphere applications not required for initial operations will not be autostarted. ConfigEngine Tasks Three ConfigEngine tasks are deployed to support improvements to HCL DX Core initialization times. They are: stop-autostart-docker-applications default -autostart-docker-applications start-advanced-editor-applications stop-autostart-docker-applications The stop-autostart-docker-applications task is executed during the Docker image build for DX Core when initialized on Docker, Red Hat OpenShift, or Kubernetes platforms. This task manages the following functions: It will stop the Advanced Rich Text Editor.ear (Textbox.io) files from autostarting. It will \u201clazy load\u201d all DX portlets that do not require the Portal and WCM functions to operate. Portlets required for DX operations that will be loaded and initialized include for example, theme modules that are loaded from Portlets. These portlets must be started in order for the theme modules to load. The \"Login\" and \"WCM Local Rendering\" portlets are in also this list as they are required to present the Woodburn Studio demonstration site entry page, and therefore the Kubernetes readiness probe. Note that the readiness probe defaults to the WebSphere Application Console via probe functions that execute an HTTP request to the \"/wps/portal\" or \"/ibm/console\" page and ensures that it responds. See Figure: Configure Editor Options the list of portlets that are needed for DX operations and will automatically load. default-autostart-docker-applications The default-autostart-docker-applications task will restore the autostart status of all applications to their \u201cout of the box\u201d status (and not apply \u201cLazy load\u201d initialization functions). start-advanced-editor-applications The start-advanced-editor-applications task will start the Advanced Rich Text Editor.ear (Textbox.io) application, if required, and if the customer configures WCM to use the WCM Advanced Editor. Prerequisites: Portal Administrators should run the start-advanced-editor-applications task to start the Advanced Rich Text Editor.ear (Textbox.io) application, then proceed to select the Advanced Editor in the Web Content Manager Authoring > Configure > Editor Options interface. Important Considerations and Limitations of the Container Initialization Improvements As a result of not autostarting, these applications and portlets, initialization of DX Portal may be faster, but the initial access of most pages will initially be slower due to the fact that the application/portlet must now be initialized. Note this only affects the first access of that application/portlet (as initialization is a once per system activity). As new DX PODS are started, initialization of DX pages with non-required applications and portlets will be slower on first HTTP request. Using Advanced Editors for WCM As noted above, beginning with Container Update CF192, and default settings for \u2018lazy load\u2019 of non-required portlets and applications, the Advanced Rich Text Editor .ear Textbox.io for WCM is now NOT started. Since this is not a lazy load but rather a stop of the Advanced Rich Text Editor Textbox IO EAR containing the advanced editor, they must also start the Advanced Rich Text Editor EAR by running the \"start-advanced-editor-applications\" task, before configuring the Advanced Rich Text editor in the Web Content Manager configuration settings , to make the editor available for content authors. It is not necessary in addition to \"commit\u201d the new Docker images once this task completes, because these changes are in the profile which is persisted in an external volume and not in the Docker image. Source File listing of HCL DX required portlets and applications that will autostart: These configuration tasks use four files to obtain the list of HCL DX portlet/applications to autostart. All the files are located in the same directory: configuration root}/PortalServer/installer/wp.config/config/includes For example, a list on this reference system is are located at: /opt/HCL/PortalServer/installer/wp.config/config/includes The four import files are: advancedEditorEAR - Enable the Advanced Editors defaultListOfEnabledApps - The out of the box autostart parameters listOfAppsDockerDisable - List of all portlets and applications who autostart is initially disabled listOfAppsDockerEnable - List of portlets and applications to autostart after having disabled the one in listOfAppsDockerDisable *List of portlets and applications that are automatically initialized by default * (Container Update CF192 release and later): PA_Login_Portlet_App PA_Site_Builder PA_WCMLRingPortJSR286 PA_WCM_Authoring_UI PA_Pingpageproperties PA_Styles PA_Wiring PA_wp.pzn.ui.actions PA_Orphaned PA_VanityUrl PA_New_Page PA_Create_Content PA_Applications PA_Toolbar_Content PA_Toolbar_SiteMap PA_Impersonation PA_WebScanner PA_Theme_Creator *List of portlets and applications initialized via \u201clazy load\u201d * (Container Update CF192 release and later): AJAX Proxy Configuration Default_Theme_85 Dojo_Resources EphoxEditLive ibmasyncrsp isclite JavaContentRepository jQuery_Resources Live_Object_Framework lwp.addtosametimelist_war lwp_groupsViewer_war lwp_peoplefinder_war lwp_peoplePicker_war MashupCommonComponent Mobile_Preview PA_Applications PA_AtiveSiteAnalytics PA_Banner_Ad PA_Blurb_1 PA_CM_Picker PA_ContactList PA_Create_Content PA_CredVaultDialog PA_Dialog_Stack PA_Dialog_State PA_DynamicUIApp PA_EitThemeProperties PA_FedDocumentsPicker PA_Feed_Service_Admin PA_FS_Disambiguation PA_Impersonation PA_IWidget_Wrapper PA_Login_Portlet_App PA_MosoftExchange2010 PA_New_Page PA_Orphaned PA_Page_Picker PA_Pingpageproperties PA_Pmizationframework PA_PortalWSRPProxy PA_PTransformationApp PA_Search_Center PA_SearchSitemapPort PA_Selfcare_Port_App PA_Site_Builder PA_spa PA_Styles PA_Tag_Cloud PA_Theme_Creator PA_Theme_Manager PA_Toolbar_Content PA_Toolbar_SiteMap PA_VanityUrl PA_WCM_Authoring_UI PA_WCMLRingPortJSR286 PA_WCMSupportTools PA_WebDockPortServlet PA_WebScanner PA_Wiring PA_WPF PA_wp.feedspace PA_wp.pzn.ui.actions PA_wp.vwat.manager Personalization_Lists Personalization_Workspace Practitioner_Studio_Theme_95 PSESearchAdapter pznpublish pznscheduler PZN_Utilities Quickr_Document_Picker RESTAPIDocs Seedlist_Servlet Simple_Theme SpellChecker StartupCheck SwaggerUI Theme_Dev_Assets ThemeDevSite Theme_Modules TinyEditorsServices TinyEditorsTextboxio Toolbar_Modules Toolbar_Theme_85 UserProfileRESTServlet wci wcm WCM_EXTENSION wcm-remote-admin-ejb Woodburn_Studio_Theme_95 worklight_extension wp.scriptportlet.editor wp.scriptportlet.importexport wps wps_scheduler wp.theme.ckeditor.ear wp.theme.toolbar.xslt wp.vwat.servlet.ear WSPolicyManager","title":"HCL Digital Experience 9.5 Docker and Container initialization performance"},{"location":"containerization/container_init_performance/#hcl-digital-experience-95-docker-and-container-initialization-performance","text":"Beginning with from HCL Digital Experience 9.5 Container Update CF192 release, container DX applications initialization performance is improved. Review the following guidance for information, defaults, and options to manage container applications initialization performance when deployed to Docker, Red Hat OpenShift, and Kubernetes platforms.","title":"HCL Digital Experience 9.5 Docker and Container initialization performance"},{"location":"containerization/container_init_performance/#introduction","text":"When deployed on the supported Red Hat OpenShift and Kubernetes environments, the HCL Digital Experience core platform \"Pod\" must be started before it can start serving requests. Furthermore, once the Pod is started, HCL Digital Experience Portal and Web Content Manager core must be initialized before the OpenShift or Kubernetes \u201creadiness\u201d probes can determine that the Pod is able to serve requests. The OpenShift or Kubernetes readiness probe functions to execute an HTTP request to the \"/wps/portal\" or \"/ibm/console\" page and ensures that it responds.The HCL DX core must be initialized by execution of the \"startServer.sh WebSphere_Portal\" process before the readiness probes completes successfully. This starts the IBM WebSphere Application Server profile containing HCL DX Portal and Web Content Manager. This initialization process includes several IBM WebSphere and HCL DX applications including select portlets that must initialize before the HCL DX Core can respond to the readiness probe. In DX 9.5 Container Update CF192 and higher, to support a faster initialization of DX core Portal and Web Content Manager in Docker, Red Hat OpenShift, and supported Kubernetes platforms, most DX portlets not required for initial operations will default to a \"lazy load\" initialization. Using this means of initialization, an HCL DX portlet application is not started by a user request, but by the first standard HTTP request that occurs and renders a DX portal page that contains the portlet application on the server. Direct access to the portlet, for example an Ajax request, does not start the portlet. In addition, some IBM WebSphere applications not required for initial operations will not be autostarted.","title":"Introduction"},{"location":"containerization/container_init_performance/#configengine-tasks","text":"Three ConfigEngine tasks are deployed to support improvements to HCL DX Core initialization times. They are: stop-autostart-docker-applications default -autostart-docker-applications start-advanced-editor-applications","title":"ConfigEngine Tasks"},{"location":"containerization/container_init_performance/#stop-autostart-docker-applications","text":"The stop-autostart-docker-applications task is executed during the Docker image build for DX Core when initialized on Docker, Red Hat OpenShift, or Kubernetes platforms. This task manages the following functions: It will stop the Advanced Rich Text Editor.ear (Textbox.io) files from autostarting. It will \u201clazy load\u201d all DX portlets that do not require the Portal and WCM functions to operate. Portlets required for DX operations that will be loaded and initialized include for example, theme modules that are loaded from Portlets. These portlets must be started in order for the theme modules to load. The \"Login\" and \"WCM Local Rendering\" portlets are in also this list as they are required to present the Woodburn Studio demonstration site entry page, and therefore the Kubernetes readiness probe. Note that the readiness probe defaults to the WebSphere Application Console via probe functions that execute an HTTP request to the \"/wps/portal\" or \"/ibm/console\" page and ensures that it responds. See Figure: Configure Editor Options the list of portlets that are needed for DX operations and will automatically load.","title":"stop-autostart-docker-applications"},{"location":"containerization/container_init_performance/#default-autostart-docker-applications","text":"The default-autostart-docker-applications task will restore the autostart status of all applications to their \u201cout of the box\u201d status (and not apply \u201cLazy load\u201d initialization functions).","title":"default-autostart-docker-applications"},{"location":"containerization/container_init_performance/#start-advanced-editor-applications","text":"The start-advanced-editor-applications task will start the Advanced Rich Text Editor.ear (Textbox.io) application, if required, and if the customer configures WCM to use the WCM Advanced Editor. Prerequisites: Portal Administrators should run the start-advanced-editor-applications task to start the Advanced Rich Text Editor.ear (Textbox.io) application, then proceed to select the Advanced Editor in the Web Content Manager Authoring > Configure > Editor Options interface.","title":"start-advanced-editor-applications"},{"location":"containerization/container_init_performance/#important-considerations-and-limitations-of-the-container-initialization-improvements","text":"As a result of not autostarting, these applications and portlets, initialization of DX Portal may be faster, but the initial access of most pages will initially be slower due to the fact that the application/portlet must now be initialized. Note this only affects the first access of that application/portlet (as initialization is a once per system activity). As new DX PODS are started, initialization of DX pages with non-required applications and portlets will be slower on first HTTP request.","title":"Important Considerations and Limitations of the Container Initialization Improvements"},{"location":"containerization/container_init_performance/#using-advanced-editors-for-wcm","text":"As noted above, beginning with Container Update CF192, and default settings for \u2018lazy load\u2019 of non-required portlets and applications, the Advanced Rich Text Editor .ear Textbox.io for WCM is now NOT started. Since this is not a lazy load but rather a stop of the Advanced Rich Text Editor Textbox IO EAR containing the advanced editor, they must also start the Advanced Rich Text Editor EAR by running the \"start-advanced-editor-applications\" task, before configuring the Advanced Rich Text editor in the Web Content Manager configuration settings , to make the editor available for content authors. It is not necessary in addition to \"commit\u201d the new Docker images once this task completes, because these changes are in the profile which is persisted in an external volume and not in the Docker image.","title":"Using Advanced Editors for WCM"},{"location":"containerization/container_init_performance/#source-file-listing-of-hcl-dx-required-portlets-and-applications-that-will-autostart","text":"These configuration tasks use four files to obtain the list of HCL DX portlet/applications to autostart. All the files are located in the same directory: configuration root}/PortalServer/installer/wp.config/config/includes For example, a list on this reference system is are located at: /opt/HCL/PortalServer/installer/wp.config/config/includes The four import files are: advancedEditorEAR - Enable the Advanced Editors defaultListOfEnabledApps - The out of the box autostart parameters listOfAppsDockerDisable - List of all portlets and applications who autostart is initially disabled listOfAppsDockerEnable - List of portlets and applications to autostart after having disabled the one in listOfAppsDockerDisable *List of portlets and applications that are automatically initialized by default * (Container Update CF192 release and later): PA_Login_Portlet_App PA_Site_Builder PA_WCMLRingPortJSR286 PA_WCM_Authoring_UI PA_Pingpageproperties PA_Styles PA_Wiring PA_wp.pzn.ui.actions PA_Orphaned PA_VanityUrl PA_New_Page PA_Create_Content PA_Applications PA_Toolbar_Content PA_Toolbar_SiteMap PA_Impersonation PA_WebScanner PA_Theme_Creator *List of portlets and applications initialized via \u201clazy load\u201d * (Container Update CF192 release and later): AJAX Proxy Configuration Default_Theme_85 Dojo_Resources EphoxEditLive ibmasyncrsp isclite JavaContentRepository jQuery_Resources Live_Object_Framework lwp.addtosametimelist_war lwp_groupsViewer_war lwp_peoplefinder_war lwp_peoplePicker_war MashupCommonComponent Mobile_Preview PA_Applications PA_AtiveSiteAnalytics PA_Banner_Ad PA_Blurb_1 PA_CM_Picker PA_ContactList PA_Create_Content PA_CredVaultDialog PA_Dialog_Stack PA_Dialog_State PA_DynamicUIApp PA_EitThemeProperties PA_FedDocumentsPicker PA_Feed_Service_Admin PA_FS_Disambiguation PA_Impersonation PA_IWidget_Wrapper PA_Login_Portlet_App PA_MosoftExchange2010 PA_New_Page PA_Orphaned PA_Page_Picker PA_Pingpageproperties PA_Pmizationframework PA_PortalWSRPProxy PA_PTransformationApp PA_Search_Center PA_SearchSitemapPort PA_Selfcare_Port_App PA_Site_Builder PA_spa PA_Styles PA_Tag_Cloud PA_Theme_Creator PA_Theme_Manager PA_Toolbar_Content PA_Toolbar_SiteMap PA_VanityUrl PA_WCM_Authoring_UI PA_WCMLRingPortJSR286 PA_WCMSupportTools PA_WebDockPortServlet PA_WebScanner PA_Wiring PA_WPF PA_wp.feedspace PA_wp.pzn.ui.actions PA_wp.vwat.manager Personalization_Lists Personalization_Workspace Practitioner_Studio_Theme_95 PSESearchAdapter pznpublish pznscheduler PZN_Utilities Quickr_Document_Picker RESTAPIDocs Seedlist_Servlet Simple_Theme SpellChecker StartupCheck SwaggerUI Theme_Dev_Assets ThemeDevSite Theme_Modules TinyEditorsServices TinyEditorsTextboxio Toolbar_Modules Toolbar_Theme_85 UserProfileRESTServlet wci wcm WCM_EXTENSION wcm-remote-admin-ejb Woodburn_Studio_Theme_95 worklight_extension wp.scriptportlet.editor wp.scriptportlet.importexport wps wps_scheduler wp.theme.ckeditor.ear wp.theme.toolbar.xslt wp.vwat.servlet.ear WSPolicyManager","title":"Source File listing of HCL DX required portlets and applications that will autostart:"},{"location":"containerization/customizing_container_deployment/","text":"Customizing the container deployment This section describes how to customize your HCL Digital Experience 9.5 container deployment. About this task Follow this procedure to deploy or update your HCL Portal deployment. DX 9.5 containerization is focused on deployment and it uses an operator-based deployment. Goals To introduce a supported containerized deployment that HCL Digital Experience can continually extend to provide customers with the best possible experience. To provide a high level of customization in the deployment and continue to expand on that, along with increased automation. Before you begin Customers need to follow the recommended deployment model to ensure the availability of future functions and prevent potential conflicts. Customizing the deployment requires updating the deploy/crds/git.cwp.pnp-hcl.com_v1_dxdeployment_cr.yamlfile located in the hcl-dx-cloud-scripts/deploy/crds directory in the HCL Digital Experience 9.5 platform packages deployed. Reference the HCL Digital Experience Deployment topic for the list of supported platforms and latest HCL DX 9.5 container package list files that can be downloaded from your HCL Digital Experience entitlements on the HCL Software License Portal. Once modified, the deployDx.sh or the updateDx.sh scripts should be run to perform (or update) the target deployment. Note: All modifications should be made to the custom resource instance and not the individual parts of the deployment. Procedure Create a backup of the git_v1_dxdeployment_cr.yaml file. Open the original file in edit mode. Find the line with the text labeled # Add fields here . Customizations should be done below this line. Add the following customizations as applicable: Volume Size By default, the volume size is 100 GB . This can be modified by changing the following: Note: The volume name and storageClassName should not be modified here. Resources By default, the resource requests are set at **2** CPU and **7G** RAM. These values can be changed. It is recommended to adjust the server heap size before changing these values. Note: Limits are not enforced in the initial 9.5 release. HCL DX 9.5 Container Update CF171 and higher Limits are enforced. Auto-scaling based on average CPU and memory utilization can be configured. Auto-scaling When using a Horizontal Pod Autoscale Service, by design, scaling up the amount of HCL DX 9.5 pods is done one at a time. HCL DX 9.5 processes will initiate the requested single instance at a given time until the instance starting is started, to manage scaling in a controlled manager from 1 to N minimum pods set. Each deployment takes approximately ~3 to 4 minutes to start, operating on typical hardware environments. Pod instance terminations are also managed with these control practices. Scaling is controlled in the configuration map with these settings, which can be configured. In this example, 5 is the maximum number of DX 9.5 Container pods requested: dx.deploy.dxcore.resources.scale.maxreplicas: '5' dx.deploy.dxcore.resources.scale.minreplicas: '1' Routes/Ingress By default in 9.5, base routes are created for the deployment. HCL DX 9.5 Container Update CF171 and higher allows a customer to configure the available routes. You can enable or disable any route and change the name of the secret to be used in the TLS context. The Configuration Wizard is still impacted by the number of running instances. Probes The default readiness and liveness probes run against the ../ibm/console. This can and should be overridden. Notes: There are two types of checks: **command** runs a command against the server **http** hits either an http or an https URL. The syntax and required fields are shown in the above image. Logging By default, logging is done on the shared profile so all instances are writing to a single set of logs, with the volume set for each instance at **1G** . For diagnosing production issues this is not ideal. This option allows each instance to write the log to its own log directory. Notes: The environment must have a self-provisioning storage class provisioner. **Enabled** must be set to **true** . Adjusting the log settings must be done to prevent running out of disk storage. See the Logging and tracing for containers and new services Help Center topic for additional information. Ports By default, the deployment uses the default DX ports. The routes in these ports expose Portal through http and https . Note: If there is a need to configure the containerized Portal to use different ports, the defaults can be overwritten. Once modified, the deployDx.sh and the updateDx.sh scripts should be run to create (or update) the target deployment.","title":"Customizing the container deployment"},{"location":"containerization/customizing_container_deployment/#customizing-the-container-deployment","text":"This section describes how to customize your HCL Digital Experience 9.5 container deployment.","title":"Customizing the container deployment"},{"location":"containerization/customizing_container_deployment/#about-this-task","text":"Follow this procedure to deploy or update your HCL Portal deployment. DX 9.5 containerization is focused on deployment and it uses an operator-based deployment. Goals To introduce a supported containerized deployment that HCL Digital Experience can continually extend to provide customers with the best possible experience. To provide a high level of customization in the deployment and continue to expand on that, along with increased automation.","title":"About this task"},{"location":"containerization/customizing_container_deployment/#before-you-begin","text":"Customers need to follow the recommended deployment model to ensure the availability of future functions and prevent potential conflicts. Customizing the deployment requires updating the deploy/crds/git.cwp.pnp-hcl.com_v1_dxdeployment_cr.yamlfile located in the hcl-dx-cloud-scripts/deploy/crds directory in the HCL Digital Experience 9.5 platform packages deployed. Reference the HCL Digital Experience Deployment topic for the list of supported platforms and latest HCL DX 9.5 container package list files that can be downloaded from your HCL Digital Experience entitlements on the HCL Software License Portal. Once modified, the deployDx.sh or the updateDx.sh scripts should be run to perform (or update) the target deployment. Note: All modifications should be made to the custom resource instance and not the individual parts of the deployment.","title":"Before you begin"},{"location":"containerization/customizing_container_deployment/#procedure","text":"Create a backup of the git_v1_dxdeployment_cr.yaml file. Open the original file in edit mode. Find the line with the text labeled # Add fields here . Customizations should be done below this line. Add the following customizations as applicable: Volume Size By default, the volume size is 100 GB . This can be modified by changing the following: Note: The volume name and storageClassName should not be modified here. Resources By default, the resource requests are set at **2** CPU and **7G** RAM. These values can be changed. It is recommended to adjust the server heap size before changing these values. Note: Limits are not enforced in the initial 9.5 release. HCL DX 9.5 Container Update CF171 and higher Limits are enforced. Auto-scaling based on average CPU and memory utilization can be configured. Auto-scaling When using a Horizontal Pod Autoscale Service, by design, scaling up the amount of HCL DX 9.5 pods is done one at a time. HCL DX 9.5 processes will initiate the requested single instance at a given time until the instance starting is started, to manage scaling in a controlled manager from 1 to N minimum pods set. Each deployment takes approximately ~3 to 4 minutes to start, operating on typical hardware environments. Pod instance terminations are also managed with these control practices. Scaling is controlled in the configuration map with these settings, which can be configured. In this example, 5 is the maximum number of DX 9.5 Container pods requested: dx.deploy.dxcore.resources.scale.maxreplicas: '5' dx.deploy.dxcore.resources.scale.minreplicas: '1' Routes/Ingress By default in 9.5, base routes are created for the deployment. HCL DX 9.5 Container Update CF171 and higher allows a customer to configure the available routes. You can enable or disable any route and change the name of the secret to be used in the TLS context. The Configuration Wizard is still impacted by the number of running instances. Probes The default readiness and liveness probes run against the ../ibm/console. This can and should be overridden. Notes: There are two types of checks: **command** runs a command against the server **http** hits either an http or an https URL. The syntax and required fields are shown in the above image. Logging By default, logging is done on the shared profile so all instances are writing to a single set of logs, with the volume set for each instance at **1G** . For diagnosing production issues this is not ideal. This option allows each instance to write the log to its own log directory. Notes: The environment must have a self-provisioning storage class provisioner. **Enabled** must be set to **true** . Adjusting the log settings must be done to prevent running out of disk storage. See the Logging and tracing for containers and new services Help Center topic for additional information. Ports By default, the deployment uses the default DX ports. The routes in these ports expose Portal through http and https . Note: If there is a need to configure the containerized Portal to use different ports, the defaults can be overwritten. Once modified, the deployDx.sh and the updateDx.sh scripts should be run to create (or update) the target deployment.","title":"Procedure"},{"location":"containerization/cw_containerdbtransfer_ibm_db2/","text":"Transfer HCL Digital Experience 9.5 container default database to IBM DB2 HCL Digital Experience 9.5 installs a copy of Derby as the default database. Administrator users can follow these steps to transfer the default database configuration detail to IBM DB2, if preferred for use as the relational database for HCL Digital Experience 9.5 container deployment data. The directions closely follow the Digital Experience database transfer steps provided for deployments to supported \"on-premises\" platforms, such as Windows, Linux, and AIX. The unique steps that account for differences in these instructions for use with an HCL Digital Experience 9.5 Docker container deployment as opposed to an on-premises Digital Experience installation are highlighted. The on-premises based HCL Digital Experience database transfer instructions can be viewed in this topic: DB2 worksheet: Transfer to multiple databases . These container deployment instructions cover the transfer of the default Derby database running on an HCL Digital Experience Container Update CF18 container image to an IBM DB2 Enterprise Version 11.1.0.0 database running on a CentOS Linux release 7.71908 server. The IBM DB2 11.5 Standard and Enterprise database is also supported for these procedures. In these instructions, the IBM DB2 database is installed to a supported on-premises platform (see IBM DB2 System Support requirements for a list of supported platforms ). The HCL Digital Experience 9.5 Docker container is deployed to a supported Docker or Kubernetes platform. The container version deployed in this example is Docker CE version 19.0.3.8. Video : HCL Digital Experience - Perform a Database Transfer on HCL Portal 9.5 Prerequisites The HCL Digital Experience 9.5 container image is deployed, and loaded to your Docker repository using the following command: docker load < hcl-dx-core-image-v95_CF18_20200427-2055.tar.gz See the HCL Digital Experience 9.5 topic Deployment for instructions and latest list of HCL Digital Experience 9.5 Container image and file names. IBM DB2 is installed on an on-premises server with a DNS hostname that is available to the HCL Digital Experience 9.5 deployed container. Start the HCL Digital Experience 9.5 Container in Docker Execute the following command to start the DX 9.5 container: docker run -d --add-host {database DNS name}:{database IP address} -p 10025-10045:10025-10045 -p 10200-10210:10200-10210 -v '{directory on the Docker host for DX container profile}':'/opt/HCL/wp_profile' {your repo name}:{your tag name} Note: An --add-host entry needs to be made to insert the DNS name of the IBM DB2 database server into the /etc/hosts file in the container running the Digital Experience 9.5 container. If, and only if, the DNS name of the DB2 server is already in a named server available to the container, then the --add-host parameter would not be needed. The ports for the Digital Experience 9.5 container (100025-10045) need to be mapped as well as the ports using by the Configuration Wizard. The Configuration Wizard is used to manage the transfer of the database. The Configuration Wizard uses ports in the range of 10200-10202. The HCL Digital Experience 9.5 container persists the profile information across restarts. This is persisted on the Docker host as {directory on docker host for DX container profile} in the run command above. The container will map /opt/HCL/wp_profile to this directory on the docker via the -v command. You need to specify the Docker repository and tag name as the reader has loaded the HCL Digital Experience container image into their docker repository. When this docker run command is executed, ensure time is allocated to check the profile as well as initialize the HCL Digital Experience 9.5 container instance. One can ensure that the HCL Digital Experience 9.5 container deployment is ready by \"tail\"-ing the file in the container located at /opt/HCL/wp_profile/logs/HCL Portal and HCL Web Content Manager/SystemOut.log. When the file displays the following message, the HCL Digital Experience 9.5 container instance is initialized: \u2026open for e- business One can access this file (as well as all others in the running container) via the following command: docker exec -it {container name} bash Transferring the Database First, ensure that your HCL Digital Experience 9.5 docker container can access the IBM DB2 on-premises platform server. Using the following command, one can simply \"ping\" the DNS name of the IBM DB2 database server and verify that it answers: docker exec -it {container name} bash If this is not successful, consult with your platform administrator for other methods to debug the network issues between a Docker container and Servers running external to Docker in your environment. Next, once connectivity is established, follow the directions for using the Configuration Wizard from the traditional (on-premises platform-based) Digital Experience database transfer steps to transfer the data from the default Derby database and configure the target IBM DB2 database server for use with the HCL Digital Experience 9.5 Docker container deployment. One can access the Configuration Wizard from a browser on the HCL Digital Experience 9.5 container deployment via the URL http://{docker host server}:10200/hcl/wizard. Proceed to the Digital Experience database transfer steps topic section Set Up a Stand-alone Server - Database Transfer . Specify the fully qualified DNS name of the database server as set above in the --add-host directive in the docker run command.","title":"Transfer HCL Digital Experience 9.5 container default database to IBM DB2"},{"location":"containerization/cw_containerdbtransfer_ibm_db2/#transfer-hcl-digital-experience-95-container-default-database-to-ibm-db2","text":"HCL Digital Experience 9.5 installs a copy of Derby as the default database. Administrator users can follow these steps to transfer the default database configuration detail to IBM DB2, if preferred for use as the relational database for HCL Digital Experience 9.5 container deployment data. The directions closely follow the Digital Experience database transfer steps provided for deployments to supported \"on-premises\" platforms, such as Windows, Linux, and AIX. The unique steps that account for differences in these instructions for use with an HCL Digital Experience 9.5 Docker container deployment as opposed to an on-premises Digital Experience installation are highlighted. The on-premises based HCL Digital Experience database transfer instructions can be viewed in this topic: DB2 worksheet: Transfer to multiple databases . These container deployment instructions cover the transfer of the default Derby database running on an HCL Digital Experience Container Update CF18 container image to an IBM DB2 Enterprise Version 11.1.0.0 database running on a CentOS Linux release 7.71908 server. The IBM DB2 11.5 Standard and Enterprise database is also supported for these procedures. In these instructions, the IBM DB2 database is installed to a supported on-premises platform (see IBM DB2 System Support requirements for a list of supported platforms ). The HCL Digital Experience 9.5 Docker container is deployed to a supported Docker or Kubernetes platform. The container version deployed in this example is Docker CE version 19.0.3.8. Video : HCL Digital Experience - Perform a Database Transfer on HCL Portal 9.5 Prerequisites The HCL Digital Experience 9.5 container image is deployed, and loaded to your Docker repository using the following command: docker load < hcl-dx-core-image-v95_CF18_20200427-2055.tar.gz See the HCL Digital Experience 9.5 topic Deployment for instructions and latest list of HCL Digital Experience 9.5 Container image and file names. IBM DB2 is installed on an on-premises server with a DNS hostname that is available to the HCL Digital Experience 9.5 deployed container.","title":"Transfer HCL Digital Experience 9.5 container default database to IBM DB2"},{"location":"containerization/cw_containerdbtransfer_ibm_db2/#start-the-hcl-digital-experience-95-container-in-docker","text":"Execute the following command to start the DX 9.5 container: docker run -d --add-host {database DNS name}:{database IP address} -p 10025-10045:10025-10045 -p 10200-10210:10200-10210 -v '{directory on the Docker host for DX container profile}':'/opt/HCL/wp_profile' {your repo name}:{your tag name} Note: An --add-host entry needs to be made to insert the DNS name of the IBM DB2 database server into the /etc/hosts file in the container running the Digital Experience 9.5 container. If, and only if, the DNS name of the DB2 server is already in a named server available to the container, then the --add-host parameter would not be needed. The ports for the Digital Experience 9.5 container (100025-10045) need to be mapped as well as the ports using by the Configuration Wizard. The Configuration Wizard is used to manage the transfer of the database. The Configuration Wizard uses ports in the range of 10200-10202. The HCL Digital Experience 9.5 container persists the profile information across restarts. This is persisted on the Docker host as {directory on docker host for DX container profile} in the run command above. The container will map /opt/HCL/wp_profile to this directory on the docker via the -v command. You need to specify the Docker repository and tag name as the reader has loaded the HCL Digital Experience container image into their docker repository. When this docker run command is executed, ensure time is allocated to check the profile as well as initialize the HCL Digital Experience 9.5 container instance. One can ensure that the HCL Digital Experience 9.5 container deployment is ready by \"tail\"-ing the file in the container located at /opt/HCL/wp_profile/logs/HCL Portal and HCL Web Content Manager/SystemOut.log. When the file displays the following message, the HCL Digital Experience 9.5 container instance is initialized: \u2026open for e- business One can access this file (as well as all others in the running container) via the following command: docker exec -it {container name} bash","title":"Start the HCL Digital Experience 9.5 Container in Docker"},{"location":"containerization/cw_containerdbtransfer_ibm_db2/#transferring-the-database","text":"First, ensure that your HCL Digital Experience 9.5 docker container can access the IBM DB2 on-premises platform server. Using the following command, one can simply \"ping\" the DNS name of the IBM DB2 database server and verify that it answers: docker exec -it {container name} bash If this is not successful, consult with your platform administrator for other methods to debug the network issues between a Docker container and Servers running external to Docker in your environment. Next, once connectivity is established, follow the directions for using the Configuration Wizard from the traditional (on-premises platform-based) Digital Experience database transfer steps to transfer the data from the default Derby database and configure the target IBM DB2 database server for use with the HCL Digital Experience 9.5 Docker container deployment. One can access the Configuration Wizard from a browser on the HCL Digital Experience 9.5 container deployment via the URL http://{docker host server}:10200/hcl/wizard. Proceed to the Digital Experience database transfer steps topic section Set Up a Stand-alone Server - Database Transfer . Specify the fully qualified DNS name of the database server as set above in the --add-host directive in the docker run command.","title":"Transferring the Database"},{"location":"containerization/dam_subscription_staging/","text":"DAM staging This topic contains the commands that administrators can use to configure the staging of Digital Asset Management (DAM) content. This allows you to manage subscriber registration or configure periodic sync. DAM staging framework The DAM staging framework allows you to stage your DAM content from an authoring environment (source environment/publisher) to multiple rendering environments (target environment/subscriber). Using DXClient , you can configure DAM staging to: Trigger a manual staging or use periodic staging processes. Set the cycle length (default: 2 minutes, maximum: 24 hours) for periodic sync. Register a subscriber with a publisher. Note: A subscriber must be registered with a publisher. Access rights from DAM staging assets are not transferred for subscribers that do not share the same Lightweight Directory Access Protocol (LDAP). Manage DAM staging Use the manage-dam-staging trigger-staging command to trigger DAM staging. Command description You can trigger the DAM staging with the following command: dxclient manage-dam-staging trigger-staging Help command This command shows the help information for manage-dam-staging trigger-staging command usage: dxclient manage-dam-staging trigger-staging -h Command options Use this attribute to specify the protocol with which to connect to the DX server (default: \"\") -dxProtocol <value> Use this attribute to specify the host name of the DX server (default: \"\") -hostname <value> Use this attribute to specify the port on which to connect to the DX server (default: \"\"; default port for any Kubernetes environment is 443): -dxPort <value> Use this attribute to specify the user name that is required for authenticating with the DX server (default: \"\") -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the DX Core (default: \"\") -dxPassword <value> Use this attribute to specify the port number of the DAM server (default: \"\"; default port for any Kubernetes environment is 443): -damAPIPort <value> Use this attribute to specify the port number of the DX Core API server (default: \"\"; default port for any Kubernetes environment is 443): -ringAPIPort <value> Use this attribute to specify the API version number of DAM (default: \"\"; default port for any Kubernetes environment is 443): -damAPIVersion <value> Use this attribute to specify the API version number of DX Core (default: \"\"; default port for any Kubernetes environment is 443): -ringAPIVersion <value> Use this attribute to specify the host name of the target environment: -targetHostname <value> Use this attribute to specify the interval between two sync cycles. The unit of interval is in minutes. (default: \"2 minutes\") -interval <value> All these command options are configured in the config.json file of the tool, which is read by default. The configuration file is located at <working-directory>/store/config.json . The options that are passed through the command line override the default values. Example Usage: ``` dxclient manage-dam-staging trigger-staging -dxProtocol -hostname -dxPort -dxUsername -dxPassword -damAPIPort -ringAPIPort -damAPIVersion -ringAPIVersion -targetHostname ``` Registering or deregistering for DAM staging Use the manage-dam-staging **register**-dam-subscriber command to register or the manage-dam-staging **deregister**-dam-subscriber command to deregister the subscriber for DAM staging. Command description You can register a subscriber for DAM staging with the following command: dxclient manage-dam-staging register-dam-subscriber You can deregister a subscriber for DAM staging with the following command: dxclient manage-dam-staging deregister-dam-subscriber Help command The following command shows the help information for manage-dam-staging **register**-dam-subscriber command usage: dxclient manage-dam-staging register-dam-subscriber -h The following command shows the help information for manage-dam-staging **deregister**-dam-subscriber command usage: dxclient manage-dam-staging deregister-dam-subscriber -h Command options Use this attribute to specify the protocol with which to connect to the DX server (default: \"\") -dxProtocol <value> Use this attribute to specify the host name of the DX server (default: \"\") -hostname <value> Use this attribute to specify the port on which to connect to the DX server (default: \"\"; default port for any Kubernetes environment is 443): -dxPort <value> Use this attribute to specify the user name that is required for authenticating with the DX server (default: \"\") -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the DX Core (default: \"\") -dxPassword <value> Use this attribute to specify the port number of the DAM server (default: \"\"; default port for any Kubernetes environment is 443): -damAPIPort <value> Use this attribute to specify the port number of the DX Core API server (default: \"\"; default port for any Kubernetes environment is 443): -ringAPIPort <value> Use this attribute to specify the API version number of DAM (default: \"\"; default port for any Kubernetes environment is 443): -damAPIVersion <value> Use this attribute to specify the API version number of DX Core (default: \"\"; default port for any Kubernetes environment is 443): -ringAPIVersion <value> Use this attribute to specify the host name of the target environment: -targetHostname <value> Use this attribute to specify the subscriber ID of the target environment: -subscriberId <value> Use this attribute to specify the interval between two sync cycles. The unit of interval is in minutes. (default: \"2 minutes\") -interval <value> All these command options are configured in the config.json file of the tool, which is read by default. The configuration file is located at <working-directory>/store/config.json . The options that are passed through the command line override the default values. Example usage: To register: dxclient manage-dam-staging register-dam-subscriber -dxProtocol <dxProtocol> -hostname <hostname> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -damAPIPort <damAPIPort> -ringAPIPort <ringAPIPort> -damAPIVersion <damAPIVersion> -ringAPIVersion <ringAPIVersion> -subscriberId <subscriberId> To deregister: dxclient manage-dam-staging deregister-dam-subscriber -dxProtocol <dxProtocol> -hostname <hostname> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -damAPIPort <damAPIPort> -ringAPIPort <ringAPIPort> -damAPIVersion <damAPIVersion> -ringAPIVersion <ringAPIVersion> -subscriberId <subscriberId> Using WCM with DAM staging The typical setup involves a WCM staging/authoring server connected to DAM staging/authoring, and a separate WCM rendering connected to DAM rendering (there could be multiple WCM rendering/DAM rendering environments, for example, a Blue/Green setup). Syndication is set up for WCM between staging/authoring and WCM rendering. DAM staging is set up between DAM staging/authoring and DAM rendering. (Optional) You can configure WCM WCMConfigService in the WAS Admin Console to allow switching the host name (and port) used for DAM references in WCM using the following: dam.host.overwrite.port=... dam.host.overwrite=... For example: dam.host.overwrite=myserver.com dam.host.overwrite.port=3000 You must restart the DX Core JVM for changes to take effect. Effect : If the properties are in place when using the REST API or WCM Admin UI or WCM API, the returned DAM references have the overwritten host name and port. For example, if a content item is moved from the staging environment to production, and production has the host overwrite set to production.hcl.com , then all DAM references are returned with production.hcl.com . For instance, production.hcl.com/dx/api/dam/v1/collections/390e9808-a6d2-4ebe-b6fb-f10046ebf642/items/fd18083c-d84b-4816-af6e-583059c73122/renditions/7855bfae-d741-41f7-815f-d15f427a4da0?binary=true even if we received the following from syndication: staging.hcl.com/dx/api/dam/v1/collections/390e9808-a6d2-4ebe-b6fb-f10046ebf642/items/fd18083c-d84b-4816-af6e-583059c73122/renditions/7855bfae-d741-41f7-815f-d15f427a4da0?binary=true.","title":"DAM staging"},{"location":"containerization/dam_subscription_staging/#dam-staging","text":"This topic contains the commands that administrators can use to configure the staging of Digital Asset Management (DAM) content. This allows you to manage subscriber registration or configure periodic sync.","title":"DAM staging"},{"location":"containerization/dam_subscription_staging/#dam-staging-framework","text":"The DAM staging framework allows you to stage your DAM content from an authoring environment (source environment/publisher) to multiple rendering environments (target environment/subscriber). Using DXClient , you can configure DAM staging to: Trigger a manual staging or use periodic staging processes. Set the cycle length (default: 2 minutes, maximum: 24 hours) for periodic sync. Register a subscriber with a publisher. Note: A subscriber must be registered with a publisher. Access rights from DAM staging assets are not transferred for subscribers that do not share the same Lightweight Directory Access Protocol (LDAP).","title":"DAM staging framework"},{"location":"containerization/dam_subscription_staging/#manage-dam-staging","text":"Use the manage-dam-staging trigger-staging command to trigger DAM staging. Command description You can trigger the DAM staging with the following command: dxclient manage-dam-staging trigger-staging Help command This command shows the help information for manage-dam-staging trigger-staging command usage: dxclient manage-dam-staging trigger-staging -h Command options Use this attribute to specify the protocol with which to connect to the DX server (default: \"\") -dxProtocol <value> Use this attribute to specify the host name of the DX server (default: \"\") -hostname <value> Use this attribute to specify the port on which to connect to the DX server (default: \"\"; default port for any Kubernetes environment is 443): -dxPort <value> Use this attribute to specify the user name that is required for authenticating with the DX server (default: \"\") -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the DX Core (default: \"\") -dxPassword <value> Use this attribute to specify the port number of the DAM server (default: \"\"; default port for any Kubernetes environment is 443): -damAPIPort <value> Use this attribute to specify the port number of the DX Core API server (default: \"\"; default port for any Kubernetes environment is 443): -ringAPIPort <value> Use this attribute to specify the API version number of DAM (default: \"\"; default port for any Kubernetes environment is 443): -damAPIVersion <value> Use this attribute to specify the API version number of DX Core (default: \"\"; default port for any Kubernetes environment is 443): -ringAPIVersion <value> Use this attribute to specify the host name of the target environment: -targetHostname <value> Use this attribute to specify the interval between two sync cycles. The unit of interval is in minutes. (default: \"2 minutes\") -interval <value> All these command options are configured in the config.json file of the tool, which is read by default. The configuration file is located at <working-directory>/store/config.json . The options that are passed through the command line override the default values. Example Usage: ``` dxclient manage-dam-staging trigger-staging -dxProtocol -hostname -dxPort -dxUsername -dxPassword -damAPIPort -ringAPIPort -damAPIVersion -ringAPIVersion -targetHostname ```","title":"Manage DAM staging"},{"location":"containerization/dam_subscription_staging/#registering-or-deregistering-for-dam-staging","text":"Use the manage-dam-staging **register**-dam-subscriber command to register or the manage-dam-staging **deregister**-dam-subscriber command to deregister the subscriber for DAM staging. Command description You can register a subscriber for DAM staging with the following command: dxclient manage-dam-staging register-dam-subscriber You can deregister a subscriber for DAM staging with the following command: dxclient manage-dam-staging deregister-dam-subscriber Help command The following command shows the help information for manage-dam-staging **register**-dam-subscriber command usage: dxclient manage-dam-staging register-dam-subscriber -h The following command shows the help information for manage-dam-staging **deregister**-dam-subscriber command usage: dxclient manage-dam-staging deregister-dam-subscriber -h Command options Use this attribute to specify the protocol with which to connect to the DX server (default: \"\") -dxProtocol <value> Use this attribute to specify the host name of the DX server (default: \"\") -hostname <value> Use this attribute to specify the port on which to connect to the DX server (default: \"\"; default port for any Kubernetes environment is 443): -dxPort <value> Use this attribute to specify the user name that is required for authenticating with the DX server (default: \"\") -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the DX Core (default: \"\") -dxPassword <value> Use this attribute to specify the port number of the DAM server (default: \"\"; default port for any Kubernetes environment is 443): -damAPIPort <value> Use this attribute to specify the port number of the DX Core API server (default: \"\"; default port for any Kubernetes environment is 443): -ringAPIPort <value> Use this attribute to specify the API version number of DAM (default: \"\"; default port for any Kubernetes environment is 443): -damAPIVersion <value> Use this attribute to specify the API version number of DX Core (default: \"\"; default port for any Kubernetes environment is 443): -ringAPIVersion <value> Use this attribute to specify the host name of the target environment: -targetHostname <value> Use this attribute to specify the subscriber ID of the target environment: -subscriberId <value> Use this attribute to specify the interval between two sync cycles. The unit of interval is in minutes. (default: \"2 minutes\") -interval <value> All these command options are configured in the config.json file of the tool, which is read by default. The configuration file is located at <working-directory>/store/config.json . The options that are passed through the command line override the default values. Example usage: To register: dxclient manage-dam-staging register-dam-subscriber -dxProtocol <dxProtocol> -hostname <hostname> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -damAPIPort <damAPIPort> -ringAPIPort <ringAPIPort> -damAPIVersion <damAPIVersion> -ringAPIVersion <ringAPIVersion> -subscriberId <subscriberId> To deregister: dxclient manage-dam-staging deregister-dam-subscriber -dxProtocol <dxProtocol> -hostname <hostname> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -damAPIPort <damAPIPort> -ringAPIPort <ringAPIPort> -damAPIVersion <damAPIVersion> -ringAPIVersion <ringAPIVersion> -subscriberId <subscriberId>","title":"Registering or deregistering for DAM staging"},{"location":"containerization/dam_subscription_staging/#using-wcm-with-dam-staging","text":"The typical setup involves a WCM staging/authoring server connected to DAM staging/authoring, and a separate WCM rendering connected to DAM rendering (there could be multiple WCM rendering/DAM rendering environments, for example, a Blue/Green setup). Syndication is set up for WCM between staging/authoring and WCM rendering. DAM staging is set up between DAM staging/authoring and DAM rendering. (Optional) You can configure WCM WCMConfigService in the WAS Admin Console to allow switching the host name (and port) used for DAM references in WCM using the following: dam.host.overwrite.port=... dam.host.overwrite=... For example: dam.host.overwrite=myserver.com dam.host.overwrite.port=3000 You must restart the DX Core JVM for changes to take effect. Effect : If the properties are in place when using the REST API or WCM Admin UI or WCM API, the returned DAM references have the overwritten host name and port. For example, if a content item is moved from the staging environment to production, and production has the host overwrite set to production.hcl.com , then all DAM references are returned with production.hcl.com . For instance, production.hcl.com/dx/api/dam/v1/collections/390e9808-a6d2-4ebe-b6fb-f10046ebf642/items/fd18083c-d84b-4816-af6e-583059c73122/renditions/7855bfae-d741-41f7-815f-d15f427a4da0?binary=true even if we received the following from syndication: staging.hcl.com/dx/api/dam/v1/collections/390e9808-a6d2-4ebe-b6fb-f10046ebf642/items/fd18083c-d84b-4816-af6e-583059c73122/renditions/7855bfae-d741-41f7-815f-d15f427a4da0?binary=true.","title":"Using WCM with DAM staging"},{"location":"containerization/damschemas/","text":"DAM schemas This topic contains the commands that administrators can use to get a list of all DAM schemas or delete inactive Digital Asset Management (DAM) schemas from persistence. Listing DAM schemas The list-dam-schemas command is used to list all the DAM schemas. Command description This command invokes list-dam-schemas inside DXClient and provides a list DAM schemas. dxclient list-dam-schemas Help command This command shows the help information for list-dam-schemas command usage: dxclient list-dam-schemas -h Command options Use this attribute to specify the protocol with which to connect to the server -dxProtocol <value> Use this attribute to specify the hostname of the target server -hostname <value> Use this attribute to specify the port on which to connect to the server(for Kubernetes Environment dxPort is 443) -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server -dxPassword <value> Use this attribute to specify the port number of the DAM server(for Kubernetes Environment default port is 443) -damAPIPort <value> Use this attribute to specify the port number of the DX Core API server(for Kubernetes Environment default port is 443) -ringAPIPort <value> Use this attribute to specify the API Version number of DAM(for Kubernetes Environment default port is 443) -damAPIVersion <value> Use this attribute to specify the API Version number of DX Core(for Kubernetes Environment default port is 443) -ringAPIVersion <value> All the above command options can be configured in the config.json file of the tool, which is read by default. The configuration file is located at <working-directory>/store/config.json . The options that are passed through the command line override the default values. Example Usage: dxclient list-dam-schemas -dxProtocol <dxProtocol> -hostname <hostname> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -damAPIPort <damAPIPort> -ringAPIPort <ringAPIPort> -ringAPIVersion <ringAPIVersion> -damAPIVersion <damAPIVersion> Deleting DAM schemas Deleting DAM schema is a recommended step when the configuration of the DAM database schema has been changed, due to a release update such as from Container Update CF196 to Container Update CF197. When a DAM database is migrated, a new schema gets generated and the old schema is rendered inactive. To avoid the accumulation of inactive schemas, you can use the delete-dam-schema command to delete them. Use the delete-dam-schema command to delete the inactive DAM schema. Command description This command invokes delete-dam-schema inside DXClient and deletes the DAM schema. dxclient delete-dam-schema Help command This command shows the help information for delete-dam-schema command usage: dxclient delete-dam-schema -h Command options Use this attribute to specify the protocol that is used to connect to the server -dxProtocol <value> Use this attribute to specify the hostname of the target server -hostname <value> Use this attribute to specify the port on which to connect to the server (for Kubernetes Environment dxPort is 443) -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server -dxPassword <value> Use this attribute to specify the port number of the DAM server (for Kubernetes Environment default port is 443) -damAPIPort <value> Use this attribute to specify the port number of the DX Core API server (for Kubernetes Environment default port is 443) -ringAPIPort <value> Use this attribute to specify the API Version number of DAM (for Kubernetes Environment default port is 443) -damAPIVersion <value> Use this attribute to specify the API Version number of DX Core (for Kubernetes Environment default port is 443) -ringAPIVersion <value> Use this attribute to specify the DAM Schema Version (for Kubernetes Environment default port is 443) -schemaVersion <value> Note: In case the user does not enter the schemaVersion , user is prompted with a list of inactive schemas to choose from. All the above command options can be configured in the config.json file of the tool, which is read by default. The configuration file is located at <working-directory>/store/config.json . The options that are passed through the command line override the default values. Example Usage: dxclient delete-dam-schema -dxProtocol <dxProtocol> -hostname <hostname> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -damAPIPort <damAPIPort> -ringAPIPort <ringAPIPort> -ringAPIVersion <ringAPIVersion> -damAPIVersion <damAPIVersion> -schemaVersion <schemaVersion>","title":"DAM schemas"},{"location":"containerization/damschemas/#dam-schemas","text":"This topic contains the commands that administrators can use to get a list of all DAM schemas or delete inactive Digital Asset Management (DAM) schemas from persistence.","title":"DAM schemas"},{"location":"containerization/damschemas/#listing-dam-schemas","text":"The list-dam-schemas command is used to list all the DAM schemas. Command description This command invokes list-dam-schemas inside DXClient and provides a list DAM schemas. dxclient list-dam-schemas Help command This command shows the help information for list-dam-schemas command usage: dxclient list-dam-schemas -h Command options Use this attribute to specify the protocol with which to connect to the server -dxProtocol <value> Use this attribute to specify the hostname of the target server -hostname <value> Use this attribute to specify the port on which to connect to the server(for Kubernetes Environment dxPort is 443) -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server -dxPassword <value> Use this attribute to specify the port number of the DAM server(for Kubernetes Environment default port is 443) -damAPIPort <value> Use this attribute to specify the port number of the DX Core API server(for Kubernetes Environment default port is 443) -ringAPIPort <value> Use this attribute to specify the API Version number of DAM(for Kubernetes Environment default port is 443) -damAPIVersion <value> Use this attribute to specify the API Version number of DX Core(for Kubernetes Environment default port is 443) -ringAPIVersion <value> All the above command options can be configured in the config.json file of the tool, which is read by default. The configuration file is located at <working-directory>/store/config.json . The options that are passed through the command line override the default values. Example Usage: dxclient list-dam-schemas -dxProtocol <dxProtocol> -hostname <hostname> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -damAPIPort <damAPIPort> -ringAPIPort <ringAPIPort> -ringAPIVersion <ringAPIVersion> -damAPIVersion <damAPIVersion>","title":"Listing DAM schemas"},{"location":"containerization/damschemas/#deleting-dam-schemas","text":"Deleting DAM schema is a recommended step when the configuration of the DAM database schema has been changed, due to a release update such as from Container Update CF196 to Container Update CF197. When a DAM database is migrated, a new schema gets generated and the old schema is rendered inactive. To avoid the accumulation of inactive schemas, you can use the delete-dam-schema command to delete them. Use the delete-dam-schema command to delete the inactive DAM schema. Command description This command invokes delete-dam-schema inside DXClient and deletes the DAM schema. dxclient delete-dam-schema Help command This command shows the help information for delete-dam-schema command usage: dxclient delete-dam-schema -h Command options Use this attribute to specify the protocol that is used to connect to the server -dxProtocol <value> Use this attribute to specify the hostname of the target server -hostname <value> Use this attribute to specify the port on which to connect to the server (for Kubernetes Environment dxPort is 443) -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server -dxPassword <value> Use this attribute to specify the port number of the DAM server (for Kubernetes Environment default port is 443) -damAPIPort <value> Use this attribute to specify the port number of the DX Core API server (for Kubernetes Environment default port is 443) -ringAPIPort <value> Use this attribute to specify the API Version number of DAM (for Kubernetes Environment default port is 443) -damAPIVersion <value> Use this attribute to specify the API Version number of DX Core (for Kubernetes Environment default port is 443) -ringAPIVersion <value> Use this attribute to specify the DAM Schema Version (for Kubernetes Environment default port is 443) -schemaVersion <value> Note: In case the user does not enter the schemaVersion , user is prompted with a list of inactive schemas to choose from. All the above command options can be configured in the config.json file of the tool, which is read by default. The configuration file is located at <working-directory>/store/config.json . The options that are passed through the command line override the default values. Example Usage: dxclient delete-dam-schema -dxProtocol <dxProtocol> -hostname <hostname> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -damAPIPort <damAPIPort> -ringAPIPort <ringAPIPort> -ringAPIVersion <ringAPIVersion> -damAPIVersion <damAPIVersion> -schemaVersion <schemaVersion>","title":"Deleting DAM schemas"},{"location":"containerization/deploy_container_platforms/","text":"Operator-based deployment This section outlines the supported container platforms for HCL Digital Experience 9.5, and instructions on how to deploy to supported container environments. Attention: Operator-based deployment is deprecated starting with HCL Digital Experience 9.5 CF196. Notes: Initial login credentials for the DX Docker image are: wpsadmin/wpsadmin . Prior to deploying on Red Hat OpenShift or Kubernetes, it is recommended that administrators read the Limitations/Requirements section. Additional guidance about storage class and volume is available for HCL Digital Experience 9.5 container administrators. See the topic Sample Storage Class and Volume for HCL Digital Experience 9.5 Container in Amazon EKS or Red Hat OpenShift . Video : Getting started with HCL Portal 9.5 on Docker . Getting started with HCL DX 9.5 on container platforms . The following container platforms are supported. Docker image list and Docker image deployment Red Hat OpenShift Amazon Elastic Kubernetes Service (EKS) Microsoft Azure Kubernetes Service (AKS) Google Kubernetes Engine (GKE)","title":"Operator-based deployment"},{"location":"containerization/deploy_container_platforms/#operator-based-deployment","text":"This section outlines the supported container platforms for HCL Digital Experience 9.5, and instructions on how to deploy to supported container environments. Attention: Operator-based deployment is deprecated starting with HCL Digital Experience 9.5 CF196. Notes: Initial login credentials for the DX Docker image are: wpsadmin/wpsadmin . Prior to deploying on Red Hat OpenShift or Kubernetes, it is recommended that administrators read the Limitations/Requirements section. Additional guidance about storage class and volume is available for HCL Digital Experience 9.5 container administrators. See the topic Sample Storage Class and Volume for HCL Digital Experience 9.5 Container in Amazon EKS or Red Hat OpenShift . Video : Getting started with HCL Portal 9.5 on Docker . Getting started with HCL DX 9.5 on container platforms . The following container platforms are supported. Docker image list and Docker image deployment Red Hat OpenShift Amazon Elastic Kubernetes Service (EKS) Microsoft Azure Kubernetes Service (AKS) Google Kubernetes Engine (GKE)","title":"Operator-based deployment"},{"location":"containerization/deploy_dx_components_using_hcl_dx_client_and_dx_connect/","text":"Deploy DX components using HCL DXClient and DXConnect HCL Digital Experience (DX) 9.5 CF19 and later releases include a DXClient toolset, and DXConnect servlet that provides developers and administrators with an approach to deploy changes or improvements to the DX platform, and partially automate the development and delivery process.","title":"Deploy DX components using HCL DXClient and DXConnect"},{"location":"containerization/deploy_dx_components_using_hcl_dx_client_and_dx_connect/#deploy-dx-components-using-hcl-dxclient-and-dxconnect","text":"HCL Digital Experience (DX) 9.5 CF19 and later releases include a DXClient toolset, and DXConnect servlet that provides developers and administrators with an approach to deploy changes or improvements to the DX platform, and partially automate the development and delivery process.","title":"Deploy DX components using HCL DXClient and DXConnect"},{"location":"containerization/deployment/","text":"Digital Experience on containerized platforms Learn how to deploy HCL Digital Experience as a cloud-native platform and optimize business-critical digital experiences for your customers. The Kubernetes container orchestration platform allows orchestration features for the automated deployment, coordination, scaling, and management of containerized applications. Originally designed by Google, now governed by the Cloud Native Computing Foundation (CNCF), and developed by Google, Red Hat, and many others, Kubernetes is now widely used by organizations of various sizes to run containers in a cloud environment.","title":"Digital Experience on containerized platforms"},{"location":"containerization/deployment/#digital-experience-on-containerized-platforms","text":"Learn how to deploy HCL Digital Experience as a cloud-native platform and optimize business-critical digital experiences for your customers. The Kubernetes container orchestration platform allows orchestration features for the automated deployment, coordination, scaling, and management of containerized applications. Originally designed by Google, now governed by the Cloud Native Computing Foundation (CNCF), and developed by Google, Red Hat, and many others, Kubernetes is now widely used by organizations of various sizes to run containers in a cloud environment.","title":"Digital Experience on containerized platforms"},{"location":"containerization/docker/","text":"Docker image list This section presents the latest HCL DX 9.5 Docker container update images available. Docker container update file list The HCL DX 9.5 container update releases may be obtained from your HCL Digital Experience entitlements in the HCL Software License Portal , in the HCL_Portal_8.5_9.0_9.5_CFs download package entry. Video: How to upload HCL Digital Experience 9.5 CF container images to a private repository HCL DX 9.5 CF199 Container Update CF199 If deploying the HCL DX 9.5 CF199 release, the package name and images are as follows: HCL DX 9.5 CF199 DXClient files : ``` hcl-dxclient-image-v95_CF199_20211029-1357.zip - ``` hcl-dxclient-v95_CF199_20211029-1357.zip **HCL DX 9.5 CF\\_199-hcl-dx-kubernetes-v95-CF199.zip** - ``` HCL DX notices V9.5 CF199.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF199_20211029-1700.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF199_20211029-1700.zip - ``` hcl-dx-content-composer-image-v1.12.0_20211029-1341.tar.gz - ``` hcl-dx-core-image-v95_CF199_20211029-1348.tar.gz - ``` hcl-dx-deployment-v2.1.0_20211029-1346.tgz - ``` hcl-dx-design-studio-image-v0.5.0_20211029-0013.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF199_20211029-1342.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.11.0_20211029-1350.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20211029-1354.zip - ``` hcl-dx-image-processor-image-v1.12.0_20211029-1346.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20211029-1342.tar.gz - ``` hcl-dx-persistence-connection-pool-image-v1.11.0_20211029-0224.tar.gz - ``` hcl-dx-persistence-image-v1.11.0_20211029-1349.tar.gz - ``` hcl-dx-persistence-metrics-exporter-image-v1.10.0_20211029-1352.tar.gz - ``` hcl-dx-persistence-node-image-v1.1_20211029-0148.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF199_20211029-1348.tar.gz - ``` hcl-dx-ringapi-image-v1.12.0_20211029-1357.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF199_20211029-1344.tar.gz - ``` hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz HCL DX 9.5 CF198 CF198 If deploying HCL DX 9.5 CF198 release, the package name and images are as follows: HCL DX 9.5 CF198 DXClient files : ``` hcl-dxclient-image-v95_CF198_20210917-1455.zip - ``` hcl-dxclient-v95_CF198_20210917-1455.zip **HCL DX 9.5 CF198 - hcl-dx-kubernetes-v95-CF198.zip**: - ``` HCL DX notices V9.5 CF198.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF198_20210917-1749.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF198_20210917-1749.zip - ``` hcl-dx-content-composer-image-v1.10.0_20210917-1437.tar.gz - ``` hcl-dx-core-image-v95_CF198_20210917-1438.tar.gz - ``` hcl-dx-deployment-v2.0.0_20210917-1441.tgz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF198_20210917-1437.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.10.0_20210917-1444.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210917-1439.zip - ``` hcl-dx-image-processor-image-v1.11.0_20210917-1449.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20210917-1437.tar.gz - ``` hcl-dx-persistence-postgres-repmgr-image-v1.1_20210916-0148.tar.gz - ``` hcl-dx-postgres-image-v1.11.0_20210917-1444.tar.gz - ``` hcl-dx-postgres-persistence-pgpool-image-v1.11.0_20210916-0224.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF198_20210917-1438.tar.gz - ``` hcl-dx-ringapi-image-v1.11.0_20210917-1441.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF198_20210917-1441.tar.gz - ``` hcl-dx-site-manager-image-v0.4.0_20210917-1445.tar.gz HCL DX 9.5 CF197 CF197 If deploying HCL DX 9.5 CF197 release, the package name and images are as follows: HCL DX 9.5 CF197 Client - hcl-dx-client-v95-CF197.zip : ``` hcl-dxclient-image-v95_CF197_20210806-1311.zip - ``` hcl-dxclient-v95_CF197_20210806-1311.zip **HCL DX 9.5 CF197 - hcl-dx-kubernetes-v95-CF197.zip**: - ``` HCL DX notices V9.5 CF197.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF197_20210806-1310.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF197_20210806-1310.zip - ``` hcl-dx-content-composer-image-v1.10.0_20210806-1258.tar.gz - ``` hcl-dx-core-image-v95_CF197_20210806-1259.tar.gz - ``` hcl-dx-deployment-v2.0.0_20210806-1300.tgz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF197_20210806-1258.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.10.0_20210806-1302.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210806-1308.zip - ``` hcl-dx-image-processor-image-v1.10.0_20210806-1300.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20210806-1258.tar.gz - ``` hcl-dx-postgres-image-v1.10.0_20210806-1302.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF197_20210806-1259.tar.gz - ``` hcl-dx-ringapi-image-v1.10.0_20210806-1311.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF197_20210806-1258.tar.gz - ``` hcl-dx-site-manager-image-v0.3.0_20210806-1308.tar.gz HCL DX 9.5 CF196 CF196 If deploying HCL DX 9.5 CF196 release, the package name and images are as follows: HCL DX 9.5 CF196 Client - hcl-dx-client-v95-CF196.zip : ``` hcl-dxclient-image-v95_CF196_20210625-2028.zip - ``` hcl-dxclient-v95_CF196_20210625-2029.zip **HCL DX 9.5 CF196 - hcl-dx-kubernetes-v95-CF196.zip**: - ``` HCL DX notices V9.5 CF196.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF196_20210625-2033.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF196_20210625-2033.zip - ``` hcl-dx-content-composer-image-v1.9.0_20210625-2012.tar.gz - ``` hcl-dx-core-image-v95_CF196_20210625-2011.tar.gz - ``` hcl-dx-deployment-v1.0.0_20210625-2026.tgz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF196_20210625-2013.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.9.0_20210625-2016.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210625-2023.zip - ``` hcl-dx-image-processor-image-v1.8.0_20210625-2015.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20210625-2013.tar.gz - ``` hcl-dx-postgres-image-v1.9.0_20210625-2016.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF196_20210625-2011.tar.gz - ``` hcl-dx-ringapi-image-v1.9.0_20210625-2026.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF196_20210625-2013.tar.gz - ``` hcl-dx-site-manager-image-v0.2.0_20210625-2023.tar.gz HCL DX 9.5 CF195 CF195 If deploying HCL DX 9.5 CF195 release, the package name and images are as follows: HCL DX 9.5 CF195 Client - hcl-dx-client-v95-CF195.zip : ``` dxclient_v1.4.0_20210514-1713.zip **HCL DX 9.5 CF195 - hcl-dx-kubernetes-v95-CF195.zip**: - ``` HCL DX notices V9.5 CF195.txt - ``` dxclient_v1.4.0_20210514-1713.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF195_20210515-0201.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF195_20210514-1707.zip - ``` hcl-dx-content-composer-image-v1.8.0_20210514-1707.tar.gz - ``` hcl-dx-core-image-v95_CF195_20210514-1708.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF195_20210514-1714.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.8.0_20210514-1711.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210514-1708.zip - ``` hcl-dx-image-processor-image-v1.8.0_20210514-1712.tar.gz - ``` hcl-dx-openldap-image-v1.1.0-master_20210514_1621013302.tar.gz - ``` hcl-dx-postgres-image-v1.8.0_20210514-1708.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF195_20210514-1708.tar.gz - ``` hcl-dx-ringapi-image-v1.8.0_20210514-1709.tar.gz ## HCL DX 9.5 CF194 - **CF194** Important note: Please consult the HCL DX Support Knowledge Base article, [Manual Steps to Apply the Digital Experience 9.5 Container Deployment Core Certificate Update](https://support.hcltechsw.com/csm?id=kb_article&sysparm_article=KB0089699), to apply the certificate update to your HCL DX 9.5 container deployment. If deploying HCL DX 9.5 CF194 release, the package name and images are as follows: **HCL DX 9.5 CF194 Client - hcl-dx-client-v95-CF194.zip**: - ``` dxclient_v1.3.0_20210415-2128.zip **HCL DX 9.5 CF194 - hcl-dx-kubernetes-v95-CF194.zip**: - ``` HCL DX notices V9.5 CF194.txt - ``` dxclient_v1.3.0_20210415-2128.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF194_20210416-0233.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF194_20210416-0233.zip - ``` hcl-dx-content-composer-image-v1.7.0_20210415-2121.tar.gz - ``` hcl-dx-core-image-v95_CF194_20210415-2120.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF194_20210415-2127.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210415-2121.zip - ``` hcl-dx-image-processor-image-v1.7.0_20210415-2120.tar.gz - ``` hcl-dx-openldap-image-v1.1.0-master_20210416_1618540820.tar.gz - ``` hcl-dx-postgres-image-v1.7.0_20210415-2120.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF194_20210415-2120.tar.gz HCL DX 9.5 CF193 CF193 If deploying HCL DX 9.5 CF193 release, the package name and images are as follows: HCL DX 9.5 CF193 Client - hcl-dx-client-v95-CF193.zip : ``` dxclient_v1.3.0_20210331-1335.zip **HCL DX 9.5 CF193 - hcl-dx-kubernetes-v95-CF193.zip**: - ``` HCL DX notices V9.5 CF193.txt - ``` dxclient_v1.3.0_20210331-1335.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF193_20210331-1847.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF193_20210331-1847.zip - ``` hcl-dx-content-composer-image-v1.7.0_20210331-1333.tar.gz - ``` hcl-dx-core-image-v95_CF193_20210331-1336.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF193_20210331-1335.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.7.0_20210331-1339.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210331-1343.zip - ``` hcl-dx-image-processor-image-v1.7.0_20210331-1336.tar.gz - ``` hcl-dx-openldap-image-v1.1.0-master_20210331_1617216873.tar.gz - ``` hcl-dx-postgres-image-v1.7.0_20210331-1337.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF193_20210331-1336.tar.gz - ``` hcl-dx-ringapi-image-v1.7.0_20210331-1339.tar.gz ## HCL DX 9.5 CF192 - **CF192** If deploying HCL DX 9.5 CF192 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF192.zip**: - ``` HCL DX notices V9.5 CF192.txt - ``` dxclient_v1.2.0_20210305-1758.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF192_20210305-2309.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF192_20210305-2309.zip - ``` hcl-dx-content-composer-image-v1.6.0_20210305-1756.tar.gz - ``` hcl-dx-core-image-v95_CF192_20210305-1758.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF192_20210305-1757.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.6.0_20210305-1802.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210305-1805.zip - ``` hcl-dx-image-processor-image-v1.6.0_20210305-1758.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20210305_1614986151.tar.gz - ``` hcl-dx-postgres-image-v1.6.0_20210305-1800.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF192_20210305-1758.tar.gz - ``` hcl-dx-ringapi-image-v1.6.0_20210305-1802.tar.gz ## HCL DX 9.5 CF191 - **CF191** If deploying HCL DX 9.5 CF191 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF191.zip** file: - ``` HCL DX notices V9.5 CF191.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF191_20201214-1527.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF191_20201214-1527.zip - ``` hcl-dx-content-composer-image-v1.5.0_20201211-2151.tar.gz - ``` hcl-dx-core-image-v95_CF191_20201212-1421.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF191_20201211-2152.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.5.0_20201211-2155.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20201211-2205.zip - ``` hcl-dx-image-processor-image-v1.5.0_20201211-2154.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20201212_1607741365.tar.gz - ``` hcl-dx-postgres-image-v1.5.0_20201211-2155.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF191_20201212-1421.tar.gz - ``` hcl-dx-ringapi-image-v1.5.0_20201211-2200.tar.gz - ``` dxclient_v1.1.0_20201211-2153.zip **Note:** HCL DX 9.5 CF191 images are available and may be installed. HCL DX 9.5 CF191 is supported for new deployments only. DX administrators should not upgrade DX 9.5 container deployments to this release. ## HCL DX 9.5 CF19 - **CF19** If deploying HCL DX 9.5 CF19 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF19.zip file**: - ``` HCL DX notices V9.5 CF19.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF19_20201110-0401.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF19_20201110-0401.zip - ``` hcl-dx-content-composer-image-v1.4.0_20201109-2203.tar.gz - ``` hcl-dx-core-image-v95_CF19_20201109-2312.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF19_20201109-2204.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.4.0_20201109-2206.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20201109-2208.zip - ``` hcl-dx-image-processor-image-v1.4.0_20201109-2204.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20201110_1604981292.tar.gz - ``` hcl-dx-postgres-image-v1.4.0_20201109-2206.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF19_20201109-2312.tar.gz - ``` hcl-dx-ringapi-image-v1.4.0_20201109-2209.tar.gz - ``` dxclient_v1.0.0_20201110-2010.zip ## HCL DX 9.5 CF184 - **HCL DX 9.5 Container Update CF184** If deploying HCL DX 9.5 CF184 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF184.zip files**: - ``` HCL DX notices V9.5 CF184.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF184_20200917-0054.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF184_20200917-0054.zip - ``` hcl-dx-content-composer-image-v1.3.0_20200916-1952.tar.gz - ``` hcl-dx-core-image-v95_CF184_20200916-2009.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF184_20200916-2034.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.3.0_20200916-1953.tar.gz - ``` hcl-dx-image-processor-image-v1.3.0_20200916-1953.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20200917_1600304449.tar.gz - ``` hcl-dx-postgres-image-v1.3.0_20200916-2003.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF184_20200916-2009.tar.gz - ``` hcl-dx-ringapi-image-v1.3.0_20200916-1953.tar.gz ## HCL DX 9.5 CF183 - **HCL DX 9.5 Container Update CF183** If deploying HCL DX 9.5 CF183 release, the image and package names are as follows: **CF183-core.zip files**: - ``` HCL DX notices V9.5 CF183.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF183_20200818-1852.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF183_20200818-1852.zip - ``` hcl-dx-core-image-v95_CF183_20200818-1342.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz **CF183-other.zip files**: - ``` HCL DX notices V9.5 CF183.txt - ``` hcl-dx-content-composer-image-v1.2.0_20200818-1343.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF183_20200818-1344.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.2.0_20200818-1346.tar.gz - ``` hcl-dx-image-processor-image-v1.2.0_20200818-1345.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20200818_1597758965.tar.gz - ``` hcl-dx-postgres-image-v1.2.0_20200818-1349.tar.gz - ``` hcl-dx-remote-search-image-v95_CF183_20200818-1342.tar.gz - ``` hcl-dx-ringapi-image-v1.2.0_20200818-1351.tar.gz HCL DX 9.5 CF182 CF182 If deploying HCL DX 9.5 CF182 release, the image and package names are as follows: hcl-dx-kubernetes-v95-CF182-core.zip : ``` HCL DX notices V9.5 CF182.txt - ``` hcl-dx-ambassador-image-0850.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF182_20200720-1708.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF182_20200720-1708.zip - ``` hcl-dx-core-image-v95_CF182_20200720-1645.tar.gz **hcl-dx-kubernetes-v95-CF182-other.zip**: - ``` HCL DX notices V9.5 CF182.txt - ``` hcl-dx-content-composer-image-v1.1.0_20200720-1708.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF182_20200720-1716.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.1.0_20200720-1712.tar.gz - ``` hcl-dx-image-processor-image-v1.1.0_20200720-1716.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20200720_1595265588.tar.gz - ``` hcl-dx-postgres-image-v1.1.0_20200720-1715.tar.gz - ``` hcl-dx-remote-search-image-v95_CF182_20200720-1645.tar.gz - ``` hcl-dx-ringapi-image-v1.1.0_20200720-1707.tar.gz HCL DX 9.5 CF181 CF181 If deploying HCL DX 9.5 CF181 and later container update release, the image and package names add HCL DX 9.5 Content Composer and Digital Asset Management components and supporting services images, as well as Remote Search and OpenLDAP images. See the following list of images presented within the HCL DX 9.5 Container Update CF181 package: hcl-dx-kubernetes-v95-CF181-core.zip : ``` hcl-dx-cloud-scripts-v95_CF181_xxxxxxxx-xxxx.zip - ``` hcl-dx-core-image-v95_CF181_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-ambassador-image-xxxx.tar.gz - ``` HCL DX notices V9.5 CF181.txt **hcl-dx-kubernetes-v95-CF181-other.zip**: - ``` hcl-dx-ringapi-image-v1.0.0_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-postgres-image-v1.0.0_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-release_xxxxxxxx_xxxxxxxxxx.tar.gz - ``` hcl-dx-medialibrary-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-image-processor-image-v1.0.0_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-dxrs-image-v95_CF181_xxxxxxxx-xxxx.tar.gz ## HCL DX 9.5 CF18 - **CF18** If deploying HCL DX 9.5 CF18 and later container update release, the image and package names included for this are: - ``` hcl-dx-kubernetes-v95-CF18.zip - ``` hcl-dx-core-image-v95_CF18_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF18_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF18_xxxxxxxx-xxxx.zip - ``` hcl-dx-ambassador-image-xxxx.tar.gz See the following sections for additional information: Docker image deployment Customizing the container deployment Requirements and limitations","title":"Docker image list"},{"location":"containerization/docker/#docker-image-list","text":"This section presents the latest HCL DX 9.5 Docker container update images available.","title":"Docker image list"},{"location":"containerization/docker/#docker-container-update-file-list","text":"The HCL DX 9.5 container update releases may be obtained from your HCL Digital Experience entitlements in the HCL Software License Portal , in the HCL_Portal_8.5_9.0_9.5_CFs download package entry. Video: How to upload HCL Digital Experience 9.5 CF container images to a private repository","title":"Docker container update file list"},{"location":"containerization/docker/#hcl-dx-95-cf199","text":"Container Update CF199 If deploying the HCL DX 9.5 CF199 release, the package name and images are as follows: HCL DX 9.5 CF199 DXClient files : ``` hcl-dxclient-image-v95_CF199_20211029-1357.zip - ``` hcl-dxclient-v95_CF199_20211029-1357.zip **HCL DX 9.5 CF\\_199-hcl-dx-kubernetes-v95-CF199.zip** - ``` HCL DX notices V9.5 CF199.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF199_20211029-1700.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF199_20211029-1700.zip - ``` hcl-dx-content-composer-image-v1.12.0_20211029-1341.tar.gz - ``` hcl-dx-core-image-v95_CF199_20211029-1348.tar.gz - ``` hcl-dx-deployment-v2.1.0_20211029-1346.tgz - ``` hcl-dx-design-studio-image-v0.5.0_20211029-0013.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF199_20211029-1342.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.11.0_20211029-1350.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20211029-1354.zip - ``` hcl-dx-image-processor-image-v1.12.0_20211029-1346.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20211029-1342.tar.gz - ``` hcl-dx-persistence-connection-pool-image-v1.11.0_20211029-0224.tar.gz - ``` hcl-dx-persistence-image-v1.11.0_20211029-1349.tar.gz - ``` hcl-dx-persistence-metrics-exporter-image-v1.10.0_20211029-1352.tar.gz - ``` hcl-dx-persistence-node-image-v1.1_20211029-0148.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF199_20211029-1348.tar.gz - ``` hcl-dx-ringapi-image-v1.12.0_20211029-1357.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF199_20211029-1344.tar.gz - ``` hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz","title":"HCL DX 9.5 CF199"},{"location":"containerization/docker/#hcl-dx-95-cf198","text":"CF198 If deploying HCL DX 9.5 CF198 release, the package name and images are as follows: HCL DX 9.5 CF198 DXClient files : ``` hcl-dxclient-image-v95_CF198_20210917-1455.zip - ``` hcl-dxclient-v95_CF198_20210917-1455.zip **HCL DX 9.5 CF198 - hcl-dx-kubernetes-v95-CF198.zip**: - ``` HCL DX notices V9.5 CF198.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF198_20210917-1749.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF198_20210917-1749.zip - ``` hcl-dx-content-composer-image-v1.10.0_20210917-1437.tar.gz - ``` hcl-dx-core-image-v95_CF198_20210917-1438.tar.gz - ``` hcl-dx-deployment-v2.0.0_20210917-1441.tgz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF198_20210917-1437.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.10.0_20210917-1444.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210917-1439.zip - ``` hcl-dx-image-processor-image-v1.11.0_20210917-1449.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20210917-1437.tar.gz - ``` hcl-dx-persistence-postgres-repmgr-image-v1.1_20210916-0148.tar.gz - ``` hcl-dx-postgres-image-v1.11.0_20210917-1444.tar.gz - ``` hcl-dx-postgres-persistence-pgpool-image-v1.11.0_20210916-0224.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF198_20210917-1438.tar.gz - ``` hcl-dx-ringapi-image-v1.11.0_20210917-1441.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF198_20210917-1441.tar.gz - ``` hcl-dx-site-manager-image-v0.4.0_20210917-1445.tar.gz","title":"HCL DX 9.5 CF198"},{"location":"containerization/docker/#hcl-dx-95-cf197","text":"CF197 If deploying HCL DX 9.5 CF197 release, the package name and images are as follows: HCL DX 9.5 CF197 Client - hcl-dx-client-v95-CF197.zip : ``` hcl-dxclient-image-v95_CF197_20210806-1311.zip - ``` hcl-dxclient-v95_CF197_20210806-1311.zip **HCL DX 9.5 CF197 - hcl-dx-kubernetes-v95-CF197.zip**: - ``` HCL DX notices V9.5 CF197.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF197_20210806-1310.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF197_20210806-1310.zip - ``` hcl-dx-content-composer-image-v1.10.0_20210806-1258.tar.gz - ``` hcl-dx-core-image-v95_CF197_20210806-1259.tar.gz - ``` hcl-dx-deployment-v2.0.0_20210806-1300.tgz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF197_20210806-1258.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.10.0_20210806-1302.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210806-1308.zip - ``` hcl-dx-image-processor-image-v1.10.0_20210806-1300.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20210806-1258.tar.gz - ``` hcl-dx-postgres-image-v1.10.0_20210806-1302.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF197_20210806-1259.tar.gz - ``` hcl-dx-ringapi-image-v1.10.0_20210806-1311.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF197_20210806-1258.tar.gz - ``` hcl-dx-site-manager-image-v0.3.0_20210806-1308.tar.gz","title":"HCL DX 9.5 CF197"},{"location":"containerization/docker/#hcl-dx-95-cf196","text":"CF196 If deploying HCL DX 9.5 CF196 release, the package name and images are as follows: HCL DX 9.5 CF196 Client - hcl-dx-client-v95-CF196.zip : ``` hcl-dxclient-image-v95_CF196_20210625-2028.zip - ``` hcl-dxclient-v95_CF196_20210625-2029.zip **HCL DX 9.5 CF196 - hcl-dx-kubernetes-v95-CF196.zip**: - ``` HCL DX notices V9.5 CF196.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF196_20210625-2033.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF196_20210625-2033.zip - ``` hcl-dx-content-composer-image-v1.9.0_20210625-2012.tar.gz - ``` hcl-dx-core-image-v95_CF196_20210625-2011.tar.gz - ``` hcl-dx-deployment-v1.0.0_20210625-2026.tgz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF196_20210625-2013.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.9.0_20210625-2016.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210625-2023.zip - ``` hcl-dx-image-processor-image-v1.8.0_20210625-2015.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20210625-2013.tar.gz - ``` hcl-dx-postgres-image-v1.9.0_20210625-2016.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF196_20210625-2011.tar.gz - ``` hcl-dx-ringapi-image-v1.9.0_20210625-2026.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF196_20210625-2013.tar.gz - ``` hcl-dx-site-manager-image-v0.2.0_20210625-2023.tar.gz","title":"HCL DX 9.5 CF196"},{"location":"containerization/docker/#hcl-dx-95-cf195","text":"CF195 If deploying HCL DX 9.5 CF195 release, the package name and images are as follows: HCL DX 9.5 CF195 Client - hcl-dx-client-v95-CF195.zip : ``` dxclient_v1.4.0_20210514-1713.zip **HCL DX 9.5 CF195 - hcl-dx-kubernetes-v95-CF195.zip**: - ``` HCL DX notices V9.5 CF195.txt - ``` dxclient_v1.4.0_20210514-1713.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF195_20210515-0201.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF195_20210514-1707.zip - ``` hcl-dx-content-composer-image-v1.8.0_20210514-1707.tar.gz - ``` hcl-dx-core-image-v95_CF195_20210514-1708.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF195_20210514-1714.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.8.0_20210514-1711.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210514-1708.zip - ``` hcl-dx-image-processor-image-v1.8.0_20210514-1712.tar.gz - ``` hcl-dx-openldap-image-v1.1.0-master_20210514_1621013302.tar.gz - ``` hcl-dx-postgres-image-v1.8.0_20210514-1708.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF195_20210514-1708.tar.gz - ``` hcl-dx-ringapi-image-v1.8.0_20210514-1709.tar.gz ## HCL DX 9.5 CF194 - **CF194** Important note: Please consult the HCL DX Support Knowledge Base article, [Manual Steps to Apply the Digital Experience 9.5 Container Deployment Core Certificate Update](https://support.hcltechsw.com/csm?id=kb_article&sysparm_article=KB0089699), to apply the certificate update to your HCL DX 9.5 container deployment. If deploying HCL DX 9.5 CF194 release, the package name and images are as follows: **HCL DX 9.5 CF194 Client - hcl-dx-client-v95-CF194.zip**: - ``` dxclient_v1.3.0_20210415-2128.zip **HCL DX 9.5 CF194 - hcl-dx-kubernetes-v95-CF194.zip**: - ``` HCL DX notices V9.5 CF194.txt - ``` dxclient_v1.3.0_20210415-2128.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF194_20210416-0233.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF194_20210416-0233.zip - ``` hcl-dx-content-composer-image-v1.7.0_20210415-2121.tar.gz - ``` hcl-dx-core-image-v95_CF194_20210415-2120.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF194_20210415-2127.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210415-2121.zip - ``` hcl-dx-image-processor-image-v1.7.0_20210415-2120.tar.gz - ``` hcl-dx-openldap-image-v1.1.0-master_20210416_1618540820.tar.gz - ``` hcl-dx-postgres-image-v1.7.0_20210415-2120.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF194_20210415-2120.tar.gz","title":"HCL DX 9.5 CF195"},{"location":"containerization/docker/#hcl-dx-95-cf193","text":"CF193 If deploying HCL DX 9.5 CF193 release, the package name and images are as follows: HCL DX 9.5 CF193 Client - hcl-dx-client-v95-CF193.zip : ``` dxclient_v1.3.0_20210331-1335.zip **HCL DX 9.5 CF193 - hcl-dx-kubernetes-v95-CF193.zip**: - ``` HCL DX notices V9.5 CF193.txt - ``` dxclient_v1.3.0_20210331-1335.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF193_20210331-1847.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF193_20210331-1847.zip - ``` hcl-dx-content-composer-image-v1.7.0_20210331-1333.tar.gz - ``` hcl-dx-core-image-v95_CF193_20210331-1336.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF193_20210331-1335.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.7.0_20210331-1339.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210331-1343.zip - ``` hcl-dx-image-processor-image-v1.7.0_20210331-1336.tar.gz - ``` hcl-dx-openldap-image-v1.1.0-master_20210331_1617216873.tar.gz - ``` hcl-dx-postgres-image-v1.7.0_20210331-1337.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF193_20210331-1336.tar.gz - ``` hcl-dx-ringapi-image-v1.7.0_20210331-1339.tar.gz ## HCL DX 9.5 CF192 - **CF192** If deploying HCL DX 9.5 CF192 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF192.zip**: - ``` HCL DX notices V9.5 CF192.txt - ``` dxclient_v1.2.0_20210305-1758.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF192_20210305-2309.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF192_20210305-2309.zip - ``` hcl-dx-content-composer-image-v1.6.0_20210305-1756.tar.gz - ``` hcl-dx-core-image-v95_CF192_20210305-1758.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF192_20210305-1757.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.6.0_20210305-1802.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210305-1805.zip - ``` hcl-dx-image-processor-image-v1.6.0_20210305-1758.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20210305_1614986151.tar.gz - ``` hcl-dx-postgres-image-v1.6.0_20210305-1800.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF192_20210305-1758.tar.gz - ``` hcl-dx-ringapi-image-v1.6.0_20210305-1802.tar.gz ## HCL DX 9.5 CF191 - **CF191** If deploying HCL DX 9.5 CF191 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF191.zip** file: - ``` HCL DX notices V9.5 CF191.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF191_20201214-1527.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF191_20201214-1527.zip - ``` hcl-dx-content-composer-image-v1.5.0_20201211-2151.tar.gz - ``` hcl-dx-core-image-v95_CF191_20201212-1421.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF191_20201211-2152.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.5.0_20201211-2155.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20201211-2205.zip - ``` hcl-dx-image-processor-image-v1.5.0_20201211-2154.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20201212_1607741365.tar.gz - ``` hcl-dx-postgres-image-v1.5.0_20201211-2155.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF191_20201212-1421.tar.gz - ``` hcl-dx-ringapi-image-v1.5.0_20201211-2200.tar.gz - ``` dxclient_v1.1.0_20201211-2153.zip **Note:** HCL DX 9.5 CF191 images are available and may be installed. HCL DX 9.5 CF191 is supported for new deployments only. DX administrators should not upgrade DX 9.5 container deployments to this release. ## HCL DX 9.5 CF19 - **CF19** If deploying HCL DX 9.5 CF19 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF19.zip file**: - ``` HCL DX notices V9.5 CF19.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF19_20201110-0401.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF19_20201110-0401.zip - ``` hcl-dx-content-composer-image-v1.4.0_20201109-2203.tar.gz - ``` hcl-dx-core-image-v95_CF19_20201109-2312.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF19_20201109-2204.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.4.0_20201109-2206.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20201109-2208.zip - ``` hcl-dx-image-processor-image-v1.4.0_20201109-2204.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20201110_1604981292.tar.gz - ``` hcl-dx-postgres-image-v1.4.0_20201109-2206.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF19_20201109-2312.tar.gz - ``` hcl-dx-ringapi-image-v1.4.0_20201109-2209.tar.gz - ``` dxclient_v1.0.0_20201110-2010.zip ## HCL DX 9.5 CF184 - **HCL DX 9.5 Container Update CF184** If deploying HCL DX 9.5 CF184 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF184.zip files**: - ``` HCL DX notices V9.5 CF184.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF184_20200917-0054.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF184_20200917-0054.zip - ``` hcl-dx-content-composer-image-v1.3.0_20200916-1952.tar.gz - ``` hcl-dx-core-image-v95_CF184_20200916-2009.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF184_20200916-2034.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.3.0_20200916-1953.tar.gz - ``` hcl-dx-image-processor-image-v1.3.0_20200916-1953.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20200917_1600304449.tar.gz - ``` hcl-dx-postgres-image-v1.3.0_20200916-2003.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF184_20200916-2009.tar.gz - ``` hcl-dx-ringapi-image-v1.3.0_20200916-1953.tar.gz ## HCL DX 9.5 CF183 - **HCL DX 9.5 Container Update CF183** If deploying HCL DX 9.5 CF183 release, the image and package names are as follows: **CF183-core.zip files**: - ``` HCL DX notices V9.5 CF183.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF183_20200818-1852.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF183_20200818-1852.zip - ``` hcl-dx-core-image-v95_CF183_20200818-1342.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz **CF183-other.zip files**: - ``` HCL DX notices V9.5 CF183.txt - ``` hcl-dx-content-composer-image-v1.2.0_20200818-1343.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF183_20200818-1344.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.2.0_20200818-1346.tar.gz - ``` hcl-dx-image-processor-image-v1.2.0_20200818-1345.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20200818_1597758965.tar.gz - ``` hcl-dx-postgres-image-v1.2.0_20200818-1349.tar.gz - ``` hcl-dx-remote-search-image-v95_CF183_20200818-1342.tar.gz - ``` hcl-dx-ringapi-image-v1.2.0_20200818-1351.tar.gz","title":"HCL DX 9.5 CF193"},{"location":"containerization/docker/#hcl-dx-95-cf182","text":"CF182 If deploying HCL DX 9.5 CF182 release, the image and package names are as follows: hcl-dx-kubernetes-v95-CF182-core.zip : ``` HCL DX notices V9.5 CF182.txt - ``` hcl-dx-ambassador-image-0850.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF182_20200720-1708.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF182_20200720-1708.zip - ``` hcl-dx-core-image-v95_CF182_20200720-1645.tar.gz **hcl-dx-kubernetes-v95-CF182-other.zip**: - ``` HCL DX notices V9.5 CF182.txt - ``` hcl-dx-content-composer-image-v1.1.0_20200720-1708.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF182_20200720-1716.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.1.0_20200720-1712.tar.gz - ``` hcl-dx-image-processor-image-v1.1.0_20200720-1716.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20200720_1595265588.tar.gz - ``` hcl-dx-postgres-image-v1.1.0_20200720-1715.tar.gz - ``` hcl-dx-remote-search-image-v95_CF182_20200720-1645.tar.gz - ``` hcl-dx-ringapi-image-v1.1.0_20200720-1707.tar.gz","title":"HCL DX 9.5 CF182"},{"location":"containerization/docker/#hcl-dx-95-cf181","text":"CF181 If deploying HCL DX 9.5 CF181 and later container update release, the image and package names add HCL DX 9.5 Content Composer and Digital Asset Management components and supporting services images, as well as Remote Search and OpenLDAP images. See the following list of images presented within the HCL DX 9.5 Container Update CF181 package: hcl-dx-kubernetes-v95-CF181-core.zip : ``` hcl-dx-cloud-scripts-v95_CF181_xxxxxxxx-xxxx.zip - ``` hcl-dx-core-image-v95_CF181_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-ambassador-image-xxxx.tar.gz - ``` HCL DX notices V9.5 CF181.txt **hcl-dx-kubernetes-v95-CF181-other.zip**: - ``` hcl-dx-ringapi-image-v1.0.0_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-postgres-image-v1.0.0_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-release_xxxxxxxx_xxxxxxxxxx.tar.gz - ``` hcl-dx-medialibrary-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-image-processor-image-v1.0.0_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-dxrs-image-v95_CF181_xxxxxxxx-xxxx.tar.gz ## HCL DX 9.5 CF18 - **CF18** If deploying HCL DX 9.5 CF18 and later container update release, the image and package names included for this are: - ``` hcl-dx-kubernetes-v95-CF18.zip - ``` hcl-dx-core-image-v95_CF18_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF18_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF18_xxxxxxxx-xxxx.zip - ``` hcl-dx-ambassador-image-xxxx.tar.gz See the following sections for additional information: Docker image deployment Customizing the container deployment Requirements and limitations","title":"HCL DX 9.5 CF181"},{"location":"containerization/docker_compose/","text":"Docker image deployment using Docker Compose This section presents availability of a new option to deploy HCL Digital Experience 9.5 Docker images for non-production using Docker Compose . This approach streamlines deployment and configuration of HCL DX 9.5 components. For more information about Docker Compose, see the Docker Compose documentation . Pre-requisite: Download the Docker images for the HCL DX 9.5 Container Update version you wish to deploy. From your HCL Digital Experience entitlements in the HCL Software License Portal , the Docker images are in the HCL_Portal_8.5_9.0_9.5_CFs download package entry. Refer to the Docker image list for the latest HCL DX 9.5 container update releases. Find the HCL DX 9.5 Docker Compose scripts, installation and configuration instructions for non-production use in the repositories on the HCL Software Github . Use the HCL-DX tag to find the DX Docker Compose entry. Video: Use Docker Compose to install HCL DX 9.5 with Cloud Components for Developer Use","title":"Docker image deployment using Docker Compose"},{"location":"containerization/docker_compose/#docker-image-deployment-using-docker-compose","text":"This section presents availability of a new option to deploy HCL Digital Experience 9.5 Docker images for non-production using Docker Compose . This approach streamlines deployment and configuration of HCL DX 9.5 components. For more information about Docker Compose, see the Docker Compose documentation . Pre-requisite: Download the Docker images for the HCL DX 9.5 Container Update version you wish to deploy. From your HCL Digital Experience entitlements in the HCL Software License Portal , the Docker images are in the HCL_Portal_8.5_9.0_9.5_CFs download package entry. Refer to the Docker image list for the latest HCL DX 9.5 container update releases. Find the HCL DX 9.5 Docker Compose scripts, installation and configuration instructions for non-production use in the repositories on the HCL Software Github . Use the HCL-DX tag to find the DX Docker Compose entry. Video: Use Docker Compose to install HCL DX 9.5 with Cloud Components for Developer Use","title":"Docker image deployment using Docker Compose"},{"location":"containerization/docker_image_deployment/","text":"Docker image deployment This section describes the steps in deploying HCL Digital Experience 9.5 containers using Docker. Follow these steps to deploy the HCL Digital Experience 9.5 and later CF container update releases in Docker. Download the Docker image from your HCL Digital Experience entitlements in the HCL Software License Portal , in the HCL_Portal_8.5_9.0_9.5_CFs download package entry. Refer to the Docker image list for the latest HCL DX 9.5 container update releases. Open a terminal window and change to the root directory of the extracted package. Load the container into your Docker repository: docker load < hcl-dx-core-image-v95-xxxxxxxx-xxxx.tar.gz Run the HCL DX Docker container using either of the following commands: ``` docker run -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 hcl/dx/core:v95_xxxxxxxx-xxxx - ``` docker run -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 -v ~/dx-store/ wp_profile:/opt/HCL/wp_profile hcl/dx/core:v95_xxxxxxxx-xxxx In HCL DX 9.5 CF171, Administrators can use this command to run the container if credentials have been updated: - ``` docker run -e WAS_ADMIN=wasadmin -e WAS_PASSWORD=wasadminpwd -e DX_ADMIN=dxadmin -e DX_PASSWORD=dxadminpwd -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 -v ~/dx-store/wp_profile:/opt/HCL/wp_profile hcl/dx/core:v95_xxxxxxxx-xxxx The additional syntax adds the ability for users to pass updated credentials for the HCL Portal Administrators. - ``` -e WAS_ADMIN=wasadmin - ``` -e WAS_PASSWORD=wasadminpwd - ``` -e DX_ADMIN=dxadmin - ``` -e DX_PASSWORD=dxadminpwd ``` **Notes:** - Make sure the ~/dx-store/wp\\_profile directory is created before you start the Docker container. This is required for persistence \\(for using `-v ~/dx-store/wp_profile:/opt/HCL/wp_profile hcl/dx/ core:v95_xxxxxxxx-xxxx`\\). - To use the HCL DX Configuration Wizard, start the Java virtual machine \\(JVM\\) within the running container with the following command: ``` docker exec <CONTAINER ID> /opt/HCL/AppServer/profiles/cw_profile/bin/startServer.sh server1 ``` - For HCL DX 9.5 CF171 and later, access the Configuration Wizard at https://localhost:10202/hcl/wizard. **Note:** For HCL DX 9.5 release earlier than CF171, access the Configuration Wizard at https://localhost:10202/ibm/wizard. - Upgrading an existing HCL DX 9.5 Docker container, using a persisted volume, to HCL DX 9.5 CF171 or HCL DX 9.5 CF172 may require launching the upgraded container twice. For example, if the following command fails with an error, re-running the command allows a successful upgrade and launch the container: ``` docker run -e WAS_ADMIN=wasadmin -e WAS_PASSWORD=wasadminpwd -e DX_ADMIN=dxadmin -e DX_PASSWORD=dxadminpwd -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 -v ~/dx-store/wp_profile:/opt/HCL/wp_profile hcl/dx/core:v95_xxxxxxxx-xxxx ``` This issue is fixed in HCL DX 9.5 CF173. See the following sections for additional information: - [How to upload HCL Digital Experience 9.5 CF container images to a private repository](https://youtu.be/XJONRdpgCuo) - [Docker image list](docker_image_deployment.md) - [Customizing the container deployment](customizing_container_deployment.md) - [Containerization Limitations/Requirements](limitations_requirements.md)","title":"Docker image deployment"},{"location":"containerization/docker_image_deployment/#docker-image-deployment","text":"This section describes the steps in deploying HCL Digital Experience 9.5 containers using Docker. Follow these steps to deploy the HCL Digital Experience 9.5 and later CF container update releases in Docker. Download the Docker image from your HCL Digital Experience entitlements in the HCL Software License Portal , in the HCL_Portal_8.5_9.0_9.5_CFs download package entry. Refer to the Docker image list for the latest HCL DX 9.5 container update releases. Open a terminal window and change to the root directory of the extracted package. Load the container into your Docker repository: docker load < hcl-dx-core-image-v95-xxxxxxxx-xxxx.tar.gz Run the HCL DX Docker container using either of the following commands: ``` docker run -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 hcl/dx/core:v95_xxxxxxxx-xxxx - ``` docker run -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 -v ~/dx-store/ wp_profile:/opt/HCL/wp_profile hcl/dx/core:v95_xxxxxxxx-xxxx In HCL DX 9.5 CF171, Administrators can use this command to run the container if credentials have been updated: - ``` docker run -e WAS_ADMIN=wasadmin -e WAS_PASSWORD=wasadminpwd -e DX_ADMIN=dxadmin -e DX_PASSWORD=dxadminpwd -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 -v ~/dx-store/wp_profile:/opt/HCL/wp_profile hcl/dx/core:v95_xxxxxxxx-xxxx The additional syntax adds the ability for users to pass updated credentials for the HCL Portal Administrators. - ``` -e WAS_ADMIN=wasadmin - ``` -e WAS_PASSWORD=wasadminpwd - ``` -e DX_ADMIN=dxadmin - ``` -e DX_PASSWORD=dxadminpwd ``` **Notes:** - Make sure the ~/dx-store/wp\\_profile directory is created before you start the Docker container. This is required for persistence \\(for using `-v ~/dx-store/wp_profile:/opt/HCL/wp_profile hcl/dx/ core:v95_xxxxxxxx-xxxx`\\). - To use the HCL DX Configuration Wizard, start the Java virtual machine \\(JVM\\) within the running container with the following command: ``` docker exec <CONTAINER ID> /opt/HCL/AppServer/profiles/cw_profile/bin/startServer.sh server1 ``` - For HCL DX 9.5 CF171 and later, access the Configuration Wizard at https://localhost:10202/hcl/wizard. **Note:** For HCL DX 9.5 release earlier than CF171, access the Configuration Wizard at https://localhost:10202/ibm/wizard. - Upgrading an existing HCL DX 9.5 Docker container, using a persisted volume, to HCL DX 9.5 CF171 or HCL DX 9.5 CF172 may require launching the upgraded container twice. For example, if the following command fails with an error, re-running the command allows a successful upgrade and launch the container: ``` docker run -e WAS_ADMIN=wasadmin -e WAS_PASSWORD=wasadminpwd -e DX_ADMIN=dxadmin -e DX_PASSWORD=dxadminpwd -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 -v ~/dx-store/wp_profile:/opt/HCL/wp_profile hcl/dx/core:v95_xxxxxxxx-xxxx ``` This issue is fixed in HCL DX 9.5 CF173. See the following sections for additional information: - [How to upload HCL Digital Experience 9.5 CF container images to a private repository](https://youtu.be/XJONRdpgCuo) - [Docker image list](docker_image_deployment.md) - [Customizing the container deployment](customizing_container_deployment.md) - [Containerization Limitations/Requirements](limitations_requirements.md)","title":"Docker image deployment"},{"location":"containerization/dxclient/","text":"DXClient DXClient is a tool that helps developers and administrators manage tasks, such as uploading one or more portlets or Script Applications, from source development environments to target HCL DX 9.5 deployments. This tool is capable of taking artifacts developed locally and deploying them to DX 9.5 servers deployed to supported on-premises platforms in standalone, cluster, or farm-topologies and supported Kubernetes platforms. Notes: DXClient is enabled in supported Kubernetes platforms from HCL Digital Experience 9.5 CF192 and later releases. DXClient is available as a Docker image from HCL DX 9.5 CF196 and later releases, See the installation section for more details. DXClient is a Node.js -based CLI tool and requires Node.js to be installed as a prerequisite. See video: Getting Started with DXClient on Red Hat OpenShift using HCL Digital Experience Container Update CF194 DXConnect DXConnect is a servlet-based application deployed on top of IBM WebSphere Application Server in the HCL DX 9.5 CF19 and later deployments, under the Configuration Wizard profile - cw_profile . DXConnect enables the DXClient tool to connect over an HTTP or HTTPS connection from a client development machine or remote server to a source or target HCL DX 9.5 server to execute certain tasks requested via DXClient commands. Architecture Notes: HCL DX 9.5 CF19 or later version is installed on target servers, on supported on premises platforms (Microsoft Windows, Linux or IBM AIX). Beginning with HCL DX 9.5 Container Update CF192 and later releases, the DXConnect Servlet is pre-configured and started on supported Red Hat OpenShift and Kubernetes platforms that DX 9.5 containers are deployed to. For supported on premises platforms with HCL DX 9.5 CF19 and later releases, the DXConnect application needs to be installed (refer to DXConnect Installation ) and started under the Configuration Wizard ( cw_profile ) on target servers. For more information on starting the Configuration Wizard, refer to Accessing the Configuration Wizard Remember: Configuration Wizard Administrator credentials are required to access the DXConnect application. Installing using the Docker image Prerequisites: You must ensure that Docker is installed on the workstation. DXClient docker image comes with a script that you can use to run the docker image. This script creates a store directory,and copies the input files from the absolute path to the shared volume location. Navigate to <working-directory> folder where you wish to use DXClient from. Download the DXClient.zip file (DXClient_VX_XXXXXXXX-XXXX.zip) to a local directory on the local workstation from your HCL Digital Experience 9.5 CF196 or higher entitlements on the HCL Software License Portal. Extract the DXClient.zip file locally. Run docker load < dxclient.tar.gz. Add the execution shell script to the bin directory to the PATH variable to be able to call dxclient from any directory. export PATH=<working-directory>/bin:$PATH For Microsoft Windows platforms: use dxclient.bat script in the bin directory to the PATH variable to be able to call DXClient from any directory. Set appropriate permission. chmod xxx <working-directory>/bin Run 'dxclient -V' to verify that the dxclient command line is installed. A folder named store will be created in your working directory. This is the shared volume location to your docker container. Configuration, logger, output, and sample files under location - <working-directory>/store. Common command arguments can be pre-configured inside the config.json file available under <working-directory>/store folder. A sample configuration file that could be used on-premises platforms in standalone, cluster platforms is also available under <working-directory>/store/samples/sample-configurations/default-config.json for reference. Installing using the node package file (deprecated in CF196) Prerequisites: Node.js version 12.18.3 is the minimum supported version, and must be installed on the local workstation or automation server. Note: DXClient node package is deprecated in the HCL Digital Experience Container CF196 release. It might be removed in the future releases. You are encouraged to use the DXClient Docker package from CF Container release CF196 and later. Complete the following steps to install the DXClient tool in your local development workstation or automation server. Note: If you are upgrading from CF19, CF191, or later releases, you should first unlink the current version using the following command before installing the newer version. Syntax for Linux and Apple macOS platforms: make unlink Syntax for Microsoft Windows platforms: make_unlink.bat Ensure that Node.js version 12.18.3 or later version is installed to the local workstation. The DXClient tool is supported on Microsoft Windows, Linux, and Apple macOS workstations and automation servers. Download the DXClient.zip file (DXClient_VX_XXXXXXXX-XXXX.zip) to a local directory on the local workstation from your DX 9.5 CF19 or later entitlements on the HCL Software License Portal . Reference the Docker topic for the latest list of HCL DX 9.5 files available for download. Extract the DXClient.zip file locally. From the extracted folder, run the following command. For Linux and Apple macOS platforms: make install For Microsoft Windows platforms: make_install.bat The following commands are run: Run the following command to link your application to the local npm module in your machine. Refer to the following Notes section before you proceed. For Linux and Apple MacOS platforms: make link For Microsoft Windows platforms: make_link.bat Notes: Avoid using this command when scripting deployments from an automation server (for example, in pipelines) as there is a chance of picking up the wrong dependencies during tool version upgrades. If the link command is not used (such as on automation servers), then use the following command to run the application: For Linux and Apple MacOS platforms: ./bin/dxclient For Microsoft Windows platforms: node bin/dxclient DXClient node installation configuration Common command arguments can be pre-configured inside the config.json file available under dist/src/configuration folder. A sample configuration file that could be used for any of the supported Kubernetes platforms is also available under samples/sample-configurations.json for reference. { \"enableLogger\": true, \"dxProtocol\": \"\", \"hostname\": \"\", \"dxPort\": \"\", \"xmlConfigPath\": \"/wps/config\", \"dxUsername\": \"\", \"dxPassword\": \"\", \"dxSoapPort\": \"10033\", \"dxProfilePath\": \"\", \"dxConnectHostname\": \"\", \"dxConnectUsername\": \"\", \"dxConnectPassword\": \"\", \"dxConnectPort\": \"10202\", \"xmlFile\": \"\", \"warFile\": \"\", \"applicationFile\": \"\", \"applicationName\": \"\", \"themeName\": \"\", \"themePath\": \"\", \"outputFilePath\": \"\", \"dxConnectProtocol\": \"https\", \"wcmSiteArea\": \"\", \"excludes\": [ \"^bin$\", \"^lib$\", \"^src$\", \"^node_modules$\", \"^\\\\.classpath$\", \"^\\\\.project$\", \"^\\\\..*ignore$\", \"^config.json$\", \"^sp-cmdln.log$\", \"^sp.bat$\", \"^sp.sh$\" ], \"wcmContentPath\": \"\", \"wcmContentName\": \"\", \"prebuiltZip\": \"\", \"contenthandlerPath\": \"/wps/mycontenthandler\", \"mainHtmlFile\": \"\", \"wcmContentTitle\": \"\", \"contentRoot\": \"\", \"wcmContentId\": \"\", \"virtualPortalContext\": \"\", \"projectContext\": \"\", \"versionName\": \"\", \"restoreAsPublished\": false } DXClient node uninstalling To uninstall the DXClient tool, perform the following commands: For Linux and Apple MacOS platforms: make clean For Microsoft Windows platforms: make uninstall.bat To unlink the DXClient tool, perform the following commands: For Linux and Apple MacOS platforms: make unlink For Microsoft Windows platforms: make_unlink.bat Verify the DXClient installation Successful installation of the DXClient tool can be checked by using the \" dxclient -V \" command, which should show the version of the DXClient tool installed. Once installed, commands can be executed using the DXClient tool to perform CI / CD actions on HCL DX 9.5 servers. Notes: Refer to the list of features that were released in the following HCL DX 9.5 Container releases: HCL DX 9.5 CF199 release: DAM Staging Create credential vault slot Create syndication relation Export and import multiple resource environment providers Specify the context root for exporting and importing personalization rules HCL DX 9.5 CF198 release: List DAM schemas Personalization export and import rules Resource environment provider Manage virtual portals HCL DX 9.5 CF197 release: Undeploy portlets Deploy and undeploy themes Deploy application manage get-syndication report Restart Core Delete DAM schema HCL DX 9.5 CF196 release: Shared library HCL DX 9.5 CF195 release: Undeploy theme MLS export and import of WCM library HCL DX 9.5 CF193 release: Restart DX Core server Deploy Application Managing syndicators Managing subscribers HCL DX 9.5 CF192 release: Undeploy script applications Deploy theme (EAR and WebDAV based) HCL DX 9.5 CF19 release: Deploy Portlets or Undeploy portlets Deploy script applications XML Access Restore Script Application DXClient commands Command syntax conventions: dxclient [command] [options] Use the following command to execute the deploy portlet action: dxclient deploy-portlet [options] Use the following command to execute the undeploy portlet action: dxclient undeploy-portlet [options] Use the following command to execute the xmlaccess action: dxclient xmlaccess [options] Use the following command to execute the pull script application action: dxclient deploy-scriptapplication pull [options] Use the following command to execute the push script application action: dxclient deploy-scriptapplication push [options] Use the following command to execute the undeploy script application action: dxclient undeploy-scriptapplication [options] Use the following command to execute the restore script application action: dxclient restore-scriptapplication [options] Use the following command to execute the deploy application action: dxclient deploy-application [options] Use the following command to execute the DX Core restart action: dxclient restart-dx-core Use the following command to execute manage-subscriber action: dxclient manage-subscriber -h Use the following command to execute manage-syndicator action: dxclient manage-syndicator -h Use the following command to execute the deploy theme action: dxclient deploy-theme [options] Use the following command to execute the undeploy theme action: dxclient undeploy-theme [options] Use the following command to execute the manage-syndicator get-syndication-report action: dxclient manage-syndicator get-syndication-report [options] Use the following command to execute the shared-library action: dxclient shared-library [options] Use the following command to execute the delete DAM schema action: dxclient delete-dam-schema [options] Use the following command to list all DAM schemas present: dxclient list-dam-schemas [options] Use the following command to export the personalization rules from the target server: dxclient pzn-export [options] Use the following command to import the personalization rules into the target server: dxclient pzn-import [options] Use the following command to manage virtual portal tasks in the DX server: dxclient manage-virtual-portal [options] Use the following command to register subscriber: dxclient manage-dam-staging register-dam-subscriber [options] Use the following command to deregister subscriber: dxclient manage-dam-staging deregister-dam-subscriber [options] Use the following command to trigger manual sync: dxclient manage-dam-staging trigger-staging [options] Use the following command to create credential vault slot in the DX server: dxclient create-credential-vault [options] Use the following command to create the syndication relation between syndicator and subscriber in DX server: dxclient create-syndication-relation [options] Use the following command to create, update, delete, export or import a custom property from an existing Resource Environment Provider: dxclient resource-env-provider [options] DXClient Help commands The following commands show the Help documents for DXClient command usage. Use the following commands to display the Help document for DXClient: dxclient dxclient -h, --help Use the following command to display the DXClient version number: dxclient -V, --version Use the following command to display the detailed help for a specific command: dxclient help [command]","title":"DXClient"},{"location":"containerization/dxclient/#dxclient","text":"DXClient is a tool that helps developers and administrators manage tasks, such as uploading one or more portlets or Script Applications, from source development environments to target HCL DX 9.5 deployments. This tool is capable of taking artifacts developed locally and deploying them to DX 9.5 servers deployed to supported on-premises platforms in standalone, cluster, or farm-topologies and supported Kubernetes platforms. Notes: DXClient is enabled in supported Kubernetes platforms from HCL Digital Experience 9.5 CF192 and later releases. DXClient is available as a Docker image from HCL DX 9.5 CF196 and later releases, See the installation section for more details. DXClient is a Node.js -based CLI tool and requires Node.js to be installed as a prerequisite. See video: Getting Started with DXClient on Red Hat OpenShift using HCL Digital Experience Container Update CF194 DXConnect DXConnect is a servlet-based application deployed on top of IBM WebSphere Application Server in the HCL DX 9.5 CF19 and later deployments, under the Configuration Wizard profile - cw_profile . DXConnect enables the DXClient tool to connect over an HTTP or HTTPS connection from a client development machine or remote server to a source or target HCL DX 9.5 server to execute certain tasks requested via DXClient commands.","title":"DXClient"},{"location":"containerization/dxclient/#architecture","text":"Notes: HCL DX 9.5 CF19 or later version is installed on target servers, on supported on premises platforms (Microsoft Windows, Linux or IBM AIX). Beginning with HCL DX 9.5 Container Update CF192 and later releases, the DXConnect Servlet is pre-configured and started on supported Red Hat OpenShift and Kubernetes platforms that DX 9.5 containers are deployed to. For supported on premises platforms with HCL DX 9.5 CF19 and later releases, the DXConnect application needs to be installed (refer to DXConnect Installation ) and started under the Configuration Wizard ( cw_profile ) on target servers. For more information on starting the Configuration Wizard, refer to Accessing the Configuration Wizard Remember: Configuration Wizard Administrator credentials are required to access the DXConnect application.","title":"Architecture"},{"location":"containerization/dxclient/#installing-using-the-docker-image","text":"Prerequisites: You must ensure that Docker is installed on the workstation. DXClient docker image comes with a script that you can use to run the docker image. This script creates a store directory,and copies the input files from the absolute path to the shared volume location. Navigate to <working-directory> folder where you wish to use DXClient from. Download the DXClient.zip file (DXClient_VX_XXXXXXXX-XXXX.zip) to a local directory on the local workstation from your HCL Digital Experience 9.5 CF196 or higher entitlements on the HCL Software License Portal. Extract the DXClient.zip file locally. Run docker load < dxclient.tar.gz. Add the execution shell script to the bin directory to the PATH variable to be able to call dxclient from any directory. export PATH=<working-directory>/bin:$PATH For Microsoft Windows platforms: use dxclient.bat script in the bin directory to the PATH variable to be able to call DXClient from any directory. Set appropriate permission. chmod xxx <working-directory>/bin Run 'dxclient -V' to verify that the dxclient command line is installed. A folder named store will be created in your working directory. This is the shared volume location to your docker container. Configuration, logger, output, and sample files under location - <working-directory>/store. Common command arguments can be pre-configured inside the config.json file available under <working-directory>/store folder. A sample configuration file that could be used on-premises platforms in standalone, cluster platforms is also available under <working-directory>/store/samples/sample-configurations/default-config.json for reference.","title":"Installing using the Docker image"},{"location":"containerization/dxclient/#installing-using-the-node-package-file-deprecated-in-cf196","text":"Prerequisites: Node.js version 12.18.3 is the minimum supported version, and must be installed on the local workstation or automation server. Note: DXClient node package is deprecated in the HCL Digital Experience Container CF196 release. It might be removed in the future releases. You are encouraged to use the DXClient Docker package from CF Container release CF196 and later. Complete the following steps to install the DXClient tool in your local development workstation or automation server. Note: If you are upgrading from CF19, CF191, or later releases, you should first unlink the current version using the following command before installing the newer version. Syntax for Linux and Apple macOS platforms: make unlink Syntax for Microsoft Windows platforms: make_unlink.bat Ensure that Node.js version 12.18.3 or later version is installed to the local workstation. The DXClient tool is supported on Microsoft Windows, Linux, and Apple macOS workstations and automation servers. Download the DXClient.zip file (DXClient_VX_XXXXXXXX-XXXX.zip) to a local directory on the local workstation from your DX 9.5 CF19 or later entitlements on the HCL Software License Portal . Reference the Docker topic for the latest list of HCL DX 9.5 files available for download. Extract the DXClient.zip file locally. From the extracted folder, run the following command. For Linux and Apple macOS platforms: make install For Microsoft Windows platforms: make_install.bat The following commands are run: Run the following command to link your application to the local npm module in your machine. Refer to the following Notes section before you proceed. For Linux and Apple MacOS platforms: make link For Microsoft Windows platforms: make_link.bat Notes: Avoid using this command when scripting deployments from an automation server (for example, in pipelines) as there is a chance of picking up the wrong dependencies during tool version upgrades. If the link command is not used (such as on automation servers), then use the following command to run the application: For Linux and Apple MacOS platforms: ./bin/dxclient For Microsoft Windows platforms: node bin/dxclient DXClient node installation configuration Common command arguments can be pre-configured inside the config.json file available under dist/src/configuration folder. A sample configuration file that could be used for any of the supported Kubernetes platforms is also available under samples/sample-configurations.json for reference. { \"enableLogger\": true, \"dxProtocol\": \"\", \"hostname\": \"\", \"dxPort\": \"\", \"xmlConfigPath\": \"/wps/config\", \"dxUsername\": \"\", \"dxPassword\": \"\", \"dxSoapPort\": \"10033\", \"dxProfilePath\": \"\", \"dxConnectHostname\": \"\", \"dxConnectUsername\": \"\", \"dxConnectPassword\": \"\", \"dxConnectPort\": \"10202\", \"xmlFile\": \"\", \"warFile\": \"\", \"applicationFile\": \"\", \"applicationName\": \"\", \"themeName\": \"\", \"themePath\": \"\", \"outputFilePath\": \"\", \"dxConnectProtocol\": \"https\", \"wcmSiteArea\": \"\", \"excludes\": [ \"^bin$\", \"^lib$\", \"^src$\", \"^node_modules$\", \"^\\\\.classpath$\", \"^\\\\.project$\", \"^\\\\..*ignore$\", \"^config.json$\", \"^sp-cmdln.log$\", \"^sp.bat$\", \"^sp.sh$\" ], \"wcmContentPath\": \"\", \"wcmContentName\": \"\", \"prebuiltZip\": \"\", \"contenthandlerPath\": \"/wps/mycontenthandler\", \"mainHtmlFile\": \"\", \"wcmContentTitle\": \"\", \"contentRoot\": \"\", \"wcmContentId\": \"\", \"virtualPortalContext\": \"\", \"projectContext\": \"\", \"versionName\": \"\", \"restoreAsPublished\": false } DXClient node uninstalling To uninstall the DXClient tool, perform the following commands: For Linux and Apple MacOS platforms: make clean For Microsoft Windows platforms: make uninstall.bat To unlink the DXClient tool, perform the following commands: For Linux and Apple MacOS platforms: make unlink For Microsoft Windows platforms: make_unlink.bat","title":"Installing using the node package file (deprecated in CF196)"},{"location":"containerization/dxclient/#verify-the-dxclient-installation","text":"Successful installation of the DXClient tool can be checked by using the \" dxclient -V \" command, which should show the version of the DXClient tool installed. Once installed, commands can be executed using the DXClient tool to perform CI / CD actions on HCL DX 9.5 servers. Notes: Refer to the list of features that were released in the following HCL DX 9.5 Container releases: HCL DX 9.5 CF199 release: DAM Staging Create credential vault slot Create syndication relation Export and import multiple resource environment providers Specify the context root for exporting and importing personalization rules HCL DX 9.5 CF198 release: List DAM schemas Personalization export and import rules Resource environment provider Manage virtual portals HCL DX 9.5 CF197 release: Undeploy portlets Deploy and undeploy themes Deploy application manage get-syndication report Restart Core Delete DAM schema HCL DX 9.5 CF196 release: Shared library HCL DX 9.5 CF195 release: Undeploy theme MLS export and import of WCM library HCL DX 9.5 CF193 release: Restart DX Core server Deploy Application Managing syndicators Managing subscribers HCL DX 9.5 CF192 release: Undeploy script applications Deploy theme (EAR and WebDAV based) HCL DX 9.5 CF19 release: Deploy Portlets or Undeploy portlets Deploy script applications XML Access Restore Script Application","title":"Verify the DXClient installation"},{"location":"containerization/dxclient/#dxclient-commands","text":"Command syntax conventions: dxclient [command] [options] Use the following command to execute the deploy portlet action: dxclient deploy-portlet [options] Use the following command to execute the undeploy portlet action: dxclient undeploy-portlet [options] Use the following command to execute the xmlaccess action: dxclient xmlaccess [options] Use the following command to execute the pull script application action: dxclient deploy-scriptapplication pull [options] Use the following command to execute the push script application action: dxclient deploy-scriptapplication push [options] Use the following command to execute the undeploy script application action: dxclient undeploy-scriptapplication [options] Use the following command to execute the restore script application action: dxclient restore-scriptapplication [options] Use the following command to execute the deploy application action: dxclient deploy-application [options] Use the following command to execute the DX Core restart action: dxclient restart-dx-core Use the following command to execute manage-subscriber action: dxclient manage-subscriber -h Use the following command to execute manage-syndicator action: dxclient manage-syndicator -h Use the following command to execute the deploy theme action: dxclient deploy-theme [options] Use the following command to execute the undeploy theme action: dxclient undeploy-theme [options] Use the following command to execute the manage-syndicator get-syndication-report action: dxclient manage-syndicator get-syndication-report [options] Use the following command to execute the shared-library action: dxclient shared-library [options] Use the following command to execute the delete DAM schema action: dxclient delete-dam-schema [options] Use the following command to list all DAM schemas present: dxclient list-dam-schemas [options] Use the following command to export the personalization rules from the target server: dxclient pzn-export [options] Use the following command to import the personalization rules into the target server: dxclient pzn-import [options] Use the following command to manage virtual portal tasks in the DX server: dxclient manage-virtual-portal [options] Use the following command to register subscriber: dxclient manage-dam-staging register-dam-subscriber [options] Use the following command to deregister subscriber: dxclient manage-dam-staging deregister-dam-subscriber [options] Use the following command to trigger manual sync: dxclient manage-dam-staging trigger-staging [options] Use the following command to create credential vault slot in the DX server: dxclient create-credential-vault [options] Use the following command to create the syndication relation between syndicator and subscriber in DX server: dxclient create-syndication-relation [options] Use the following command to create, update, delete, export or import a custom property from an existing Resource Environment Provider: dxclient resource-env-provider [options]","title":"DXClient commands"},{"location":"containerization/dxclient/#dxclient-help-commands","text":"The following commands show the Help documents for DXClient command usage. Use the following commands to display the Help document for DXClient: dxclient dxclient -h, --help Use the following command to display the DXClient version number: dxclient -V, --version Use the following command to display the detailed help for a specific command: dxclient help [command]","title":"DXClient Help commands"},{"location":"containerization/dxconnect/","text":"DXConnect DXConnect is a servlet-based internal application deployed on top of IBM WebSphere Application Server in the HCL DX 9.5 CF19 and later releases, under the Configuration Wizard profile - cw_profile . DXConnect enables the DXClient tool to connect over an HTTP or HTTPS connection from a client development machine or remote server to a source or target HCL DX 9.5 server to execute certain tasks requested via DXClient commands. This topic covers the DXConnect installation and configuration instructions. Authentication DXConnect is a servlet-based application deployed on top of IBM WebSphere Application Server in an HCL DX 9.5 CF19 and later deployment, under the Configuration Wizard profile - cw_profile . DXConnect enables the DXClient tool to connect over an HTTPS connection from a client development workstation or automation server to a target HCL DX 9.5 server to execute certain tasks requested via DXClient commands. Authentication DXConnect requires the cw_profile Administrator security role to access the application servlet APIs. DXConnect Installation To install DXConnect use the command below: ./ConfigEngine.sh install-dxconnect-application This task will not only install the DxConnect application, but it will create the \"DXC_ConfigSettings\" WAS Resource Environment Provider and will create two custom properties in that REP: DXCONNECT_MAX_MEMORY_SIZE_MB DXCONNECT_MAX_FILE_SIZE_MB To remove DXConnect use the command below: ./ConfigEngine.sh remove-dxconnect-application To re-install DXConnect use the command below: ./ConfigEngine.sh reinstall-dxconnect-application Notes: In Standalone and Cluster setups, the ConfigEngine task should be run under the wp_profile to have DXConnect installed in the correct location, and a restart of the cw_profile server may be required. To verify it is installed on a given HCL DX Server 9.5 with CF19 or later, navigate to the Configuration Wizard Admin console and then under Enterprise Applications . The dxconnect application will appear on the console as shown in the example below. For more information on accessing and working with the Configuration Wizard, refer to Accessing the Configuration Wizard topics. In Red Hat OpenShift, the route for DXConnect is available under the name dx-deployment-service-dxconnect. For the other supported platforms, there is only one route path as usual.","title":"DXConnect"},{"location":"containerization/dxconnect/#dxconnect","text":"DXConnect is a servlet-based internal application deployed on top of IBM WebSphere Application Server in the HCL DX 9.5 CF19 and later releases, under the Configuration Wizard profile - cw_profile . DXConnect enables the DXClient tool to connect over an HTTP or HTTPS connection from a client development machine or remote server to a source or target HCL DX 9.5 server to execute certain tasks requested via DXClient commands. This topic covers the DXConnect installation and configuration instructions.","title":"DXConnect"},{"location":"containerization/dxconnect/#authentication","text":"DXConnect is a servlet-based application deployed on top of IBM WebSphere Application Server in an HCL DX 9.5 CF19 and later deployment, under the Configuration Wizard profile - cw_profile . DXConnect enables the DXClient tool to connect over an HTTPS connection from a client development workstation or automation server to a target HCL DX 9.5 server to execute certain tasks requested via DXClient commands. Authentication DXConnect requires the cw_profile Administrator security role to access the application servlet APIs. DXConnect Installation To install DXConnect use the command below: ./ConfigEngine.sh install-dxconnect-application This task will not only install the DxConnect application, but it will create the \"DXC_ConfigSettings\" WAS Resource Environment Provider and will create two custom properties in that REP: DXCONNECT_MAX_MEMORY_SIZE_MB DXCONNECT_MAX_FILE_SIZE_MB To remove DXConnect use the command below: ./ConfigEngine.sh remove-dxconnect-application To re-install DXConnect use the command below: ./ConfigEngine.sh reinstall-dxconnect-application Notes: In Standalone and Cluster setups, the ConfigEngine task should be run under the wp_profile to have DXConnect installed in the correct location, and a restart of the cw_profile server may be required. To verify it is installed on a given HCL DX Server 9.5 with CF19 or later, navigate to the Configuration Wizard Admin console and then under Enterprise Applications . The dxconnect application will appear on the console as shown in the example below. For more information on accessing and working with the Configuration Wizard, refer to Accessing the Configuration Wizard topics. In Red Hat OpenShift, the route for DXConnect is available under the name dx-deployment-service-dxconnect. For the other supported platforms, there is only one route path as usual.","title":"Authentication"},{"location":"containerization/dxcoreserver/","text":"DX Core server This topic provides information about restarting the DX Core server by using the DXClient tool. Restart DX Core server The restart-dx-core command is used to restart the DX Core server. Command description This command invokes the restart-dx-core tool inside the DXClient and runs the DX Core restart action. dxclient restart-dx-core Help command This command shows the help information for restart-dx-core command usage: dxclient restart-dx-core -h Command options Use this attribute to specify the username that is required for authenticating with the DX Core -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the DX Core -dxPassword <value> Use this attribute to specify the config wizard home (route change only in case of Open Shift Kubernetes Enviornment, otherwise same as hostname) that is required for authenticating to the cw_profile -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile(for Kubernetes Environment dxConnectPort is 443) -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile -dxConnectPassword <value> Specify either the dxProfileName or dxProfilePath of the DX core server: Use this attribute to specify the profile name of the DX core server (for example: wp_profile ) -dxProfileName <Profile name of the DX core server> OR Use this attribute to specify the profile path of the DX server (for example: /opt/HCL/wp_profile ) -dxProfilePath <Path of the DX core server profile> All the above command options can also be configured inside the config.json configuration file of the DXClient tool, available in the <working-directory>/store directory of the DXClient installation. Note: If you have installed DXClient using the node package file, then you can find the config.json file in the following path: dist/src/configuration. The values that are passed through the command line override the default values. Example Usage: dxclient restart-dx-core -dxUsername <dxUsername> -dxPassword <dxPassword> -dxConnectHostname <hostname> -dxConnectPort <dxConnectPort> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -dxProfileName <Profile name of the DX core server> Limitation In Kubernetes based deployments, if there are more than one pod for the core container, the restart command of DXClient only restarts the pod it is connected to and not all running pods. For a full restart of all pods, leverage using the Kubernetes interfaces like kubectl .","title":"DX Core server"},{"location":"containerization/dxcoreserver/#dx-core-server","text":"This topic provides information about restarting the DX Core server by using the DXClient tool.","title":"DX Core server"},{"location":"containerization/dxcoreserver/#restart-dx-core-server","text":"The restart-dx-core command is used to restart the DX Core server. Command description This command invokes the restart-dx-core tool inside the DXClient and runs the DX Core restart action. dxclient restart-dx-core Help command This command shows the help information for restart-dx-core command usage: dxclient restart-dx-core -h Command options Use this attribute to specify the username that is required for authenticating with the DX Core -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the DX Core -dxPassword <value> Use this attribute to specify the config wizard home (route change only in case of Open Shift Kubernetes Enviornment, otherwise same as hostname) that is required for authenticating to the cw_profile -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile(for Kubernetes Environment dxConnectPort is 443) -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile -dxConnectPassword <value> Specify either the dxProfileName or dxProfilePath of the DX core server: Use this attribute to specify the profile name of the DX core server (for example: wp_profile ) -dxProfileName <Profile name of the DX core server> OR Use this attribute to specify the profile path of the DX server (for example: /opt/HCL/wp_profile ) -dxProfilePath <Path of the DX core server profile> All the above command options can also be configured inside the config.json configuration file of the DXClient tool, available in the <working-directory>/store directory of the DXClient installation. Note: If you have installed DXClient using the node package file, then you can find the config.json file in the following path: dist/src/configuration. The values that are passed through the command line override the default values. Example Usage: dxclient restart-dx-core -dxUsername <dxUsername> -dxPassword <dxPassword> -dxConnectHostname <hostname> -dxConnectPort <dxConnectPort> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -dxProfileName <Profile name of the DX core server> Limitation In Kubernetes based deployments, if there are more than one pod for the core container, the restart command of DXClient only restarts the pod it is connected to and not all running pods. For a full restart of all pods, leverage using the Kubernetes interfaces like kubectl .","title":"Restart DX Core server"},{"location":"containerization/dxtools_dxctl/","text":"dxctl Learn how to use dxctl for custom HCL Digital Experience 9.5 container deployments About this task Administrators can use the dxctl tool provided with Container Update CF19 and later releases to define and configure custom DX container deployments. See the following guidance: Video : Using dxctl to Deploy DX Portal on OpenShift General help for the dxctl tool or help related for sub-commands ( create , update , collect , and destroy ) and the command syntax are found with --help . dxctl can be used to deploy DX using a properties file. Sample properties files are included in the dxctl/properties directory. The properties files function as follows: Full deployment config: full-deployment.properties hybrid.enabled: false hybrid.host: onprem_hostname.com hybrid.port: 10042 For Mac: ./mac/dxctl --deploy -p properties/full-deployment.properties For Windows: win\\dxctl.exe -\u2013deploy -p properties/full-deployment.properties For Linux: ./linux/dxctl -\u2013deploy -p properties/full-deployment.properties Hybrid deployment config: hybrid-deployment.properties hybrid.enabled: true hybrid.host: aws-hybrid.sample-dx-deploy.com hybrid.port: 10042 For Mac: ./mac/dxctl --deploy -p properties/hybrid-deployment.properties For Windows: win\\dxctl.exe -\u2013deploy -p properties/hybrid-deployment.properties For Linux: ./linux/dxctl -\u2013deploy -p properties/hybrid-deployment.properties These create a hybrid deployment with Experience API, Content Composer, and Digital Asset Management. You can disable any of these features by making a copy of the hybrid file and setting the value to false to disable it. Example: composer.enabled: false disables Content Composer. Note: Experience API must be enabled to deploy Content Composer and Digital Asset Management. Prerequisites The following are the prerequisites for using dxctl . Before running the dxctl tool, you must log in on the targeted cluster using your platform's cloud-specific command-line interface (CLI), such as Azure CLI (az), gcloud CLI, AWS CLI, OpenShift CLI (oc), etc. For example, in Red Hat OpenShift, you must use oc login . dxctl does not deploy the DxDeployment custom resource definition. You must run the ./scripts/deployCrd.sh before using dxctl . Creating a deployment Follow these steps to create a deployment. You must copy the properties file once a deployment is created. Use the copied file to perform a deployment and maintain and update a deployment. For example: mkdir -p /home/$USER/deployments/ cp dxctl/properties/full-deployment.properties /home/$USER/deployments/myfirst_deployment.properties Change the settings. For example, change dx.namespace: to myfirst-dx-deployment . ./linux/dxctl --deploy -p /home/$USER/deployments/myfirst_deployment.properties Note: For OpenShift deployments, /linux/dxctl --deploy is all you need. For all other Kubernetes environments (EKS, GKE, etc.), you need to generate a TLS certification and private key. See the Generate TLS Certificate topic for more information. Updating a deployment Limitation: If you have a DX-only deployment (a deployment that contains only DX without any other features, such as the Experience API, Content Composer, or Digital Asset Management) installed using the deployment script, the dxctl tool cannot be used to update this deployment. You may continue to use the DX deployment script to update this deployment. Note: When working with HCL Digital Experience 9.5 Container Update CF192 and later, the dxctl tool can be used to update the deployment. The dxctl tool does not deploy or update the DxDeployment custom resource definition. Prior to running an update process, administrators should check the DxDeployment custom resource definition ( hcl-dx-cloud-scripts/deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml ) for changes and update accordingly: Red Hat OpenShift command: oc delete crd dxdeployments.git.cwp.pnp-hcl.com Kubernetes command: kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com CAUTION: Since crd is a cluster-wide resource, the use of kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com or oc delete crd dxdeployments.git.cwp.pnp-hcl.com causes a service outage for all the dx-deployment across the cluster. Red Hat OpenShift command: oc create -f deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml Kubernetes command: kubectl create -f deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml Follow these steps to update a deployment. Update the properties file with the new image values and run the update command: For Mac: ./mac/dxctl --update -p properties/myfirst_deployment.properties For Windows: .\\win\\dxctl.exe --update -p properties\\myfirst_deployment.properties For Linux: ./linux/dxctl -\u2013update -p properties/myfirst_deployment.properties With the updated deployment, if you were switching to a next release, you can use the properties file to replace the repository, image, and tag as required and perform the update command. Deleting a deployment There are two ways to delete a deployment. Method 1: Remove the deployment but allow for redeployment with the same volumes. ./linux/dxctl --destroy -p properties/hybrid-deployment.properties Method 2: Remove the entire namespace/project . ./linux/dxctl --destroy -p properties/hybrid-deployment.properties -all true If some resources, like services, are not deleted, run the following command: kubectl patch services $(kubectl get services -n $NAMESPACE | grep -v \"NAME\" |awk '{print $1}') -p '{\"metadata\":{\"finalizers\":null}}' -n $NAMESPACE Main usage Usage information for dxctl , for additional information, use --help with an action. Deploy Run to deploy a DX deployment. dxctl --deploy --help Update Run to update a DX deployment. dxctl --update --help Collect Run to collect support data for a given deployment. dxctl --collect --help Destroy Run to destroy a DX deployment. dxctl --destroy --help dxctl help Sub-commands, required: deploy , update , collect , or destroy . --deploy or --update action string Update an existing DX deployment. Default: update dx.database string The database type Oracle, DB2, etc. Default: derby dx.image string Required, the DX core image. dx.name string Deployment name. Default: dx-deployment dx.namespace string Required, the target namespace/project . dx.operator.image string Required, the HCL cloud operator image. dx.operator.tag string Required, the HCL cloud operator tag. dx.repository string Required, the image HCL cloud operator repository. dx.tag string Required, the DX core tag. filename string File name to write into dx-tests.dx-deployment.txt. This contains the test and deployment logs. By default, the namespace is used as the filename. Example: NAMESPACE.txt ingress.image string Required, the ambassador image. Not used in OpenShift deployments. ingress.tag string Required, the ambassador tag. Not used in OpenShift deployments. p string dxctl can be run from a properties file, -p namespace.properties, no default. verbose Display messages on the command line. Default: false --collect action string Collecting deployment information about an existing deployment. Default: collect dx.name string Deployment name. Default: dx-deployment dx.namespace string Required, the target namespace/project . filename string File name to write into dx-tests.dx-deployment.txt. This contains the test and deployment logs. By default, the namespace is used as the filename. Example: NAMESPACE.txt verbose Display messages on the command line. Default: false --destroy action string Destroy a DX deployment. Default: destroy all Delete the project/namespace and all artifacts. Default: false dx.name string Deployment name. Default: dx-deployment dx.namespace string Required, the target namespace/project . filename string File name to write into dx-tests.dx-deployment.txt. This contains the test and deployment logs. By default, the namespace is used as the filename. Example: NAMESPACE.txt verbose Display messages on the command line. Default: false","title":"dxctl"},{"location":"containerization/dxtools_dxctl/#dxctl","text":"Learn how to use dxctl for custom HCL Digital Experience 9.5 container deployments","title":"dxctl"},{"location":"containerization/dxtools_dxctl/#about-this-task","text":"Administrators can use the dxctl tool provided with Container Update CF19 and later releases to define and configure custom DX container deployments. See the following guidance: Video : Using dxctl to Deploy DX Portal on OpenShift General help for the dxctl tool or help related for sub-commands ( create , update , collect , and destroy ) and the command syntax are found with --help . dxctl can be used to deploy DX using a properties file. Sample properties files are included in the dxctl/properties directory. The properties files function as follows: Full deployment config: full-deployment.properties hybrid.enabled: false hybrid.host: onprem_hostname.com hybrid.port: 10042 For Mac: ./mac/dxctl --deploy -p properties/full-deployment.properties For Windows: win\\dxctl.exe -\u2013deploy -p properties/full-deployment.properties For Linux: ./linux/dxctl -\u2013deploy -p properties/full-deployment.properties Hybrid deployment config: hybrid-deployment.properties hybrid.enabled: true hybrid.host: aws-hybrid.sample-dx-deploy.com hybrid.port: 10042 For Mac: ./mac/dxctl --deploy -p properties/hybrid-deployment.properties For Windows: win\\dxctl.exe -\u2013deploy -p properties/hybrid-deployment.properties For Linux: ./linux/dxctl -\u2013deploy -p properties/hybrid-deployment.properties These create a hybrid deployment with Experience API, Content Composer, and Digital Asset Management. You can disable any of these features by making a copy of the hybrid file and setting the value to false to disable it. Example: composer.enabled: false disables Content Composer. Note: Experience API must be enabled to deploy Content Composer and Digital Asset Management.","title":"About this task"},{"location":"containerization/dxtools_dxctl/#prerequisites","text":"The following are the prerequisites for using dxctl . Before running the dxctl tool, you must log in on the targeted cluster using your platform's cloud-specific command-line interface (CLI), such as Azure CLI (az), gcloud CLI, AWS CLI, OpenShift CLI (oc), etc. For example, in Red Hat OpenShift, you must use oc login . dxctl does not deploy the DxDeployment custom resource definition. You must run the ./scripts/deployCrd.sh before using dxctl .","title":"Prerequisites"},{"location":"containerization/dxtools_dxctl/#creating-a-deployment","text":"Follow these steps to create a deployment. You must copy the properties file once a deployment is created. Use the copied file to perform a deployment and maintain and update a deployment. For example: mkdir -p /home/$USER/deployments/ cp dxctl/properties/full-deployment.properties /home/$USER/deployments/myfirst_deployment.properties Change the settings. For example, change dx.namespace: to myfirst-dx-deployment . ./linux/dxctl --deploy -p /home/$USER/deployments/myfirst_deployment.properties Note: For OpenShift deployments, /linux/dxctl --deploy is all you need. For all other Kubernetes environments (EKS, GKE, etc.), you need to generate a TLS certification and private key. See the Generate TLS Certificate topic for more information.","title":"Creating a deployment"},{"location":"containerization/dxtools_dxctl/#updating-a-deployment","text":"Limitation: If you have a DX-only deployment (a deployment that contains only DX without any other features, such as the Experience API, Content Composer, or Digital Asset Management) installed using the deployment script, the dxctl tool cannot be used to update this deployment. You may continue to use the DX deployment script to update this deployment. Note: When working with HCL Digital Experience 9.5 Container Update CF192 and later, the dxctl tool can be used to update the deployment. The dxctl tool does not deploy or update the DxDeployment custom resource definition. Prior to running an update process, administrators should check the DxDeployment custom resource definition ( hcl-dx-cloud-scripts/deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml ) for changes and update accordingly: Red Hat OpenShift command: oc delete crd dxdeployments.git.cwp.pnp-hcl.com Kubernetes command: kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com CAUTION: Since crd is a cluster-wide resource, the use of kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com or oc delete crd dxdeployments.git.cwp.pnp-hcl.com causes a service outage for all the dx-deployment across the cluster. Red Hat OpenShift command: oc create -f deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml Kubernetes command: kubectl create -f deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml Follow these steps to update a deployment. Update the properties file with the new image values and run the update command: For Mac: ./mac/dxctl --update -p properties/myfirst_deployment.properties For Windows: .\\win\\dxctl.exe --update -p properties\\myfirst_deployment.properties For Linux: ./linux/dxctl -\u2013update -p properties/myfirst_deployment.properties With the updated deployment, if you were switching to a next release, you can use the properties file to replace the repository, image, and tag as required and perform the update command.","title":"Updating a deployment"},{"location":"containerization/dxtools_dxctl/#deleting-a-deployment","text":"There are two ways to delete a deployment. Method 1: Remove the deployment but allow for redeployment with the same volumes. ./linux/dxctl --destroy -p properties/hybrid-deployment.properties Method 2: Remove the entire namespace/project . ./linux/dxctl --destroy -p properties/hybrid-deployment.properties -all true If some resources, like services, are not deleted, run the following command: kubectl patch services $(kubectl get services -n $NAMESPACE | grep -v \"NAME\" |awk '{print $1}') -p '{\"metadata\":{\"finalizers\":null}}' -n $NAMESPACE","title":"Deleting a deployment"},{"location":"containerization/dxtools_dxctl/#main-usage","text":"Usage information for dxctl , for additional information, use --help with an action. Deploy Run to deploy a DX deployment. dxctl --deploy --help Update Run to update a DX deployment. dxctl --update --help Collect Run to collect support data for a given deployment. dxctl --collect --help Destroy Run to destroy a DX deployment. dxctl --destroy --help","title":"Main usage"},{"location":"containerization/dxtools_dxctl/#dxctl-help","text":"Sub-commands, required: deploy , update , collect , or destroy . --deploy or --update action string Update an existing DX deployment. Default: update dx.database string The database type Oracle, DB2, etc. Default: derby dx.image string Required, the DX core image. dx.name string Deployment name. Default: dx-deployment dx.namespace string Required, the target namespace/project . dx.operator.image string Required, the HCL cloud operator image. dx.operator.tag string Required, the HCL cloud operator tag. dx.repository string Required, the image HCL cloud operator repository. dx.tag string Required, the DX core tag. filename string File name to write into dx-tests.dx-deployment.txt. This contains the test and deployment logs. By default, the namespace is used as the filename. Example: NAMESPACE.txt ingress.image string Required, the ambassador image. Not used in OpenShift deployments. ingress.tag string Required, the ambassador tag. Not used in OpenShift deployments. p string dxctl can be run from a properties file, -p namespace.properties, no default. verbose Display messages on the command line. Default: false --collect action string Collecting deployment information about an existing deployment. Default: collect dx.name string Deployment name. Default: dx-deployment dx.namespace string Required, the target namespace/project . filename string File name to write into dx-tests.dx-deployment.txt. This contains the test and deployment logs. By default, the namespace is used as the filename. Example: NAMESPACE.txt verbose Display messages on the command line. Default: false --destroy action string Destroy a DX deployment. Default: destroy all Delete the project/namespace and all artifacts. Default: false dx.name string Deployment name. Default: dx-deployment dx.namespace string Required, the target namespace/project . filename string File name to write into dx-tests.dx-deployment.txt. This contains the test and deployment logs. By default, the namespace is used as the filename. Example: NAMESPACE.txt verbose Display messages on the command line. Default: false","title":"dxctl help"},{"location":"containerization/google_gke/","text":"Deploying HCL Digital Experience Containers to Google Kubernetes Engine (GKE) Learn how to deploy different releases of HCL Digital Experience (DX) containers, along with the Ambassador, to Kubernetes as verified in Google Kubernetes Engine (GKE) . Note: Refer to the latest HCL DX 9.5 Container Update Release CF192 and later file listings in the Docker deployment topic.","title":"Deploying HCL Digital Experience Containers to Google Kubernetes Engine \\(GKE\\)"},{"location":"containerization/google_gke/#deploying-hcl-digital-experience-containers-to-google-kubernetes-engine-gke","text":"Learn how to deploy different releases of HCL Digital Experience (DX) containers, along with the Ambassador, to Kubernetes as verified in Google Kubernetes Engine (GKE) . Note: Refer to the latest HCL DX 9.5 Container Update Release CF192 and later file listings in the Docker deployment topic.","title":"Deploying HCL Digital Experience Containers to Google Kubernetes Engine (GKE)"},{"location":"containerization/hybrid_deployment_operator/","text":"Hybrid deployment - Operator This section describes how to install HCL Digital Experience 9.5 Portal Server and Web Content Manager services to on-premises platforms, operating with Digital Asset Management, Content Composer, Experience API deployed to cloud-based Kubernetes and OpenShift platforms deployed using the Operator (dxctl) method. Many organizations are using cloud and containerized deployments as part of their overall systems environments. In parallel, organizations continue to operate software applications and processes on-premises. The HCL Digital Experience 9.5 Hybrid deployment reference architecture and topics describe an approach to deploy and manage HCL Digital Experience 9.5 core Portal Server and Web Content Manager services on premises, and connect to cloud native components Digital Asset Management, Content Composer, Experience API and related services in a production environment. Notes: The hybrid deployment of cloud-based components HCL Digital Asset Management, Content Composer, and Experience API is supported on Red Hat OpenShift with the HCL DX 9.5 CF19 release. It is not yet supported with an on-premises clustered environment. The HCL DX hybrid deployment model will be supported on additional Kubernetes platforms in later release updates. Virtual Portal support with Content Composer is not yet supported. Support for this component in a virtual portal deployment model will be available in later releases. Prerequisites HCL Digital Experience V9.5 CF19 or higher is deployed to supported on-premises platforms, in a standalone, cluster, or farm topology. See the Roadmaps to deploy your Digital Experience 9.5 system topic for more information. Practitioner Studio has been enabled in the Digital Experience 9.5 CF19 or higher installation. See the How to enable Practitioner Studio topic for instructions. A common domain, using an SSL connection, is established for both the on premise HCL DX 9.5 CF19 and higher on-premises environment, and the target Red Hat Open Shift platform deployment to contain cloud native components HCL DX Experience API, Digital Asset Management and Content Composer. For example, mytargetcloud.dx.com and myonprem.dx.com would have the same domain: dx.com. Single sign-on must be enabled on HCL DX 9.5 CF19 or higher on-premises environment. On DMGR or WAS Admin console under Security > Global Security > Web & SIP Security > Single Sign-On , Enabled is checked and Domain name is set to common domain. For example, dx.com. A high-performance network connection is established between the HCL DX 9.5 CF19 and higher on-premises environment, and the target DX Red Hat Open Shift platform deployment. dxctl tool. Volume Requirement : Requires an AccessMode of ReadWriteMany. Reference the Storage Class and Volume topic for more information. Ensure you have obtained a backup of the HCL DX 9.5 on-premises deployment. See the Backup and Restore topic for additional information. Enabling Hybrid Deployment support in the HCL Digital Experience 9.5 on-premises environment Follow the steps below to enable Hybrid deployment support in the HCL Digital Experience 9.5 on-premises environment. Access the latest HCL DX 9.5 CF19 or higher release software from the HCL Software License Portal . The package will include the file hcl-dx-cloud-scripts, which should be downloaded and extracted to your local system. This will present the following deploy, dxctl and scripts directories as shown below: Configure Properties of the dxctl tool. Navigate to the dxctl/properties directory and open the hybrid-deployment.properties file in your favorite editor. Sample properties for deploying Digital Asset Management, Content Composer, and the Experience API for use with an on-premise DX environment are shown below. Use following example to modify the values in the dxctl/properties/hybrid-deployment.properties file to reflect your target Red Hat Open Shift environment to deploy the Content Composer, Digital Asset Management, and the Experience API components. Note: For a Hybrid deployment, the following items should be enabled and/or not included in the deployment: Enable api.enabled(eAPI), composer.enabled(Content Composer) and/or dam.enabled(DAM) to true. Not included **dx.tag** This is not required for a hybrid deployment as DX Portal and WCM is installed on premises. **dx.database** This is not required for a hybrid deployment as it is installed with DX Portal and WCM on premises. See the Reference list below for explanations of the file items. dx.namespace: dxhybwin dx.config.cors: https://portal-dxhybwin.apps.dxdev.dx-dev.net:10042 hybrid.enabled: true hybrid.host: portal-dxhybwin.apps.dxdev.dx-dev.net hybrd.port: 10042 api.enabled: true api.tag: v1.3.0_20201019-1240_develop composer.enabled: true composer.tag: v1.3.0_20201019-1239_develop dam.enabled: true dam.volume: dxhybwin-dam dam.stgclass: dx-deploy-stg dam.tag: v1.3.0_20201019-1259_develop imgproc.tag: v1.3.0_20201015-1133_develop override.ingress.host: dx-hybrid-service-dxhybwin-dxhybwin.apps.hcl-dxdev.dx-dev.net dam.operator.tag: v95_CF19_20201020_dev_build Reference list : dx.namespace - The project or the namespace to create or use for deployment. dx.tag - Tag of the latest DX HCL DX 9.5 Portal and Web Content Manager image. dx.database - By default, and initially, this is Derby. HCL DX 9.5 supports Apache Derby, Oracle Database, IBM DB2, or Microsoft SQL Server. Acceptable values are derby , oracle , db2 , or msSql . dx.config.cors - URL of the on-premises HCL DX 9.5 CF19 or higher Portal and Web Content Manager server. hybrid.enabled - Boolean value to enable/disable. hybrid.host - URL of On-premises DX 9.5 Portal and Web Content Manager server without port. hybrd.port - Port of the on-premises HCL DX 9.5 CF19 or higher Portal and Web Content Manager server. api.enabled - Boolean value to enable/disable the Experience API. api.tag - Tag of the your Experience API image. composer.enabled - Boolean value to enable/disable Content Composer. composer.tag - Tag of the latest Content Composer image. dam.enabled - Boolean value to enable/disable Digital Asset Management. dam.volume - Volume for Digital Asset Management. dam.stgclass - Storage class for Digital Asset Management. dam.tag - Tag of your Digital Asset Management image. imgproc.tag - Tag of your Image-Processor image. override.ingress.host - Generated base URL of the route. dx.operator.tag - Tag of your hcldx-cloud-operator image. dam.operator.tag - Tag of your Docker hcl-dam-operator tag. Deploy using the dxctl tool Login to the Red Hat OpenShift platform using Kubectl. Log in to the target Red Hat OpenShift platform using the following OpenShift command: $ oc login Enter your username and password in response to system prompts. See the example below: Deploy using dxctl. On your local workstation, navigate to the dxctl folder: $ cd ./hcl-dx-cloud-scripts/dxctl Execute the below command to deploy from the local workstation to the target Red Hat OpenShift platform to contain the HCL DX 9.5 CF19 and higher cloud native components using the dxctl tool based on configured properties: From a MacOS workstation: MacBook-Pro:macuser$ ./mac/dxctl --deploy -p properties/hybrid-deployment.properties From a Windows workstation: Cmd> ./win/dxctl -\u2013deploy -p properties/hybrid-deployment.properties Note: On Windows 10 MD and PowerShell, rename the dxctl file to dxctl.exe and run the deploy command. From a Linux workstation: $ ./linux/dxctl \u2013deploy -p properties/hybrid-deployment.properties Validate the Deployment Login to the Red Hat OpenShift Console. Login to OpenShift Dashboard Console with your credentials: Once logged in, search for the HCL DX 9.5 project: Ensure the deployment and pods are running and in a ready state. Access the HCL Digital Experience 9.5 on-premises server. Access the host URL of the HCL Digital Experience 9.5 on-premises deployment, which is defined in the hybrid-deployment.properties file and add the path below to the URL: /wps/myportal. Login to the HCL DX 9.5 on-premises deployment using your appropriate credentials. Once logged in, click the Open Applications Menu in the right top corner. If the Web Content and Digital Assets menu items do not appear in the menu selections, follow the steps below to enable Content Composer and Digital Asset Management using the Configuration Engine. Enable Content Composer and Digital Asset Management Locate the Content Composer and Digital Asset Management URLs. The URLs will match the location values of the Red Hat OpenShift route of the component. Copy the Content Composer and Digital Asset Management URL from the Red Hat OpenShift dashboard console and add the suffix/static as in the example below: Content Composer \u2013 Content UI URL Example : https://dx-hybrid-service-dxhybwin-dxhybwin.apps.comp-dxdev.hcl-dx-dev.net/dx/ui/content/static Digital Asset Management UI URL Example : https://dx-hybrid-service-dxhybwin-dxhybwin.apps.comp-dxdev.hcl-dx-dev.net/dx/ui/dam/static Navigate to the ConfigEngine service. Connect/login to the HCL DX 9.5 on-premises system and open a command line prompt. Navigate to the following path on your deployment: C:\\> cd <path to wp_profile>\\ConfigEngine Enable Content Composer and Digital Asset Management to the Hybrid deployment. If the on-premises HCL DX 9.5 Portal and Web Content Manager deployment is installed to a Windows platform, execute: Enable Content Composer : <Path to wp_profile>\\ConfigEngine\\ConfigEngine.bat enable-headless-content -Dstatic.ui.url=CONTENT_UI_URL -DWasPassword=... -DPortalAdminPwd=... Note: In the command above, the CONTENT_UI_URL=Route URL is copied from the Red Hat OpenShift dashboard with /static . -DWasPassword=type is the IBM WebSphere Application Server password and -DPortalAdminPwd=type is the DX admin password. Enable Digital Asset Management : <Path to wp_profile>\\ConfigEngine\\ConfigEngine.bat enable-media-library -Dstatic.ui.url=DAM_URL -DWasPassword=... -DPortalAdminPwd=... Note: In the command above, the DAM_URL=Route URL is copied from the Red Hat OpenShift dashboard with /static . -DWasPassword=type is the IBM WebSphere Application Server password and -DPortalAdminPwd=type is the DX admin password. If the on-premises HCL DX 9.5 Portal and Web Content Manager deployment is installed to a Linux platform, execute: Enable Content Composer : <Path to wp_profile>\\ConfigEngine\\ConfigEngine.sh enable-headless-content -Dstatic.ui.url=CONTENT_UI_URL -DWasPassword=... -DPortalAdminPwd=... Note: In the command above, the CONTENT_UI_URL=Route URL is copied from the Red Hat OpenShift dashboard with /static . -DWasPassword=type is the IBM WebSphere Application Server password and -DPortalAdminPwd=type is the DX admin password. Enable Digital Asset Management : <Path to wp_profile>\\ConfigEngine\\ConfigEngine.sh enable-media-library -Dstatic.ui.url=DAM_URL -DWasPassword=... -DPortalAdminPwd=... Note: In the command above, the DAM_URL=Route URL is copied from the Red Hat OpenShift dashboard with /static . -DWasPassword=type is the IBM WebSphere Application Server password and -DPortalAdminPwd=type is the DX admin password. Once the above steps are completed, log in to the Digital Experience 9.5 deployment and validate the Web Content and Digital Assets menu items appear. Example :","title":"Hybrid deployment - Operator"},{"location":"containerization/hybrid_deployment_operator/#hybrid-deployment-operator","text":"This section describes how to install HCL Digital Experience 9.5 Portal Server and Web Content Manager services to on-premises platforms, operating with Digital Asset Management, Content Composer, Experience API deployed to cloud-based Kubernetes and OpenShift platforms deployed using the Operator (dxctl) method. Many organizations are using cloud and containerized deployments as part of their overall systems environments. In parallel, organizations continue to operate software applications and processes on-premises. The HCL Digital Experience 9.5 Hybrid deployment reference architecture and topics describe an approach to deploy and manage HCL Digital Experience 9.5 core Portal Server and Web Content Manager services on premises, and connect to cloud native components Digital Asset Management, Content Composer, Experience API and related services in a production environment. Notes: The hybrid deployment of cloud-based components HCL Digital Asset Management, Content Composer, and Experience API is supported on Red Hat OpenShift with the HCL DX 9.5 CF19 release. It is not yet supported with an on-premises clustered environment. The HCL DX hybrid deployment model will be supported on additional Kubernetes platforms in later release updates. Virtual Portal support with Content Composer is not yet supported. Support for this component in a virtual portal deployment model will be available in later releases.","title":"Hybrid deployment - Operator"},{"location":"containerization/hybrid_deployment_operator/#prerequisites","text":"HCL Digital Experience V9.5 CF19 or higher is deployed to supported on-premises platforms, in a standalone, cluster, or farm topology. See the Roadmaps to deploy your Digital Experience 9.5 system topic for more information. Practitioner Studio has been enabled in the Digital Experience 9.5 CF19 or higher installation. See the How to enable Practitioner Studio topic for instructions. A common domain, using an SSL connection, is established for both the on premise HCL DX 9.5 CF19 and higher on-premises environment, and the target Red Hat Open Shift platform deployment to contain cloud native components HCL DX Experience API, Digital Asset Management and Content Composer. For example, mytargetcloud.dx.com and myonprem.dx.com would have the same domain: dx.com. Single sign-on must be enabled on HCL DX 9.5 CF19 or higher on-premises environment. On DMGR or WAS Admin console under Security > Global Security > Web & SIP Security > Single Sign-On , Enabled is checked and Domain name is set to common domain. For example, dx.com. A high-performance network connection is established between the HCL DX 9.5 CF19 and higher on-premises environment, and the target DX Red Hat Open Shift platform deployment. dxctl tool. Volume Requirement : Requires an AccessMode of ReadWriteMany. Reference the Storage Class and Volume topic for more information. Ensure you have obtained a backup of the HCL DX 9.5 on-premises deployment. See the Backup and Restore topic for additional information.","title":"Prerequisites"},{"location":"containerization/hybrid_deployment_operator/#enabling-hybrid-deployment-support-in-the-hcl-digital-experience-95-on-premises-environment","text":"Follow the steps below to enable Hybrid deployment support in the HCL Digital Experience 9.5 on-premises environment. Access the latest HCL DX 9.5 CF19 or higher release software from the HCL Software License Portal . The package will include the file hcl-dx-cloud-scripts, which should be downloaded and extracted to your local system. This will present the following deploy, dxctl and scripts directories as shown below: Configure Properties of the dxctl tool. Navigate to the dxctl/properties directory and open the hybrid-deployment.properties file in your favorite editor. Sample properties for deploying Digital Asset Management, Content Composer, and the Experience API for use with an on-premise DX environment are shown below. Use following example to modify the values in the dxctl/properties/hybrid-deployment.properties file to reflect your target Red Hat Open Shift environment to deploy the Content Composer, Digital Asset Management, and the Experience API components. Note: For a Hybrid deployment, the following items should be enabled and/or not included in the deployment: Enable api.enabled(eAPI), composer.enabled(Content Composer) and/or dam.enabled(DAM) to true. Not included **dx.tag** This is not required for a hybrid deployment as DX Portal and WCM is installed on premises. **dx.database** This is not required for a hybrid deployment as it is installed with DX Portal and WCM on premises. See the Reference list below for explanations of the file items. dx.namespace: dxhybwin dx.config.cors: https://portal-dxhybwin.apps.dxdev.dx-dev.net:10042 hybrid.enabled: true hybrid.host: portal-dxhybwin.apps.dxdev.dx-dev.net hybrd.port: 10042 api.enabled: true api.tag: v1.3.0_20201019-1240_develop composer.enabled: true composer.tag: v1.3.0_20201019-1239_develop dam.enabled: true dam.volume: dxhybwin-dam dam.stgclass: dx-deploy-stg dam.tag: v1.3.0_20201019-1259_develop imgproc.tag: v1.3.0_20201015-1133_develop override.ingress.host: dx-hybrid-service-dxhybwin-dxhybwin.apps.hcl-dxdev.dx-dev.net dam.operator.tag: v95_CF19_20201020_dev_build Reference list : dx.namespace - The project or the namespace to create or use for deployment. dx.tag - Tag of the latest DX HCL DX 9.5 Portal and Web Content Manager image. dx.database - By default, and initially, this is Derby. HCL DX 9.5 supports Apache Derby, Oracle Database, IBM DB2, or Microsoft SQL Server. Acceptable values are derby , oracle , db2 , or msSql . dx.config.cors - URL of the on-premises HCL DX 9.5 CF19 or higher Portal and Web Content Manager server. hybrid.enabled - Boolean value to enable/disable. hybrid.host - URL of On-premises DX 9.5 Portal and Web Content Manager server without port. hybrd.port - Port of the on-premises HCL DX 9.5 CF19 or higher Portal and Web Content Manager server. api.enabled - Boolean value to enable/disable the Experience API. api.tag - Tag of the your Experience API image. composer.enabled - Boolean value to enable/disable Content Composer. composer.tag - Tag of the latest Content Composer image. dam.enabled - Boolean value to enable/disable Digital Asset Management. dam.volume - Volume for Digital Asset Management. dam.stgclass - Storage class for Digital Asset Management. dam.tag - Tag of your Digital Asset Management image. imgproc.tag - Tag of your Image-Processor image. override.ingress.host - Generated base URL of the route. dx.operator.tag - Tag of your hcldx-cloud-operator image. dam.operator.tag - Tag of your Docker hcl-dam-operator tag.","title":"Enabling Hybrid Deployment support in the HCL Digital Experience 9.5 on-premises environment"},{"location":"containerization/hybrid_deployment_operator/#deploy-using-the-dxctl-tool","text":"Login to the Red Hat OpenShift platform using Kubectl. Log in to the target Red Hat OpenShift platform using the following OpenShift command: $ oc login Enter your username and password in response to system prompts. See the example below: Deploy using dxctl. On your local workstation, navigate to the dxctl folder: $ cd ./hcl-dx-cloud-scripts/dxctl Execute the below command to deploy from the local workstation to the target Red Hat OpenShift platform to contain the HCL DX 9.5 CF19 and higher cloud native components using the dxctl tool based on configured properties: From a MacOS workstation: MacBook-Pro:macuser$ ./mac/dxctl --deploy -p properties/hybrid-deployment.properties From a Windows workstation: Cmd> ./win/dxctl -\u2013deploy -p properties/hybrid-deployment.properties Note: On Windows 10 MD and PowerShell, rename the dxctl file to dxctl.exe and run the deploy command. From a Linux workstation: $ ./linux/dxctl \u2013deploy -p properties/hybrid-deployment.properties","title":"Deploy using the dxctl tool"},{"location":"containerization/hybrid_deployment_operator/#validate-the-deployment","text":"Login to the Red Hat OpenShift Console. Login to OpenShift Dashboard Console with your credentials: Once logged in, search for the HCL DX 9.5 project: Ensure the deployment and pods are running and in a ready state. Access the HCL Digital Experience 9.5 on-premises server. Access the host URL of the HCL Digital Experience 9.5 on-premises deployment, which is defined in the hybrid-deployment.properties file and add the path below to the URL: /wps/myportal. Login to the HCL DX 9.5 on-premises deployment using your appropriate credentials. Once logged in, click the Open Applications Menu in the right top corner. If the Web Content and Digital Assets menu items do not appear in the menu selections, follow the steps below to enable Content Composer and Digital Asset Management using the Configuration Engine.","title":"Validate the Deployment"},{"location":"containerization/hybrid_deployment_operator/#enable-content-composer-and-digital-asset-management","text":"Locate the Content Composer and Digital Asset Management URLs. The URLs will match the location values of the Red Hat OpenShift route of the component. Copy the Content Composer and Digital Asset Management URL from the Red Hat OpenShift dashboard console and add the suffix/static as in the example below: Content Composer \u2013 Content UI URL Example : https://dx-hybrid-service-dxhybwin-dxhybwin.apps.comp-dxdev.hcl-dx-dev.net/dx/ui/content/static Digital Asset Management UI URL Example : https://dx-hybrid-service-dxhybwin-dxhybwin.apps.comp-dxdev.hcl-dx-dev.net/dx/ui/dam/static Navigate to the ConfigEngine service. Connect/login to the HCL DX 9.5 on-premises system and open a command line prompt. Navigate to the following path on your deployment: C:\\> cd <path to wp_profile>\\ConfigEngine Enable Content Composer and Digital Asset Management to the Hybrid deployment. If the on-premises HCL DX 9.5 Portal and Web Content Manager deployment is installed to a Windows platform, execute: Enable Content Composer : <Path to wp_profile>\\ConfigEngine\\ConfigEngine.bat enable-headless-content -Dstatic.ui.url=CONTENT_UI_URL -DWasPassword=... -DPortalAdminPwd=... Note: In the command above, the CONTENT_UI_URL=Route URL is copied from the Red Hat OpenShift dashboard with /static . -DWasPassword=type is the IBM WebSphere Application Server password and -DPortalAdminPwd=type is the DX admin password. Enable Digital Asset Management : <Path to wp_profile>\\ConfigEngine\\ConfigEngine.bat enable-media-library -Dstatic.ui.url=DAM_URL -DWasPassword=... -DPortalAdminPwd=... Note: In the command above, the DAM_URL=Route URL is copied from the Red Hat OpenShift dashboard with /static . -DWasPassword=type is the IBM WebSphere Application Server password and -DPortalAdminPwd=type is the DX admin password. If the on-premises HCL DX 9.5 Portal and Web Content Manager deployment is installed to a Linux platform, execute: Enable Content Composer : <Path to wp_profile>\\ConfigEngine\\ConfigEngine.sh enable-headless-content -Dstatic.ui.url=CONTENT_UI_URL -DWasPassword=... -DPortalAdminPwd=... Note: In the command above, the CONTENT_UI_URL=Route URL is copied from the Red Hat OpenShift dashboard with /static . -DWasPassword=type is the IBM WebSphere Application Server password and -DPortalAdminPwd=type is the DX admin password. Enable Digital Asset Management : <Path to wp_profile>\\ConfigEngine\\ConfigEngine.sh enable-media-library -Dstatic.ui.url=DAM_URL -DWasPassword=... -DPortalAdminPwd=... Note: In the command above, the DAM_URL=Route URL is copied from the Red Hat OpenShift dashboard with /static . -DWasPassword=type is the IBM WebSphere Application Server password and -DPortalAdminPwd=type is the DX admin password. Once the above steps are completed, log in to the Digital Experience 9.5 deployment and validate the Web Content and Digital Assets menu items appear. Example :","title":"Enable Content Composer and Digital Asset Management"},{"location":"containerization/install_config_cc_dam/","text":"Install the HCL Digital Experience 9.5 components This section provides a high-level overview of the architecture and the steps to install, configure, and update the HCL Digital Experience 9.5 components: Experience API, Content Composer, and Digital Asset Management. Video : Install HCL Digital Experience components (Experience API, Content Composer, and Digital Asset Management) on Red Hat OpenShift High-level architecture and topology Prerequisite DX Administrators can choose to install the DX Core containers then proceed to install Content Composer and Digital Asset Management containers to the supported Kubernetes container platforms as outlined in the following steps. See the Deployment section for the latest DX 9.5 container file listings. Deploying the HCL Digital Asset Management or Content Composer components is supported on Kubernetes or OpenShift platforms and is not supported for deployment to Docker platforms. See the System requirements section for more information and the latest updates. Note: For initial deployments, it is recommended to install the HCL Digital Experience 9.5 components (Experience API, Content Composer, and Digital Asset Management) to a non-production (test) HCL Digital Experience 9.5 environment. Installing the HCL Digital Experience 9.5 Container components Follow these steps to install your HCL Digital Experience 9.5 components (Experience API, Content Composer, and Digital Asset Management): Asset Management components If installing in conjunction with HCL Digital Experience 9.5 CF181 or higher, follow the instructions in the Container Deployment topic. This page lists the latest HCL Digital Experience 9.5 CF181 or higher product images available and how to obtain and load the images into your Docker repository before continuing with these instructions. If installing to an existing HCL Digital Experience 9.5 CF181 or higher Kubernetes environment: Verify that you can access the HCL Digital Experience 9.5 CF181 or higher Practitioner Studio by logging in to your HCL Digital Experience 9.5 Practitioner Studio interface. See the HCL Digital Experience 9.5 Practitioner Studio topic for information. Download and extract the HCL Digital Experience 9.5 components from your Digital Experience entitlements from the HCL Software License Portal to the local file system. Sample download package name : hcl-dx-kubernetes-v95-CF181-other.zip or higher, depending on the DX 9.5 Container Update version you are installing. Example : **hcl-dx-kubernetes-v95-CF181-other.zip:** HCL Experience API (Docker image) - hcl-dx-ringapi-image-v1.0.0_xxxxxxxx-xxxx.tar.gz Postgres - hcl-dx-postgres-image-v1.0.0_xxxxxxxx-xxxx.tar.gz HCL Digital Asset Management (Operator) - hcl-dx-medialibrary-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz HCL Digital Asset Management (Image processor) - hcl-dx-image-processor-image-v1.0.0_xxxxxxxx-xxxx.tar.gz HCL Digital Asset Management (Docker image) - hcl-dx-digital-asset-manager-image-v1.0.0_xxxxxxxx-xxxx.tar.gz HCL Content Composer (Docker image) - hcl-dx-content-composer-image-v1.0.0_xxxxxxxx-xxxx.tar.gz Extract the images to the local file system. Open a terminal window and change to the root directory of the extracted package images. Load the images into your Docker environment. Example: Docker load < hcl-dx-ringapi-image-v1.0.0_xxxxxxxx-xxxx.tar.gz Docker load < hcl-dx-medialibrary-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz Docker load < hcl-dx-digital-asset-manager-image-v1.0.0_xxxxxxxx-xxxx.tar.gz Docker load < hcl-dx-content-composer-image-v1.0.0_xxxxxxxx-xxxx.tar.gz Docker load < hcl-dx-image-processor-image-v1.0.0_xxxxxxxx-xxxx.tar.gz Docker load < hcl-dx-postgres-image-v1.0.0_xxxxxxxx-xxxx.tar.gz Note: Either -i or < works for the load command. In case you encounter an error when using one, try running the command using the other. Get the Docker images in your local Docker repository to your target Kubernetes system by tagging and pushing them appropriately. If you used docker load to get your images on the target environment, proceed to the next step. Install the HCL Digital Experience 9.5 CF181 or higher Experience API, Content Composer, and Digital Asset Management components by using the following steps. Container Update CF182 or higher is required if deploying to Microsoft Azure Kubernetes Service (AKS). Notes: The config map name value used must be the same as the HCL Digital Experience 9.5 CF181 and higher deployment. By default, the config map deployment name value is dx-deployment. The HCL Experience API must be installed to access and use the HCL Content Composer and the HCL Digital Asset Management features. Reminder : If you are currently running an HCL Digital Experience 9.5 CF181 or higher Kubernetes deployment in production, adding new components requires an outage and setup time so plan it carefully. If you are creating the dx-deployment config map, you can use the following content (adjusting the image tag values to match your environment) to create a YAML file and use a command line client to create the config map which is used to deploy the HCL Experience API, HCL Content Composer, and HCL Digital Asset Management CF181 and later components. Note: It is possible to deploy the services for the HCL Experience API and HCL Content Composer and/or Digital Asset Management, if either of those combinations is preferred, by removing either the HCL Content Composer or HCL Digital Asset Management service lines from the YAML file. Confirm your HCL Digital Experience 9.5 CF181 and higher container instance is up and running on Amazon EKS, Microsoft Azure (CF182 or higher), or Red Hat OpenShift platform. Note: If you are adding components for HCL Content Composer, HCL Digital Asset Management, and HCL Experience API to an existing HCL Digital Experience 9.5 environment (must be at level 9.5 CF181 or higher) deployment, you must stop the deployment and restart it with one (1) replica. Reminder : For an initial deployment, it is not advisable to deploy these components to a production HCL Digital Experience 9.5 deployment. Update the HCL Digital Experience 9.5 CF181 or higher container deployment configuration map to deploy the HCL Experience API, HCL Content Composer, and HCL Digital Asset Management CF181 and higher components. Note: The config map name value used to support the CF181 or higher components must be the same as the HCL Digital Experience 9.5 CF181 and higher deployment. By default, the config map deployment name value is dx-deployment . Create a YAML file with the following config map settings: kind: `ConfigMap` metadata: name: dx-deployment Use the following example YAML ( dx-deploy-config-map.yaml ) to deploy the HCL Experience API, HCL Content Composer, and HCL Digital Asset Management CF181 or higher components. If deploying HCL Content Composer and HCL Digital Asset Management CF181 components, replace their file names in the sample YAML file services lines used. Note: It is possible to deploy the services for the HCL Experience API and HCL Content Composer and/or Digital Asset Management by removing either the HCL Content Composer or HCL Digital Asset Management service lines from the YAML file. ``` kind: ConfigMap metadata: name: dx-deployment data: dx.deploy.dam.persistence.tag: v1.0.0_20200622-1806 dx.deploy.dam.persistence.image: portal/persistence/postgres dx.deploy.dam.volume: volume name dx.deploy.dam.imgprocessor.tag: v95_CF181_20200622-1550 dx.deploy.remotesearch.tag: v95_CF181_20200622-1550 dx.deploy.dam.imgprocessor.image: portal/image-processor dx.deploy.dam.storageclass: dx-deploy-stg dx.deploy.remotesearch.image: dxrs dx.deploy.openldap.tag: v1.0.0-release_20200622_1592846796 dx.deploy.openldap.image: dx-openldap dx.deploy.contentui.tag: v1.0.0_20200622-1709 dx.deploy.contentui.image: portal/content-ui dx.deploy.remotesearch.enabled: 'true' dx.deploy.dam.tag: v1.0.0_20200622-1718 dx.deploy.experienceapi.tag: v1.0.0_20200622-1719 dx.deploy.experienceapi.image: portal/api/ringapi dx.deploy.dam.image: portal/media-library dx.deploy.openldap.enabled: 'true' dx.deploy.contentui.enabled: 'true' dx.deploy.experienceapi.enabled: 'true' dx.deploy.dam.enabled: 'true' dx.deploy.dam.operator.tag: v95_CF181_20200622-1756 dx.deploy.dam.operator.image: hcl-medialibrary-operator dx.deploy.remotesearch.volume.storageclass: gp2 ``` Notes: The deployment of HCL Content Composer and HCL Experience API components create: The dx.deploy.contentui.enabled and dx.deploy.experienceapi.enabled configurations tell the operator to deploy HCL Content Composer and HCL Experience API components. This defaults to using the same repository as the HCL Digital Experience 9.5 CF181 or higher container core deployment. Services dx-deployment-service-content-ui and dx-deployment-service-ring-api , and a route for each. Administrators can override the repository by adding the following to the config map entries: dx.deploy.contentui.repository The dx.deploy.dam.enabled tells the operator to deploy the HCL Digital Asset Management component. Note that there are 4 required sets of image/tag parameters: The HCL Digital Asset Management operator component uses prefixdx.deploy.dam.operator . The Postgres datastore component uses dx.deploy.dam.persistence . The HCL Digital Asset Management library services use dx.deploy.dam . The Image processor uses dx.deploy.dam.persistence . To override the repository values for the components above, use dx.deploy.COMPONENT.repository The last two parameters in the example YAML file provide the storage class and volume (must be ReadWriteMany ) for the HCL Digital Asset Management component. This is where the persistence layer maintains the datastore layer. The dx.deploy.dam.volume: volume name setting is optional if the storage class used/specified by dx.deploy.dam.storageclass is self-provisioning. A dx.dam.config.cors config map setting is auto-generated and provides the ability for Cross Origin Resource Sharing across Content Composer and Digital Asset Management resources. In the Digital Experience 9.5 core deployment, the dx.config.cors setting is set in the DX configuration map. Reference the Containerization Deployment pages for additional details. An additional self-provisioning volume is created for each of the HCL Digital Asset Management Persistence (Postgres) pods. The access mode of these self-provisioning persistent volumes must include ReadWriteOnce . If this volume is not present the images are lost and shows blank if/when the HCL Digital Asset Management library is restarted. Administrators can override the repository by adding to: dx.deploy.contentui.repository In addition, the following default settings are configurable: dx.deploy.contentui.resources.cpurequest , the default is 1. dx.deploy.contentui.resources.cpulimit , the default is 3. dx.deploy.contentui.resources.memoryrequest , the default is 2G. dx.deploy.contentui.resources.memorylimit , the default is 4G. dx.deploy.experienceapi.resources.cpurequest , the default is 1. dx.deploy.experienceapi.resources.cpulimit , the default is 3. dx.deploy.experienceapi.resources.memoryrequest , the default is 2G. dx.deploy.experienceapi.resources.memorylimit , the default is 4G. Additional configuration options are currently not supported. Deploy the YAML ( dx-deploy-config-map.yaml ) by issuing the following: Kubernetes command: ``` kubectl apply -f dx-deploy-config-map.yaml -n your-namespace - OpenShift command: - ``` oc project your-namespace followed by - ``` oc apply -f dx-deploy-config-map.yaml ``` Stop and restart the HCL Digital Experience 9.5 CF181 and higher container deployment. Note: If you are adding components for HCL Content Composer, HCL Digital Asset Management, and HCL Experience API to an existing HCL Digital Experience 9.5 environment (must be at level 9.5 CF181 or higher) deployment, you must stop the deployment and restart it with one (1) replica. Once it is fully started, you can safely scale it to N instances. Reminder : As outlined in this section, adding new components to a production deployment requires an outage and some setup time. It is advisable to plan carefully if you are currently running a Digital Experience container deployment in a supported Kubernetes environment. Change to the extracted hcl-dx-cloud-scripts directory. ./scripts/removeDx.sh NAMESPACE Note: This script removes resources from the existing deployment (pods, statefulsets, etc) but does not remove persisted data or existing configmaps. Remove the claimRef from the PersistedVolume. Note: Instructions to re-use the Persistent Volume may also be viewed in the Deploy HCL Digital Experience 9.5 Container to Amazon EKS topic. Open the persistent volume in a visual editor (vi) using the Kubernetes or OpenShift command line client command: kubectl edit pv <pv name> or oc edit pv <pv name> Remove the claimRef section: claimRef: apiVersion: v1 kind: PersistentVolumeClaim name: dx-deploy-pvc namespace: awseks-demo resourceVersion: \"488931120\" uid: ebd58361-0e2a-11ea-b02e-02f8fe687954 Ensure you get the ' persistentvolume/your_namespace edited ' message. Change to the extracted hcl-dx-cloud-scripts directory. ./scripts/deployDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE or ./scripts/deployDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE INGRESSIMAGE INGRESSTAG Note: You must restart the deployment with one (1) replica. Once it is fully started, you can safely scale it to N instances. Reminder : As outlined in this section, adding new components to a production deployment requires an outage and some setup time. It is advisable to plan carefully if you are currently running a Digital Experience container deployment in a supported Kubernetes environment. Access the HCL Content Composer and HCL Digital Asset Management components by navigating to Practitioner Studio > Web Content > Content , or Practitioner Studio > Digital Assets . https://your-portal.net/wps/myportal/Practitioner/Web Content/Content Library https://your-portal.net/wps/myportal/Practitioner/Digital Assets Access the HCL Experience API Explorer at the following URL: http://<HOST>:<PORT>dx/api/core/v1/explorer For example, http://127.0.0.1:3000/dx/api/core/v1/explorer (Optional) Configure Digital Asset Management with a CDN If you are using a content delivery network (CDN) such as Akamai , using Vary: Origin may prevent you from caching content. To bypass this limitation, your configuration must strip the Vary header on the way in, to reinstate your ability to cache content. On the way out, you can append the Origin parameter to the Vary header when serving a response using 'Modify Outgoing Response Header' .","title":"Install the HCL Digital Experience 9.5 components"},{"location":"containerization/install_config_cc_dam/#install-the-hcl-digital-experience-95-components","text":"This section provides a high-level overview of the architecture and the steps to install, configure, and update the HCL Digital Experience 9.5 components: Experience API, Content Composer, and Digital Asset Management. Video : Install HCL Digital Experience components (Experience API, Content Composer, and Digital Asset Management) on Red Hat OpenShift","title":"Install the HCL Digital Experience 9.5 components"},{"location":"containerization/install_config_cc_dam/#high-level-architecture-and-topology","text":"","title":"High-level architecture and topology"},{"location":"containerization/install_config_cc_dam/#prerequisite","text":"DX Administrators can choose to install the DX Core containers then proceed to install Content Composer and Digital Asset Management containers to the supported Kubernetes container platforms as outlined in the following steps. See the Deployment section for the latest DX 9.5 container file listings. Deploying the HCL Digital Asset Management or Content Composer components is supported on Kubernetes or OpenShift platforms and is not supported for deployment to Docker platforms. See the System requirements section for more information and the latest updates. Note: For initial deployments, it is recommended to install the HCL Digital Experience 9.5 components (Experience API, Content Composer, and Digital Asset Management) to a non-production (test) HCL Digital Experience 9.5 environment.","title":"Prerequisite"},{"location":"containerization/install_config_cc_dam/#installing-the-hcl-digital-experience-95-container-components","text":"Follow these steps to install your HCL Digital Experience 9.5 components (Experience API, Content Composer, and Digital Asset Management): Asset Management components If installing in conjunction with HCL Digital Experience 9.5 CF181 or higher, follow the instructions in the Container Deployment topic. This page lists the latest HCL Digital Experience 9.5 CF181 or higher product images available and how to obtain and load the images into your Docker repository before continuing with these instructions. If installing to an existing HCL Digital Experience 9.5 CF181 or higher Kubernetes environment: Verify that you can access the HCL Digital Experience 9.5 CF181 or higher Practitioner Studio by logging in to your HCL Digital Experience 9.5 Practitioner Studio interface. See the HCL Digital Experience 9.5 Practitioner Studio topic for information. Download and extract the HCL Digital Experience 9.5 components from your Digital Experience entitlements from the HCL Software License Portal to the local file system. Sample download package name : hcl-dx-kubernetes-v95-CF181-other.zip or higher, depending on the DX 9.5 Container Update version you are installing. Example : **hcl-dx-kubernetes-v95-CF181-other.zip:** HCL Experience API (Docker image) - hcl-dx-ringapi-image-v1.0.0_xxxxxxxx-xxxx.tar.gz Postgres - hcl-dx-postgres-image-v1.0.0_xxxxxxxx-xxxx.tar.gz HCL Digital Asset Management (Operator) - hcl-dx-medialibrary-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz HCL Digital Asset Management (Image processor) - hcl-dx-image-processor-image-v1.0.0_xxxxxxxx-xxxx.tar.gz HCL Digital Asset Management (Docker image) - hcl-dx-digital-asset-manager-image-v1.0.0_xxxxxxxx-xxxx.tar.gz HCL Content Composer (Docker image) - hcl-dx-content-composer-image-v1.0.0_xxxxxxxx-xxxx.tar.gz Extract the images to the local file system. Open a terminal window and change to the root directory of the extracted package images. Load the images into your Docker environment. Example: Docker load < hcl-dx-ringapi-image-v1.0.0_xxxxxxxx-xxxx.tar.gz Docker load < hcl-dx-medialibrary-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz Docker load < hcl-dx-digital-asset-manager-image-v1.0.0_xxxxxxxx-xxxx.tar.gz Docker load < hcl-dx-content-composer-image-v1.0.0_xxxxxxxx-xxxx.tar.gz Docker load < hcl-dx-image-processor-image-v1.0.0_xxxxxxxx-xxxx.tar.gz Docker load < hcl-dx-postgres-image-v1.0.0_xxxxxxxx-xxxx.tar.gz Note: Either -i or < works for the load command. In case you encounter an error when using one, try running the command using the other. Get the Docker images in your local Docker repository to your target Kubernetes system by tagging and pushing them appropriately. If you used docker load to get your images on the target environment, proceed to the next step. Install the HCL Digital Experience 9.5 CF181 or higher Experience API, Content Composer, and Digital Asset Management components by using the following steps. Container Update CF182 or higher is required if deploying to Microsoft Azure Kubernetes Service (AKS). Notes: The config map name value used must be the same as the HCL Digital Experience 9.5 CF181 and higher deployment. By default, the config map deployment name value is dx-deployment. The HCL Experience API must be installed to access and use the HCL Content Composer and the HCL Digital Asset Management features. Reminder : If you are currently running an HCL Digital Experience 9.5 CF181 or higher Kubernetes deployment in production, adding new components requires an outage and setup time so plan it carefully. If you are creating the dx-deployment config map, you can use the following content (adjusting the image tag values to match your environment) to create a YAML file and use a command line client to create the config map which is used to deploy the HCL Experience API, HCL Content Composer, and HCL Digital Asset Management CF181 and later components. Note: It is possible to deploy the services for the HCL Experience API and HCL Content Composer and/or Digital Asset Management, if either of those combinations is preferred, by removing either the HCL Content Composer or HCL Digital Asset Management service lines from the YAML file. Confirm your HCL Digital Experience 9.5 CF181 and higher container instance is up and running on Amazon EKS, Microsoft Azure (CF182 or higher), or Red Hat OpenShift platform. Note: If you are adding components for HCL Content Composer, HCL Digital Asset Management, and HCL Experience API to an existing HCL Digital Experience 9.5 environment (must be at level 9.5 CF181 or higher) deployment, you must stop the deployment and restart it with one (1) replica. Reminder : For an initial deployment, it is not advisable to deploy these components to a production HCL Digital Experience 9.5 deployment. Update the HCL Digital Experience 9.5 CF181 or higher container deployment configuration map to deploy the HCL Experience API, HCL Content Composer, and HCL Digital Asset Management CF181 and higher components. Note: The config map name value used to support the CF181 or higher components must be the same as the HCL Digital Experience 9.5 CF181 and higher deployment. By default, the config map deployment name value is dx-deployment . Create a YAML file with the following config map settings: kind: `ConfigMap` metadata: name: dx-deployment Use the following example YAML ( dx-deploy-config-map.yaml ) to deploy the HCL Experience API, HCL Content Composer, and HCL Digital Asset Management CF181 or higher components. If deploying HCL Content Composer and HCL Digital Asset Management CF181 components, replace their file names in the sample YAML file services lines used. Note: It is possible to deploy the services for the HCL Experience API and HCL Content Composer and/or Digital Asset Management by removing either the HCL Content Composer or HCL Digital Asset Management service lines from the YAML file. ``` kind: ConfigMap metadata: name: dx-deployment data: dx.deploy.dam.persistence.tag: v1.0.0_20200622-1806 dx.deploy.dam.persistence.image: portal/persistence/postgres dx.deploy.dam.volume: volume name dx.deploy.dam.imgprocessor.tag: v95_CF181_20200622-1550 dx.deploy.remotesearch.tag: v95_CF181_20200622-1550 dx.deploy.dam.imgprocessor.image: portal/image-processor dx.deploy.dam.storageclass: dx-deploy-stg dx.deploy.remotesearch.image: dxrs dx.deploy.openldap.tag: v1.0.0-release_20200622_1592846796 dx.deploy.openldap.image: dx-openldap dx.deploy.contentui.tag: v1.0.0_20200622-1709 dx.deploy.contentui.image: portal/content-ui dx.deploy.remotesearch.enabled: 'true' dx.deploy.dam.tag: v1.0.0_20200622-1718 dx.deploy.experienceapi.tag: v1.0.0_20200622-1719 dx.deploy.experienceapi.image: portal/api/ringapi dx.deploy.dam.image: portal/media-library dx.deploy.openldap.enabled: 'true' dx.deploy.contentui.enabled: 'true' dx.deploy.experienceapi.enabled: 'true' dx.deploy.dam.enabled: 'true' dx.deploy.dam.operator.tag: v95_CF181_20200622-1756 dx.deploy.dam.operator.image: hcl-medialibrary-operator dx.deploy.remotesearch.volume.storageclass: gp2 ``` Notes: The deployment of HCL Content Composer and HCL Experience API components create: The dx.deploy.contentui.enabled and dx.deploy.experienceapi.enabled configurations tell the operator to deploy HCL Content Composer and HCL Experience API components. This defaults to using the same repository as the HCL Digital Experience 9.5 CF181 or higher container core deployment. Services dx-deployment-service-content-ui and dx-deployment-service-ring-api , and a route for each. Administrators can override the repository by adding the following to the config map entries: dx.deploy.contentui.repository The dx.deploy.dam.enabled tells the operator to deploy the HCL Digital Asset Management component. Note that there are 4 required sets of image/tag parameters: The HCL Digital Asset Management operator component uses prefixdx.deploy.dam.operator . The Postgres datastore component uses dx.deploy.dam.persistence . The HCL Digital Asset Management library services use dx.deploy.dam . The Image processor uses dx.deploy.dam.persistence . To override the repository values for the components above, use dx.deploy.COMPONENT.repository The last two parameters in the example YAML file provide the storage class and volume (must be ReadWriteMany ) for the HCL Digital Asset Management component. This is where the persistence layer maintains the datastore layer. The dx.deploy.dam.volume: volume name setting is optional if the storage class used/specified by dx.deploy.dam.storageclass is self-provisioning. A dx.dam.config.cors config map setting is auto-generated and provides the ability for Cross Origin Resource Sharing across Content Composer and Digital Asset Management resources. In the Digital Experience 9.5 core deployment, the dx.config.cors setting is set in the DX configuration map. Reference the Containerization Deployment pages for additional details. An additional self-provisioning volume is created for each of the HCL Digital Asset Management Persistence (Postgres) pods. The access mode of these self-provisioning persistent volumes must include ReadWriteOnce . If this volume is not present the images are lost and shows blank if/when the HCL Digital Asset Management library is restarted. Administrators can override the repository by adding to: dx.deploy.contentui.repository In addition, the following default settings are configurable: dx.deploy.contentui.resources.cpurequest , the default is 1. dx.deploy.contentui.resources.cpulimit , the default is 3. dx.deploy.contentui.resources.memoryrequest , the default is 2G. dx.deploy.contentui.resources.memorylimit , the default is 4G. dx.deploy.experienceapi.resources.cpurequest , the default is 1. dx.deploy.experienceapi.resources.cpulimit , the default is 3. dx.deploy.experienceapi.resources.memoryrequest , the default is 2G. dx.deploy.experienceapi.resources.memorylimit , the default is 4G. Additional configuration options are currently not supported. Deploy the YAML ( dx-deploy-config-map.yaml ) by issuing the following: Kubernetes command: ``` kubectl apply -f dx-deploy-config-map.yaml -n your-namespace - OpenShift command: - ``` oc project your-namespace followed by - ``` oc apply -f dx-deploy-config-map.yaml ``` Stop and restart the HCL Digital Experience 9.5 CF181 and higher container deployment. Note: If you are adding components for HCL Content Composer, HCL Digital Asset Management, and HCL Experience API to an existing HCL Digital Experience 9.5 environment (must be at level 9.5 CF181 or higher) deployment, you must stop the deployment and restart it with one (1) replica. Once it is fully started, you can safely scale it to N instances. Reminder : As outlined in this section, adding new components to a production deployment requires an outage and some setup time. It is advisable to plan carefully if you are currently running a Digital Experience container deployment in a supported Kubernetes environment. Change to the extracted hcl-dx-cloud-scripts directory. ./scripts/removeDx.sh NAMESPACE Note: This script removes resources from the existing deployment (pods, statefulsets, etc) but does not remove persisted data or existing configmaps. Remove the claimRef from the PersistedVolume. Note: Instructions to re-use the Persistent Volume may also be viewed in the Deploy HCL Digital Experience 9.5 Container to Amazon EKS topic. Open the persistent volume in a visual editor (vi) using the Kubernetes or OpenShift command line client command: kubectl edit pv <pv name> or oc edit pv <pv name> Remove the claimRef section: claimRef: apiVersion: v1 kind: PersistentVolumeClaim name: dx-deploy-pvc namespace: awseks-demo resourceVersion: \"488931120\" uid: ebd58361-0e2a-11ea-b02e-02f8fe687954 Ensure you get the ' persistentvolume/your_namespace edited ' message. Change to the extracted hcl-dx-cloud-scripts directory. ./scripts/deployDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE or ./scripts/deployDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE INGRESSIMAGE INGRESSTAG Note: You must restart the deployment with one (1) replica. Once it is fully started, you can safely scale it to N instances. Reminder : As outlined in this section, adding new components to a production deployment requires an outage and some setup time. It is advisable to plan carefully if you are currently running a Digital Experience container deployment in a supported Kubernetes environment. Access the HCL Content Composer and HCL Digital Asset Management components by navigating to Practitioner Studio > Web Content > Content , or Practitioner Studio > Digital Assets . https://your-portal.net/wps/myportal/Practitioner/Web Content/Content Library https://your-portal.net/wps/myportal/Practitioner/Digital Assets Access the HCL Experience API Explorer at the following URL: http://<HOST>:<PORT>dx/api/core/v1/explorer For example, http://127.0.0.1:3000/dx/api/core/v1/explorer","title":"Installing the HCL Digital Experience 9.5 Container components"},{"location":"containerization/install_config_cc_dam/#optional-configure-digital-asset-management-with-a-cdn","text":"If you are using a content delivery network (CDN) such as Akamai , using Vary: Origin may prevent you from caching content. To bypass this limitation, your configuration must strip the Vary header on the way in, to reinstate your ability to cache content. On the way out, you can append the Origin parameter to the Vary header when serving a response using 'Modify Outgoing Response Header' .","title":"(Optional) Configure Digital Asset Management with a CDN"},{"location":"containerization/kubernetes_remote_search/","text":"Configure Remote Search in OpenShift and Kubernetes This section shows how to configure Remote Search for your HCL Digital Experience 9.5 environments on supported Red Hat OpenShift and Kubernetes container platforms. Note: Prior to Container Update CF195, the HCL Digital Experience 9.5 Remote Search image is supported for deployment to Red Hat OpenShift. With the Container Update CF195 release, Remote Search can be configured for deployment on supported Kubernetes platforms. Introduction Using HCL Digital Experience 9.5 Remote Search images in the supported cloud container platforms, such as Red Hat OpenShift, require a different setup and configuration steps than those used to set up Remote Search on a non-Docker or Kubernetes container platform . As information, the serverindex.xml file on the Remote Search server when deployed to on-premises environments may have a host name that is not accurate in a container environment with respect to the actual host name of the server hosting the Remote Search server. Follow the guidance in this section to define collections in the core HCL DX 9.5 container environment with respect to JCR text search collections, rather than guidance published for the on-premises (non-Docker or Kubernetes) platforms for the JCR collection URL. Deploying Remote Search in HCL Digital Experience 9.5 OpenShift and Kubernetes platforms Prerequisite : Download the HCL Digital Experience 9.5 Docker containers from your HCL Digital Experience entitlements from the HCL Software License Portal . The HCL DX 9.5 container update CF181 and later packages include a core software and Remote search container. Load both of these images into an OpenShift release platform supported by HCL DX 9.5 such as Red Hat OpenShift. Use CF195 and later if you deploy to a Kubernetes platform. See the following Additional Routing Configuration for supported Kubernetes platforms topic for information about deploying to Kubernetes container platforms such as Amazon EKS, Azure AKS, or Google GKE. In this example, the OpenShift load command can be used. Note that if your organization has a corporate OpenShift repository, you might use OpenShift pull instead to put it into your local repository. hcl-dx-core-image-v95_CF181_xxxxxxxx-xxxx.tar.gz hcl-dx-dxrs-image-v95_CF181_xxxxxxxx-xxxx.tar.gz The first one (dx-core-image), is the core HCL DX 9.5 Portal and Web Content Manager image while the second one (dx-dxrs-image) is the remote search image. After the Remote Search images are loaded to the Kubernetes environment that you deploy to, follow deployment steps for that platform presented in the HCL Digital Experience 9.5 Container Deployment topic pages. EJBs and host names HCL Digital Experience 9.5 Container core and Portal Remote Search each use WebSphere Application Server as a base. As these components are on different hosts (containers), they need to communicate via IP. The initial conversation between HCL Digital Experience 9.5 core and the Remote Search server takes place over IIOP (rmi) which is the internet protocol of EJBs. Ideally, the /etc/hosts file of both containers would have the host name of the other. In other words, the /etc/hosts file of the HCL Digital Experience Container core would have a host reference for the Remote Search and vice versa. However, three factors make this impossible. The containers are based on Red Hat UBI, the /etc/hosts file is owned by root , and the root password (and sudo ) is not available. Apply the command below to define host references for the Remote Search service from the Digital Experience Container core. Therefore, a way to force the Kubernetes environment, such as Red Hat OpenShift to write the /etc/hosts file at container initialization time is needed. HCL DX 9.5 Container operators that execute image deployment to Kubernetes platforms such as Red Hat OpenShift create the correct host-name in /etc/hosts for the local container. In addition, these operators execute a DNS resolution on foreign host-names as long as they are on the same Kubernetes deployment. Portal and Portal Remote Search both use WebSphere Application Server as a base. As they are on different hosts (containers), they have to be able to talk to each other via IP. The initial conversation between Portal DX and the Remote Search server take place over IIOP (rmi) which is the internet protocol of EJBs. Defining serverindex.xml on the Remote Search server When deploying the Remote Search image on supported Kubernetes platforms, additional configuration settings for the serverindex.xml are required. When deployed to Kubernetes, the HCL DX 9.5 container operators are configured to check to ensure that the server name is correct. Note that dx-deployment-service-remotesearch is a DNS resolvable name from the point of view of the HCL Digital Experience 9.5 Server. The remote search server includes the \u201cping\u201d command. You can use this to verify that the host name dx-deployment-service-remotesearch resolves to a valid IP address. Now, when the HCL DX 9.5 server communicates to the Remote Search server over IIOP, the Remote Search Server returns dx-deployment-service-remotesearch as the host name of the Remote Search Server. The HCL DX 9.5 Server has configuration that appends the port to the host name that was just returned. Remote Search services configuration The following guidance aligns with the Remote Search services configuration instructions available in the Remote Search services topic for deployment to non-container HCL Digital Experience servers. All of the instructions contained in the Remote Search services topic must be completed in a Kubernetes container-based HCL Digital Experience deployment. The following guidance outlines specific settings that were used in the Remote Search service DX 9.5 image deployment to supported Kubernetes platforms. Create a single sign-on (SSO) domain between HCL Digital Experience 9.5 container and the Remote Search service container by following the non-container on-premises procedure for Creating a single sign-on domain between HCL Portal and the Remote Search service . This entails exchanging SSL certificates and LTPA domain tokens. Note: When retrieving the SSL certificates from the host server, use the URL configuration host as defined in the table below (dx-deployment-service-remotesearch) as the host, and the appropriate port for the SSL access. You must also complete Setting the search user ID and Removing search collections before creating a new search service. Create a new search service and use the following values for a Remote Search services configuration to a Kubernetes container deployment. See the section on Creating a new search service for more information. For testing Search Services configuration, the following are used: Item Value IIOP_URL iiop://dx-deployment-service-remotesearch:2809 PSE TYPE Select ejb from the pull down. EJB ejb/com/ibm/hrl/portlets/WsPse/WebScannerLiteEJBHome DefaultCollectionsDirectory Leave empty. Search service implementation Select Portal Search Service Type from the pull down. CONFIG_FOLDER_PATH Did not set (differs from non-container instructions). Note: Once completed and saved, the HCL Digital Experience 9.5 container deployment has a new search service called Remote PSE service EJB , with a green check mark confirming that the service was correctly set up and is able to communicate with the Remote Search container. Based on the previously created Remote Search service, create a Portal Search Collection and a JCR Search Collection using the following parameters. Use the following parameters to create a Portal search collection . Parameter Value Search collection name Portal Search Collection Search collection location /opt/HCL/AppServer/profiles/prs_profile/SearchCollections/PortalSearchCollection Note: The \u201csearch collection location\u201d is relative to the remote search container. Furthermore, one places the collection in the profile of the Remote Search server because the profile of the remote search server is persisted. One obviously wants the search indexes persisted across restarts. Use the following parameters to create a Content Source JCR search collection . The Collect documents linked from this URL is https://dx-deployment-service:10042/wps/seedlist/myserver?Source=com.ibm.lotus.search.plugins.seedlist.retriever.portal.PortalRetrieverFactory&Action=GetDocuments&Range=100&locale=en-US Note that the host and port are the Kubernetes (for example, Red Hat OpenShift) service host and the port to which 10042 was mapped. In this case, 10042 is the HttpQueueInboundDefaultSecure port on the HCL DX 9.5 Server. Note also that one can put this URL in a browser (on the OpenShift host) and confirm that the response is an ATOM feed. On the Security panel, use dx-deployment-service as the host name, along with the username wpsadmin and the associated password for wpsadmin . One can also specify Realm as CrawlerUsersRealm Complete the following configuration parameters to enable search in the Web Content Manager Authoring i interfaces: Parameter Value Search collection name JCRCollection1 Search collection location /opt/HCL/AppServer/profiles/prs_profile/SearchCollections/JCRCollection1 JCR Content Source Configuration Use the following URL for Collect documents linked from this URL : https://dx-deployment-service:10042/wps/seedlist/myserver?Action=GetDocuments&Format=ATOM&Locale=en_US&Range=100&Source=com.ibm.lotus.search.plugins.seedlist.retriever.jcr.JCRRetrieverFactory&Start=0&SeedlistId=1@OOTB_CRAWLER1 The parsing of the SeedlistId positional parameter in this URL uses an index of the virtual portal being crawled. In this case 1 (in 2 places) represents the base virtual portal. Note: The host and port are the Kubernetes (for example, Red Hat OpenShift ) service host and the port to which 10042 was mapped. In this case, 10042 is the HttpQueueInboundDefaultSecure port on the HCL DX 9.5 server. Note also that one can put this URL in a browser (on the OpenShift host) and confirm that the response is an ATOM feed. On the Security panel, use dx-deployment-service as the host name, along with the username wpsadmin and the associated password for wpsadmin . One can also specify Realm as CrawlerUsersRealm Configure WCM Authoring Portlet search function Note: Even though the documents are gathered by the Remote Search function from the JCR, additional configuration is needed in order for the HCL Web Content Manager (WCM) Authoring Portlet search to use document search. Set the following values for this configuration. Set the Custom properties for the WebSphere Application Server Resource Environment Provider, JCR ConfigService , using the following values: Property Value jcr.textsearch.enabled true jcr.textsearch.indexdirectory /opt/HCL/AppServer/profiles/prs_profile/SearchCollections jcr.textsearch.PSE.type ejb jcr.textsearch.EJB.IIOP.URL iiop://dx-deployment-service-remotesearch:2809 jcr.textsearch.EJB.EJBName ejb/com/ibm/hrl/portlets/WsPse/WebScannerLiteEJBHome Note: On the jcr.textsearch.indexdirectory , the sub-directory JCRCollection1 is NOT included in the path. Additional Routing Configuration for supported Kubernetes platforms To configure Remote Search to DX 9.5 container deployments to supported Kubernetes platforms: Amazon Elastic Kubernetes Service (EKS), Microsoft Azure Kubernetes Service (AKS), or Google Kubernetes Engine (GKE) requires specific steps. The HCL DX 9.5 core and Remote Search services each require /ibm/console as the route path when accessing the Admin Console. Due to the overlapping of the path mappings, to configure Remote Search, DX administrators can apply a solution to expose the Remote Search route via an additional Load balancer, as follows: Create a new service for Remote Search with service type as Loadbalancer. Note: Do not alter the Remote Search Service created by the DX-Operator. Sample Yaml: apiVersion: v1 kind: Service metadata: labels: app: dx-deployment-remotesearch release: dx-deployment name: dx-deployment-service-remotesearch-lb spec: ports: - name: was-admin port: 9060 protocol: TCP targetPort: 9060 - name: was-admin-sec port: 9043 protocol: TCP targetPort: 9043 - name: boot-port port: 2809 protocol: TCP targetPort: 2809 - name: rs-port port: 9403 protocol: TCP targetPort: 9403 selector: app: dx-deployment-remotesearch sessionAffinity: None type: LoadBalancer Apply this configuration using the following example command: $ kubectl apply -f filename.yaml Remote Search Routes (example results): Access the Remote Search Admin Console via the external IP address of your DX 9.5 Container deployment: Example: https://35.xxx.174.3:9043/ibm/console","title":"Configure Remote Search in OpenShift and Kubernetes"},{"location":"containerization/kubernetes_remote_search/#configure-remote-search-in-openshift-and-kubernetes","text":"This section shows how to configure Remote Search for your HCL Digital Experience 9.5 environments on supported Red Hat OpenShift and Kubernetes container platforms. Note: Prior to Container Update CF195, the HCL Digital Experience 9.5 Remote Search image is supported for deployment to Red Hat OpenShift. With the Container Update CF195 release, Remote Search can be configured for deployment on supported Kubernetes platforms.","title":"Configure Remote Search in OpenShift and Kubernetes"},{"location":"containerization/kubernetes_remote_search/#introduction","text":"Using HCL Digital Experience 9.5 Remote Search images in the supported cloud container platforms, such as Red Hat OpenShift, require a different setup and configuration steps than those used to set up Remote Search on a non-Docker or Kubernetes container platform . As information, the serverindex.xml file on the Remote Search server when deployed to on-premises environments may have a host name that is not accurate in a container environment with respect to the actual host name of the server hosting the Remote Search server. Follow the guidance in this section to define collections in the core HCL DX 9.5 container environment with respect to JCR text search collections, rather than guidance published for the on-premises (non-Docker or Kubernetes) platforms for the JCR collection URL.","title":"Introduction"},{"location":"containerization/kubernetes_remote_search/#deploying-remote-search-in-hcl-digital-experience-95-openshift-and-kubernetes-platforms","text":"Prerequisite : Download the HCL Digital Experience 9.5 Docker containers from your HCL Digital Experience entitlements from the HCL Software License Portal . The HCL DX 9.5 container update CF181 and later packages include a core software and Remote search container. Load both of these images into an OpenShift release platform supported by HCL DX 9.5 such as Red Hat OpenShift. Use CF195 and later if you deploy to a Kubernetes platform. See the following Additional Routing Configuration for supported Kubernetes platforms topic for information about deploying to Kubernetes container platforms such as Amazon EKS, Azure AKS, or Google GKE. In this example, the OpenShift load command can be used. Note that if your organization has a corporate OpenShift repository, you might use OpenShift pull instead to put it into your local repository. hcl-dx-core-image-v95_CF181_xxxxxxxx-xxxx.tar.gz hcl-dx-dxrs-image-v95_CF181_xxxxxxxx-xxxx.tar.gz The first one (dx-core-image), is the core HCL DX 9.5 Portal and Web Content Manager image while the second one (dx-dxrs-image) is the remote search image. After the Remote Search images are loaded to the Kubernetes environment that you deploy to, follow deployment steps for that platform presented in the HCL Digital Experience 9.5 Container Deployment topic pages.","title":"Deploying Remote Search in HCL Digital Experience 9.5 OpenShift and Kubernetes platforms"},{"location":"containerization/kubernetes_remote_search/#ejbs-and-host-names","text":"HCL Digital Experience 9.5 Container core and Portal Remote Search each use WebSphere Application Server as a base. As these components are on different hosts (containers), they need to communicate via IP. The initial conversation between HCL Digital Experience 9.5 core and the Remote Search server takes place over IIOP (rmi) which is the internet protocol of EJBs. Ideally, the /etc/hosts file of both containers would have the host name of the other. In other words, the /etc/hosts file of the HCL Digital Experience Container core would have a host reference for the Remote Search and vice versa. However, three factors make this impossible. The containers are based on Red Hat UBI, the /etc/hosts file is owned by root , and the root password (and sudo ) is not available. Apply the command below to define host references for the Remote Search service from the Digital Experience Container core. Therefore, a way to force the Kubernetes environment, such as Red Hat OpenShift to write the /etc/hosts file at container initialization time is needed. HCL DX 9.5 Container operators that execute image deployment to Kubernetes platforms such as Red Hat OpenShift create the correct host-name in /etc/hosts for the local container. In addition, these operators execute a DNS resolution on foreign host-names as long as they are on the same Kubernetes deployment. Portal and Portal Remote Search both use WebSphere Application Server as a base. As they are on different hosts (containers), they have to be able to talk to each other via IP. The initial conversation between Portal DX and the Remote Search server take place over IIOP (rmi) which is the internet protocol of EJBs.","title":"EJBs and host names"},{"location":"containerization/kubernetes_remote_search/#defining-serverindexxml-on-the-remote-search-server","text":"When deploying the Remote Search image on supported Kubernetes platforms, additional configuration settings for the serverindex.xml are required. When deployed to Kubernetes, the HCL DX 9.5 container operators are configured to check to ensure that the server name is correct. Note that dx-deployment-service-remotesearch is a DNS resolvable name from the point of view of the HCL Digital Experience 9.5 Server. The remote search server includes the \u201cping\u201d command. You can use this to verify that the host name dx-deployment-service-remotesearch resolves to a valid IP address. Now, when the HCL DX 9.5 server communicates to the Remote Search server over IIOP, the Remote Search Server returns dx-deployment-service-remotesearch as the host name of the Remote Search Server. The HCL DX 9.5 Server has configuration that appends the port to the host name that was just returned.","title":"Defining serverindex.xml on the Remote Search server"},{"location":"containerization/kubernetes_remote_search/#remote-search-services-configuration","text":"The following guidance aligns with the Remote Search services configuration instructions available in the Remote Search services topic for deployment to non-container HCL Digital Experience servers. All of the instructions contained in the Remote Search services topic must be completed in a Kubernetes container-based HCL Digital Experience deployment. The following guidance outlines specific settings that were used in the Remote Search service DX 9.5 image deployment to supported Kubernetes platforms. Create a single sign-on (SSO) domain between HCL Digital Experience 9.5 container and the Remote Search service container by following the non-container on-premises procedure for Creating a single sign-on domain between HCL Portal and the Remote Search service . This entails exchanging SSL certificates and LTPA domain tokens. Note: When retrieving the SSL certificates from the host server, use the URL configuration host as defined in the table below (dx-deployment-service-remotesearch) as the host, and the appropriate port for the SSL access. You must also complete Setting the search user ID and Removing search collections before creating a new search service. Create a new search service and use the following values for a Remote Search services configuration to a Kubernetes container deployment. See the section on Creating a new search service for more information. For testing Search Services configuration, the following are used: Item Value IIOP_URL iiop://dx-deployment-service-remotesearch:2809 PSE TYPE Select ejb from the pull down. EJB ejb/com/ibm/hrl/portlets/WsPse/WebScannerLiteEJBHome DefaultCollectionsDirectory Leave empty. Search service implementation Select Portal Search Service Type from the pull down. CONFIG_FOLDER_PATH Did not set (differs from non-container instructions). Note: Once completed and saved, the HCL Digital Experience 9.5 container deployment has a new search service called Remote PSE service EJB , with a green check mark confirming that the service was correctly set up and is able to communicate with the Remote Search container. Based on the previously created Remote Search service, create a Portal Search Collection and a JCR Search Collection using the following parameters. Use the following parameters to create a Portal search collection . Parameter Value Search collection name Portal Search Collection Search collection location /opt/HCL/AppServer/profiles/prs_profile/SearchCollections/PortalSearchCollection Note: The \u201csearch collection location\u201d is relative to the remote search container. Furthermore, one places the collection in the profile of the Remote Search server because the profile of the remote search server is persisted. One obviously wants the search indexes persisted across restarts. Use the following parameters to create a Content Source JCR search collection . The Collect documents linked from this URL is https://dx-deployment-service:10042/wps/seedlist/myserver?Source=com.ibm.lotus.search.plugins.seedlist.retriever.portal.PortalRetrieverFactory&Action=GetDocuments&Range=100&locale=en-US Note that the host and port are the Kubernetes (for example, Red Hat OpenShift) service host and the port to which 10042 was mapped. In this case, 10042 is the HttpQueueInboundDefaultSecure port on the HCL DX 9.5 Server. Note also that one can put this URL in a browser (on the OpenShift host) and confirm that the response is an ATOM feed. On the Security panel, use dx-deployment-service as the host name, along with the username wpsadmin and the associated password for wpsadmin . One can also specify Realm as CrawlerUsersRealm Complete the following configuration parameters to enable search in the Web Content Manager Authoring i interfaces: Parameter Value Search collection name JCRCollection1 Search collection location /opt/HCL/AppServer/profiles/prs_profile/SearchCollections/JCRCollection1","title":"Remote Search services configuration"},{"location":"containerization/kubernetes_remote_search/#jcr-content-source-configuration","text":"Use the following URL for Collect documents linked from this URL : https://dx-deployment-service:10042/wps/seedlist/myserver?Action=GetDocuments&Format=ATOM&Locale=en_US&Range=100&Source=com.ibm.lotus.search.plugins.seedlist.retriever.jcr.JCRRetrieverFactory&Start=0&SeedlistId=1@OOTB_CRAWLER1 The parsing of the SeedlistId positional parameter in this URL uses an index of the virtual portal being crawled. In this case 1 (in 2 places) represents the base virtual portal. Note: The host and port are the Kubernetes (for example, Red Hat OpenShift ) service host and the port to which 10042 was mapped. In this case, 10042 is the HttpQueueInboundDefaultSecure port on the HCL DX 9.5 server. Note also that one can put this URL in a browser (on the OpenShift host) and confirm that the response is an ATOM feed. On the Security panel, use dx-deployment-service as the host name, along with the username wpsadmin and the associated password for wpsadmin . One can also specify Realm as CrawlerUsersRealm","title":"JCR Content Source Configuration"},{"location":"containerization/kubernetes_remote_search/#configure-wcm-authoring-portlet-search-function","text":"Note: Even though the documents are gathered by the Remote Search function from the JCR, additional configuration is needed in order for the HCL Web Content Manager (WCM) Authoring Portlet search to use document search. Set the following values for this configuration. Set the Custom properties for the WebSphere Application Server Resource Environment Provider, JCR ConfigService , using the following values: Property Value jcr.textsearch.enabled true jcr.textsearch.indexdirectory /opt/HCL/AppServer/profiles/prs_profile/SearchCollections jcr.textsearch.PSE.type ejb jcr.textsearch.EJB.IIOP.URL iiop://dx-deployment-service-remotesearch:2809 jcr.textsearch.EJB.EJBName ejb/com/ibm/hrl/portlets/WsPse/WebScannerLiteEJBHome Note: On the jcr.textsearch.indexdirectory , the sub-directory JCRCollection1 is NOT included in the path.","title":"Configure WCM Authoring Portlet search function"},{"location":"containerization/kubernetes_remote_search/#additional-routing-configuration-for-supported-kubernetes-platforms","text":"To configure Remote Search to DX 9.5 container deployments to supported Kubernetes platforms: Amazon Elastic Kubernetes Service (EKS), Microsoft Azure Kubernetes Service (AKS), or Google Kubernetes Engine (GKE) requires specific steps. The HCL DX 9.5 core and Remote Search services each require /ibm/console as the route path when accessing the Admin Console. Due to the overlapping of the path mappings, to configure Remote Search, DX administrators can apply a solution to expose the Remote Search route via an additional Load balancer, as follows: Create a new service for Remote Search with service type as Loadbalancer. Note: Do not alter the Remote Search Service created by the DX-Operator. Sample Yaml: apiVersion: v1 kind: Service metadata: labels: app: dx-deployment-remotesearch release: dx-deployment name: dx-deployment-service-remotesearch-lb spec: ports: - name: was-admin port: 9060 protocol: TCP targetPort: 9060 - name: was-admin-sec port: 9043 protocol: TCP targetPort: 9043 - name: boot-port port: 2809 protocol: TCP targetPort: 2809 - name: rs-port port: 9403 protocol: TCP targetPort: 9403 selector: app: dx-deployment-remotesearch sessionAffinity: None type: LoadBalancer Apply this configuration using the following example command: $ kubectl apply -f filename.yaml Remote Search Routes (example results): Access the Remote Search Admin Console via the external IP address of your DX 9.5 Container deployment: Example: https://35.xxx.174.3:9043/ibm/console","title":"Additional Routing Configuration for supported Kubernetes platforms"},{"location":"containerization/limitations_requirements/","text":"Containerization requirements and limitations This section describes the requirements to deploy the HCL Digital Experience 9.5 images to container platforms and current limitations. Consult the HCL Digital Experience 9.5 Support Statements on the HCL Digital Experience Support pages for the latest updates on supported platforms, components, and release levels. Requirements and Limitations using dxctl The following describes the requirements to deploy the HCL Digital Experience 9.5 images to container platforms and current limitations using the dxctl deployment process: HCL Digital Experience 9.5 is supported on Docker, Red Hat OpenShift, Amazon Elastic Kubernetes Service (EKS), and Microsoft Azure Kubernetes Service (AKS), and Google Kubernetes Engine (GKE). Other Kubernetes platforms are not fully supported. The HCL Operator is not likely to work, however, support for additional Kubernetes as a Service (KaaS) is ongoing and additions is reflected in the HCL Digital Experience 9.5 Support Statements. Additional features and functions may be tied to the use of the HCL DX Operators for deployment. HCL highly recommends following the deployment strategies outlined within this documentation. HCL Digital Experience 9.5 containerization is focused on deployment and it uses an operator-based deployment. The goals are: To introduce a supported containerized deployment that HCL can continually extend; To provide customers with the best possible experience; To provide a high level of customization in the deployment and continue to expand on that, along with increased automation; and To maintain separation of product and custom code. Customers need to follow the recommended deployment model to ensure the availability of future functions and prevent potential conflicts. Notes: HCL Digital Experience is a database-intensive application, it is not recommended to use Apache Derby for production use. For specific versions of databases supported for production, see the HCL Digital Experience 9.5 Support Statements on the HCL Digital Experience Support pages. Creation of Virtual Portals take longer when implemented in Red Hat OpenShift. Plan for adequate time to allow processing, and re-verify the results are completed by refreshing the web browser administrative panel. Customers should not modify the HCL Digital Experience 9.5 Docker images provided by HCL for deployment. This restriction includes use of these images as a base to create a new image, which results in a new image ID and an unsupported configuration. Instead, customers deploying the images should follow best practices and maintain customizations in the wp_profile and the deployment database. Scripts and custom files should be stored in wp_profile (/opt/HCL/wp_profile/). See the Deployment Help Center topics for more information Customers should not run multiple HCL Digital Experience 9.5 container deployments in a single Kubernetes namespace (in the case of Red Hat OpenShift, in a single OpenShift project). This configuration is not supported at this time. It is not supported to run two different versions of HCL Digital Experience 9.5 container deployments in a single Kubernetes cluster. Use of Web Application Bridge is currently unsupported on HCL Digital Experience 9.5 deployments to container platforms such as Kubernetes and Red Hat OpenShift, using the Operator (dxctl) deployment method. Beginning with HCL DX Container Update CF199, Web Application Bridge can be used in container deployments using the Helm deployment method. Supported file system requirements : Requires an **AccessMode** of **ReadWriteMany** . Requires a minimum of 40 GB , with the default request set to 100 GB . Note: HCL Digital Experience is input-output (I/O) intensive and requires a high performing file system for optimization. Container platform capacity requirements : The following table outlines the minimum and maximum capacity requested and managed by HCL DX 9.5 Container Components: Component Pod minimum CPU Pod maximum CPU Pod minimum memory Pod maximum memory No. of minimum pods DX 9.5 Core 2 5 6 GB 8 GB 1 Experience API 0.5 1 1 GB 2 GB 1 Content Composer 0.5 1 1 GB 2 GB 1 Digital Asset Management 0.5 2 1 GB 2 GB 3 Persistence 1 2 1 GB 3 GB 1 Image processor 1 2 2 GB 2 GB 1 Remote search 1 3 1 GB 4 GB 1 Operators Shared - minimal Shared - minimal Shared - minimal Shared - minimal 2 Ambassador 0.3 1 400 MB 600 MB 3 Redis 0.3 1 400 MB 600 MB 3 Postgres-RO 1 2 1 GB 3 GB 1 Additional considerations in implementation : ConfigEngine and ConfigWizard should only be used when there is a single instance When more than one instance is running, the ConfigEngine is disabled and the ConfigWizard route is removed. As an example, the Site Builder is calling the ConfigEngine in the background. But because multiple instances are running, an Error 500 occurs because the ConfigEngine is disabled. AllConfigEngine.sh tasks should be run in configure mode with only one instance running. JavaServer Faces (JSF) portlet bridge With DX 9.5 Container Update CF171 and higher, WebSphere Application Server 9.0.5.2 is included and that IBM fix pack removed the IBM JSF portlet bridge. If you are using JSF portlets and leverage the JSF portlet bridge, proceed to the HCL DX 9.5 Container Update CF18 for the required JavaServer Faces Bridge support before moving to a container-based deployment. The HCL JavaServer Faces Bridge is added to HCL Digital Experience offerings with Container Update CF18 and CF18 on-premises platform CF update. For more information please see What's New in Container Update CF18 . Note: For information about the limitations related to JSF 2.2 support, see Limitations when running HCL DX Portlet Bridge on WebSphere Application Server 9.0 . Requirements and Limitations using Helm This section describes requirements and current limitations for an HCL Digital Experience 9.5 CF196 and later deployment to Kubernetes or Red Hat OpenShift, with HCL DX 9.5 Container Update CF196 and later using Helm. Note: Deployment of new HCL DX 9.5 CF196 deployments supported on Google Kubernetes Engine (GKE) only. Beginning with HCL Digital Experience CF197 and later releases, the Helm deployment pattern is supported for new deployments to Red Hat OpenShift , Amazon Elastic Kubernetes Service (EKS) , and Microsoft Azure Kubernetes Service (AKS) . For HCL Digital Experience 9.5 deployment to supported Kubernetes or OpenShift platforms deployment using Helm, the following are required: Helm installation : Download and install Helm to your target environment. HCL DX 9.5 CF196 and later container deployment is tested and is supported with Helm v3.5.4. For more information and to download Helm, visit the Helm documentation . Deployment using Helm to supported Kubernetes platforms : With DX 9.5 Container Update CF196, deployment has been tested and is supported to Kubernetes v1.19 and v1.20, using Google Kubernetes Engine (GKE). Beginning with HCL Digital Experience CF197 and later releases, the Helm deployment pattern is supported for new deployments to Red Hat OpenShift , Amazon Elastic Kubernetes Service (EKS) , and Microsoft Azure Kubernetes Service (AKS) . See the HCL DX supported hardware and software statements on the HCL Support Knowledge Base for additional supported platform details. Container platform capacity resource requirements : The following table outlines the default minimum and maximum capacity of container resources requested by the HCL DX 9.5 Container Components deployed to Kubernetes and Red Hat OpenShift via Helm. Component Pod Minimum CPU Pod Maximum CPU Pod Minimum Memory Pod Maximum Memory No. of Pods Minimum DX 9.5 Core 2 5 6GB 8GB 1 Ring API 0.5 1 1GB 2GB 1 Content Composer 0.5 1 1GB 2GB 1 Digital Asset Management 0.5 2 1GB 2GB 1 Digital Asset Management Persistence 1 2 1GB 4GB 1 Image processor 1 2 2GB 2GB 1 Open LDAP 0.5 2 500MB 2GB 1 Remote search 1 3 2GB 4GB 1 Runtime Controller 0.1 0.5 256MB 500MB 1 Ambassador 0.2 1 300MB 600MB 3 Redis 0.1 0.5 256MB 500MB 3 Limitations : The following limitations exist for an HCL Digital Experience 9.5 Container Update CF196 deployment using Helm on Google Kubernetes Engine (GKE), or to Red Hat OpenShift , Amazon Elastic Kubernetes Service (EKS) , or Microsoft Azure Kubernetes Service (AKS) with Container Update CF197 and later. Migration Using HCL DX 9.5 Container Updates CF196 and CF197, there is no supported process to migrate an existing Operator-based DX 9.5 container deployment to a Helm-based one. Helm is only supported as a mechanism for new HCL DX 9.5 Container Update CF196 deployments on Google Kubernetes Engine (GKE), Red Hat OpenShift , Amazon Elastic Kubernetes Service (EKS) , or Microsoft Azure Kubernetes Service (AKS) with Container Update CF197 and later. Version upgrade Currently, upgrading of a DX 9.5 Container Update release prior to CF196 on Google Kubernetes Engine (GKE) is not supported in Helm-based deployments. Helm is only supported as a mechanism for new deployments. Beginning with HCL DX 9.5 Container Update CF197, updating the version of DX 9.5 container deployment is supported for the Google Kubernetes Engine (GKE) platform only. See Update deployment to a later version for additional guidance.","title":"Containerization requirements and limitations"},{"location":"containerization/limitations_requirements/#containerization-requirements-and-limitations","text":"This section describes the requirements to deploy the HCL Digital Experience 9.5 images to container platforms and current limitations. Consult the HCL Digital Experience 9.5 Support Statements on the HCL Digital Experience Support pages for the latest updates on supported platforms, components, and release levels.","title":"Containerization requirements and limitations"},{"location":"containerization/limitations_requirements/#requirements-and-limitations-using-dxctl","text":"The following describes the requirements to deploy the HCL Digital Experience 9.5 images to container platforms and current limitations using the dxctl deployment process: HCL Digital Experience 9.5 is supported on Docker, Red Hat OpenShift, Amazon Elastic Kubernetes Service (EKS), and Microsoft Azure Kubernetes Service (AKS), and Google Kubernetes Engine (GKE). Other Kubernetes platforms are not fully supported. The HCL Operator is not likely to work, however, support for additional Kubernetes as a Service (KaaS) is ongoing and additions is reflected in the HCL Digital Experience 9.5 Support Statements. Additional features and functions may be tied to the use of the HCL DX Operators for deployment. HCL highly recommends following the deployment strategies outlined within this documentation. HCL Digital Experience 9.5 containerization is focused on deployment and it uses an operator-based deployment. The goals are: To introduce a supported containerized deployment that HCL can continually extend; To provide customers with the best possible experience; To provide a high level of customization in the deployment and continue to expand on that, along with increased automation; and To maintain separation of product and custom code. Customers need to follow the recommended deployment model to ensure the availability of future functions and prevent potential conflicts. Notes: HCL Digital Experience is a database-intensive application, it is not recommended to use Apache Derby for production use. For specific versions of databases supported for production, see the HCL Digital Experience 9.5 Support Statements on the HCL Digital Experience Support pages. Creation of Virtual Portals take longer when implemented in Red Hat OpenShift. Plan for adequate time to allow processing, and re-verify the results are completed by refreshing the web browser administrative panel. Customers should not modify the HCL Digital Experience 9.5 Docker images provided by HCL for deployment. This restriction includes use of these images as a base to create a new image, which results in a new image ID and an unsupported configuration. Instead, customers deploying the images should follow best practices and maintain customizations in the wp_profile and the deployment database. Scripts and custom files should be stored in wp_profile (/opt/HCL/wp_profile/). See the Deployment Help Center topics for more information Customers should not run multiple HCL Digital Experience 9.5 container deployments in a single Kubernetes namespace (in the case of Red Hat OpenShift, in a single OpenShift project). This configuration is not supported at this time. It is not supported to run two different versions of HCL Digital Experience 9.5 container deployments in a single Kubernetes cluster. Use of Web Application Bridge is currently unsupported on HCL Digital Experience 9.5 deployments to container platforms such as Kubernetes and Red Hat OpenShift, using the Operator (dxctl) deployment method. Beginning with HCL DX Container Update CF199, Web Application Bridge can be used in container deployments using the Helm deployment method. Supported file system requirements : Requires an **AccessMode** of **ReadWriteMany** . Requires a minimum of 40 GB , with the default request set to 100 GB . Note: HCL Digital Experience is input-output (I/O) intensive and requires a high performing file system for optimization. Container platform capacity requirements : The following table outlines the minimum and maximum capacity requested and managed by HCL DX 9.5 Container Components: Component Pod minimum CPU Pod maximum CPU Pod minimum memory Pod maximum memory No. of minimum pods DX 9.5 Core 2 5 6 GB 8 GB 1 Experience API 0.5 1 1 GB 2 GB 1 Content Composer 0.5 1 1 GB 2 GB 1 Digital Asset Management 0.5 2 1 GB 2 GB 3 Persistence 1 2 1 GB 3 GB 1 Image processor 1 2 2 GB 2 GB 1 Remote search 1 3 1 GB 4 GB 1 Operators Shared - minimal Shared - minimal Shared - minimal Shared - minimal 2 Ambassador 0.3 1 400 MB 600 MB 3 Redis 0.3 1 400 MB 600 MB 3 Postgres-RO 1 2 1 GB 3 GB 1 Additional considerations in implementation : ConfigEngine and ConfigWizard should only be used when there is a single instance When more than one instance is running, the ConfigEngine is disabled and the ConfigWizard route is removed. As an example, the Site Builder is calling the ConfigEngine in the background. But because multiple instances are running, an Error 500 occurs because the ConfigEngine is disabled. AllConfigEngine.sh tasks should be run in configure mode with only one instance running. JavaServer Faces (JSF) portlet bridge With DX 9.5 Container Update CF171 and higher, WebSphere Application Server 9.0.5.2 is included and that IBM fix pack removed the IBM JSF portlet bridge. If you are using JSF portlets and leverage the JSF portlet bridge, proceed to the HCL DX 9.5 Container Update CF18 for the required JavaServer Faces Bridge support before moving to a container-based deployment. The HCL JavaServer Faces Bridge is added to HCL Digital Experience offerings with Container Update CF18 and CF18 on-premises platform CF update. For more information please see What's New in Container Update CF18 . Note: For information about the limitations related to JSF 2.2 support, see Limitations when running HCL DX Portlet Bridge on WebSphere Application Server 9.0 .","title":"Requirements and Limitations using dxctl"},{"location":"containerization/limitations_requirements/#requirements-and-limitations-using-helm","text":"This section describes requirements and current limitations for an HCL Digital Experience 9.5 CF196 and later deployment to Kubernetes or Red Hat OpenShift, with HCL DX 9.5 Container Update CF196 and later using Helm. Note: Deployment of new HCL DX 9.5 CF196 deployments supported on Google Kubernetes Engine (GKE) only. Beginning with HCL Digital Experience CF197 and later releases, the Helm deployment pattern is supported for new deployments to Red Hat OpenShift , Amazon Elastic Kubernetes Service (EKS) , and Microsoft Azure Kubernetes Service (AKS) . For HCL Digital Experience 9.5 deployment to supported Kubernetes or OpenShift platforms deployment using Helm, the following are required: Helm installation : Download and install Helm to your target environment. HCL DX 9.5 CF196 and later container deployment is tested and is supported with Helm v3.5.4. For more information and to download Helm, visit the Helm documentation . Deployment using Helm to supported Kubernetes platforms : With DX 9.5 Container Update CF196, deployment has been tested and is supported to Kubernetes v1.19 and v1.20, using Google Kubernetes Engine (GKE). Beginning with HCL Digital Experience CF197 and later releases, the Helm deployment pattern is supported for new deployments to Red Hat OpenShift , Amazon Elastic Kubernetes Service (EKS) , and Microsoft Azure Kubernetes Service (AKS) . See the HCL DX supported hardware and software statements on the HCL Support Knowledge Base for additional supported platform details. Container platform capacity resource requirements : The following table outlines the default minimum and maximum capacity of container resources requested by the HCL DX 9.5 Container Components deployed to Kubernetes and Red Hat OpenShift via Helm. Component Pod Minimum CPU Pod Maximum CPU Pod Minimum Memory Pod Maximum Memory No. of Pods Minimum DX 9.5 Core 2 5 6GB 8GB 1 Ring API 0.5 1 1GB 2GB 1 Content Composer 0.5 1 1GB 2GB 1 Digital Asset Management 0.5 2 1GB 2GB 1 Digital Asset Management Persistence 1 2 1GB 4GB 1 Image processor 1 2 2GB 2GB 1 Open LDAP 0.5 2 500MB 2GB 1 Remote search 1 3 2GB 4GB 1 Runtime Controller 0.1 0.5 256MB 500MB 1 Ambassador 0.2 1 300MB 600MB 3 Redis 0.1 0.5 256MB 500MB 3 Limitations : The following limitations exist for an HCL Digital Experience 9.5 Container Update CF196 deployment using Helm on Google Kubernetes Engine (GKE), or to Red Hat OpenShift , Amazon Elastic Kubernetes Service (EKS) , or Microsoft Azure Kubernetes Service (AKS) with Container Update CF197 and later. Migration Using HCL DX 9.5 Container Updates CF196 and CF197, there is no supported process to migrate an existing Operator-based DX 9.5 container deployment to a Helm-based one. Helm is only supported as a mechanism for new HCL DX 9.5 Container Update CF196 deployments on Google Kubernetes Engine (GKE), Red Hat OpenShift , Amazon Elastic Kubernetes Service (EKS) , or Microsoft Azure Kubernetes Service (AKS) with Container Update CF197 and later. Version upgrade Currently, upgrading of a DX 9.5 Container Update release prior to CF196 on Google Kubernetes Engine (GKE) is not supported in Helm-based deployments. Helm is only supported as a mechanism for new deployments. Beginning with HCL DX 9.5 Container Update CF197, updating the version of DX 9.5 container deployment is supported for the Google Kubernetes Engine (GKE) platform only. See Update deployment to a later version for additional guidance.","title":"Requirements and Limitations using Helm"},{"location":"containerization/openshift/","text":"Deploy DX 9.5 Container to Red Hat OpenShift Learn how to deploy HCL Digital Experience (DX) 9.5 to Red Hat OpenShift platform.","title":"Deploy DX 9.5 Container to Red Hat OpenShift"},{"location":"containerization/openshift/#deploy-dx-95-container-to-red-hat-openshift","text":"Learn how to deploy HCL Digital Experience (DX) 9.5 to Red Hat OpenShift platform.","title":"Deploy DX 9.5 Container to Red Hat OpenShift"},{"location":"containerization/Amazon%20EKS/customizing_kubernetes_eks_deployment/","text":"Customizing the Kubernetes EKS deployment This section describes how to customize your HCL Portal deployment. About this task Follow this procedure to deploy or update your HCL Portal deployment. DX 9.5 containerization is focused on deployment and it uses an operator-based deployment. Goals To introduce a supported containerized deployment that HCL Digital Experience can continually extend to provide customers with the best possible experience. To provide a high level of customization in the deployment and continue to expand on that, along with increased automation. Before you begin Customers need to follow the recommended deployment model to ensure the availability of future functions and prevent potential conflicts. Customizing the deployment requires updating the deploy/crds/git_v1_dxdeployment_cr.yaml file located in the hcl-dx-cloud-scripts/deploy/crds directory in the hcl-dx-kubernetes-v95-CF184 package downloaded from the HCL Software Licensing Portal. Once modified, the deployDx.sh or the updateDx.sh scripts should be run to perform (or update) the target deployment. Note: All modifications should be made to the custom resource instance and not the individual parts of the deployment. Procedure Create a backup of the git_v1_dxdeployment_cr.yaml file. Open the original file in edit mode. Find the line with the text labeled, # Add fields here . Customizations should be done below this line. Add the following customizations as applicable: Volume Size By default, the volume size is 100 GB . This can be modified by changing the following: Note: The volume name and storageClassName should not be modified here. Resources By default, the resource requests are set at **2** CPU and **7G** RAM. These values can be changed. It is recommended to adjust the server heap size before changing these values. Note: Limits are not enforced in the initial 9.5 release. Probes The default readiness and liveness probes run against the ../ibm/console. This can and should be overridden. Notes: There are two types of checks: **command** runs a command against the server **http** hits either an http or an https URL. The syntax and required fields are shown in the above image. Logging By default, logging is done on the shared profile so all instances are writing to a single set of logs, with the volume set for each instance at **1G** . For diagnosing production issues this is not ideal. This option allows each instance to write the log to its own log directory. Notes: The environment must have a self-provisioning storage class provisioner. **Enabled** must be set to **true** . Adjusting the log settings must be done to prevent running out of disk storage. Ports By default, the deployment uses the default DX ports. The routes in these ports expose Portal through http and https . Note: If there is a need to configure the containerized Portal to use different ports, the defaults can be overwritten. Once modified, the deployDx.sh and the updateDx.sh scripts should be run to create (or update) the target deployment.","title":"Customizing the Kubernetes EKS deployment"},{"location":"containerization/Amazon%20EKS/customizing_kubernetes_eks_deployment/#customizing-the-kubernetes-eks-deployment","text":"This section describes how to customize your HCL Portal deployment.","title":"Customizing the Kubernetes EKS deployment"},{"location":"containerization/Amazon%20EKS/customizing_kubernetes_eks_deployment/#about-this-task","text":"Follow this procedure to deploy or update your HCL Portal deployment. DX 9.5 containerization is focused on deployment and it uses an operator-based deployment. Goals To introduce a supported containerized deployment that HCL Digital Experience can continually extend to provide customers with the best possible experience. To provide a high level of customization in the deployment and continue to expand on that, along with increased automation.","title":"About this task"},{"location":"containerization/Amazon%20EKS/customizing_kubernetes_eks_deployment/#before-you-begin","text":"Customers need to follow the recommended deployment model to ensure the availability of future functions and prevent potential conflicts. Customizing the deployment requires updating the deploy/crds/git_v1_dxdeployment_cr.yaml file located in the hcl-dx-cloud-scripts/deploy/crds directory in the hcl-dx-kubernetes-v95-CF184 package downloaded from the HCL Software Licensing Portal. Once modified, the deployDx.sh or the updateDx.sh scripts should be run to perform (or update) the target deployment. Note: All modifications should be made to the custom resource instance and not the individual parts of the deployment.","title":"Before you begin"},{"location":"containerization/Amazon%20EKS/customizing_kubernetes_eks_deployment/#procedure","text":"Create a backup of the git_v1_dxdeployment_cr.yaml file. Open the original file in edit mode. Find the line with the text labeled, # Add fields here . Customizations should be done below this line. Add the following customizations as applicable: Volume Size By default, the volume size is 100 GB . This can be modified by changing the following: Note: The volume name and storageClassName should not be modified here. Resources By default, the resource requests are set at **2** CPU and **7G** RAM. These values can be changed. It is recommended to adjust the server heap size before changing these values. Note: Limits are not enforced in the initial 9.5 release. Probes The default readiness and liveness probes run against the ../ibm/console. This can and should be overridden. Notes: There are two types of checks: **command** runs a command against the server **http** hits either an http or an https URL. The syntax and required fields are shown in the above image. Logging By default, logging is done on the shared profile so all instances are writing to a single set of logs, with the volume set for each instance at **1G** . For diagnosing production issues this is not ideal. This option allows each instance to write the log to its own log directory. Notes: The environment must have a self-provisioning storage class provisioner. **Enabled** must be set to **true** . Adjusting the log settings must be done to prevent running out of disk storage. Ports By default, the deployment uses the default DX ports. The routes in these ports expose Portal through http and https . Note: If there is a need to configure the containerized Portal to use different ports, the defaults can be overwritten. Once modified, the deployDx.sh and the updateDx.sh scripts should be run to create (or update) the target deployment.","title":"Procedure"},{"location":"containerization/Amazon%20EKS/kubernetes_eks/","text":"Deploy DX Container to Amazon EKS Learn how to deploy, find, understand, and customize the different releases of HCL Digital Experience 9.5 containers, along with Ambassador to Kubernetes, as verified in Amazon Elastic Kubernetes Service (Amazon EKS) .","title":"Deploy DX Container to Amazon EKS"},{"location":"containerization/Amazon%20EKS/kubernetes_eks/#deploy-dx-container-to-amazon-eks","text":"Learn how to deploy, find, understand, and customize the different releases of HCL Digital Experience 9.5 containers, along with Ambassador to Kubernetes, as verified in Amazon Elastic Kubernetes Service (Amazon EKS) .","title":"Deploy DX Container to Amazon EKS"},{"location":"containerization/Amazon%20EKS/kubernetes_eks_cf191andearlier/","text":"Deploy DX CF191 and earlier release Containers to Amazon EKS This section describes how to deploy HCL Digital Experience 9.5 CF191 and earlier release containers, along with the Ambassador, to Kubernetes as verified in Amazon Elastic Kubernetes Service (Amazon EKS) . Prerequisites Prior to using the procedure below, it is assumed that the HCL DX Administrator is generally experienced in using Kubernetes. Additionally, the DX Administrator must have the appropriate access to the target environment. If not, following are some preliminary steps that need to be taken. Setup KUBECONFIG to refer to the target server. This will ensure any kubectl commands executed locally affect the target environment. Amazon EKS Cluster The following tools must be installed on a machine other than the Portal server: Docker AWS Command Line Interface (CLI) - used to get image details. Volume requirement: Requires an AccessMode of ReadWriteMany Requires a minimum of 40 GB , with the default request set to 100 GB RECLAIM POLICY = Retain For DAM, additional volume is needed. For more details on storage class and volume, see Sample storage class and volume . Note: HCL Digital Experience is input-output (I/O) intensive which requires a high performing file system for optimization; NFS is an option for this. Hardware: 4 cores / 7 GB - - - 5 cores / 9 GB Amazon Elastic Container Registry (ECR) or any container registry access, for tagging and pushing. See the support topic for the HCL Digital Experience detailed system requirements . Download and extract the contents of the HCL DX 9.5 package to the local (local to cloud or system). If deploying HCL Digital Experience 9.5 Container Update release, the image and package names are as follows: CF183-core.zip files: HCL DX notices V9.5 CF183.txt hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF183_20200819-1711.tar.gz hcl-dx-cloud-scripts-v95_CF183_20200819-1711.zip hcl-dx-core-image-v95_CF183_20200819-1159.tar.gz hcl-dx-redis-image-5.0.1.tar.gz Note: Images included in the \u2018other\u2019 package released with CF183 are optional and used to support use of OpenLDAP, Remote Search, the Experience API, Content Composer, and Digital Asset Management components and services. About this task Follow these steps to deploy HCL Digital Experience 9.5 CF183 and higher container release, along with Ambassador to Kubernetes, as verified in Amazon Elastic Kubernetes Service (Amazon EKS) . This deployment relies heavily on Kubernetes Operators for full functionality. Note: Reference the latest HCL DX 9.5 Container Release and Update file listings in the Docker deployment topic. DX Container Management Change directory Open a terminal window and change to the root directory of the extracted package. Docker load Load the containers into your Docker repository: ``` docker load < hcl-dx-core-image-v95_CF183_20200819-1159.tar.gz - ``` docker load < hcl-dx-ambassador-image-154.tar.gz - ``` docker load < hcl-dx-cloud-operator-image-v95_CF183_20200819-1711.tar.gz - ``` docker load < hcl-dx-redis-image-5.0.1.tar.gz **Note:** Either **`-i`** or **`<`** works for the load command. In case you encounter an error when using one, try running the command using the other. ![](../images/container_eks_load_01.png \"Loading containers into your Docker repository\") Docker tag and push Get the Docker images in your local Docker repository to your target Kubernetes system by tagging and pushing them appropriately. If you used docker load to get your images on the target environment, proceed to the next step. ``` docker tag - ``` docker push <image:tag> DX Deployment Unzip Extract the HCL DX deployment scripts onto your environment as follows: unzip hcl-dx-cloud-scripts-v95_CF183_20200819-1711.zip Change directory Change to the extracted files directory, cd hcl-dx-cloud-scripts Custom resource definition Install the DxDeployment custom resource definition. Do not modify the git_v1_dxdeployment_crd.yaml file. Customize ./deploy/crds/git_v1_dxdeployment_cr.yaml, if required Use either of the following commands: ``` kubectl create -f hcl-dx-cloud-scripts/deploy/crds/git_v1_dxdeployment_crd.yaml - ``` ./scripts/deployCrd.sh Persistence volume Create (or have the Kubernetes Administrator create) a persistent volume and storage class where the AccessMode must be ReadWriteMany and the persistent volume reclaim policy must be Retain . For more details on storage class and volume, see Sample storage class and volume NFS server Provide the HCL DX 9.5 CF183 and higher Docker image access to the volume mount created in order to copy the profile. There are various ways to do this and NFS is one option. If NFS is used, here are the parameters that have been tested to work: rw, (Default) sync , (Default after NFS 1.0, means that the server does not reply until after the commit) insecure ,** (Requires requests originate on ports less than 1024) root_squash ,** (Map requests to the nobody user). hard ,** (Required because this means the system will keep trying to write until it works.) nfsvers=4.1 , rsize=8388608 , (Avoids dropped packages, default 8192) wsize=8388608 , (Avoids dropped packages, default 8192) timeo=600 , (60 seconds) retrans=2 , (Number of retries after a time out) noresvport ** (Tells the NFS client to use a new Transmission Control Protocol (TCP) source port when a network connection is reestablished. Doing this helps make sure that the EFS file system has uninterrupted availability after a network recovery event.) Note: Those marked with ** are critical and, in many cases, it is recommended to have the rsize and wsize set to 8388608 . Update YAML Replace the REPOSITORY NAME , IMAGE TAG , AMBASSADOR , and REDIS values in operator.yaml Deploy Create the DX container deployment. Run the deployDx.sh script to create the namespace, install the project scoped service account, role, role binding, operator, and deployment. ./scripts/deployDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE INGRESSIMAGE INGRESSTAG NAMESPACE - the project or the namespace to create or use for deployment. This namespace will be used in subsequent commands. REPLICAS - the number of initial instances for the deployment. REPOSITORY - your local repository, the repository used by Kubernetes. IMAGENAME - the name of the dxCore image, as added to the repository above. IMAGETAG - the tag for the target image as added to the repository above. VOLUMENAME - the volume to be used by the deployment for persistence, this must use AccessMode ReadWriteMany . STORAGECLASS - the storage class name used to create the persistent volume. DBTYPE - the database type. By default, and initially, this is Derby. HCL DX 9.5 uses Apache Derby, Oracle Database, IBM DB2, or Microsoft SQL Server. Acceptable values are derby , oracle , db2 , or msSql . INGRESSIMAGE - The image name to use for ambassador (Native K8s). INGRESSTAG - The image tag to use for ambassador (Native K8s). For example: ``` $cd hcl-dx-cloud-scripts $scripts/deployDx.sh dx-11 1 REPO_NAME dxen v95_CF183_20200818-1342 dx-pv-11 dx-deploy-stg derby ambassador 154 ``` Generate TLS Certificate Create a TLS certification to be used by the deployment: For development purposes: Using OpenSSL, you can create a private key: openssl genrsa -out my-key.pem 2048 Using OpenSSL, you can create a certificate signed by the private key: openssl req -x509 -key my-key.pem -out my-cert.pem -days 365 -subj '/CN=my-cert Create a TLS certification: kubectl create secret tls dx-tls-cert --cert=my-cert.pem --key=my-key.pem -n aws-mynamespace Note: The default name is the dx-tls-cert this can be changed in the configuration. aws-mynamespace is your Kubernetes namespace. You can set your preferred namespace but you must consistently use this namespace in subsequent commands. See Customizing the Kubernetes deployment . Final Output External IP from Load balancer in the below example can be used to access PORTAL Output $ kubectl get all -n NAMESPACE https://EXTERNAL_IP/wps/portal","title":"Deploy DX CF191 and earlier release Containers to Amazon EKS"},{"location":"containerization/Amazon%20EKS/kubernetes_eks_cf191andearlier/#deploy-dx-cf191-and-earlier-release-containers-to-amazon-eks","text":"This section describes how to deploy HCL Digital Experience 9.5 CF191 and earlier release containers, along with the Ambassador, to Kubernetes as verified in Amazon Elastic Kubernetes Service (Amazon EKS) .","title":"Deploy DX CF191 and earlier release Containers to Amazon EKS"},{"location":"containerization/Amazon%20EKS/kubernetes_eks_cf191andearlier/#prerequisites","text":"Prior to using the procedure below, it is assumed that the HCL DX Administrator is generally experienced in using Kubernetes. Additionally, the DX Administrator must have the appropriate access to the target environment. If not, following are some preliminary steps that need to be taken. Setup KUBECONFIG to refer to the target server. This will ensure any kubectl commands executed locally affect the target environment. Amazon EKS Cluster The following tools must be installed on a machine other than the Portal server: Docker AWS Command Line Interface (CLI) - used to get image details. Volume requirement: Requires an AccessMode of ReadWriteMany Requires a minimum of 40 GB , with the default request set to 100 GB RECLAIM POLICY = Retain For DAM, additional volume is needed. For more details on storage class and volume, see Sample storage class and volume . Note: HCL Digital Experience is input-output (I/O) intensive which requires a high performing file system for optimization; NFS is an option for this. Hardware: 4 cores / 7 GB - - - 5 cores / 9 GB Amazon Elastic Container Registry (ECR) or any container registry access, for tagging and pushing. See the support topic for the HCL Digital Experience detailed system requirements . Download and extract the contents of the HCL DX 9.5 package to the local (local to cloud or system). If deploying HCL Digital Experience 9.5 Container Update release, the image and package names are as follows: CF183-core.zip files: HCL DX notices V9.5 CF183.txt hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF183_20200819-1711.tar.gz hcl-dx-cloud-scripts-v95_CF183_20200819-1711.zip hcl-dx-core-image-v95_CF183_20200819-1159.tar.gz hcl-dx-redis-image-5.0.1.tar.gz Note: Images included in the \u2018other\u2019 package released with CF183 are optional and used to support use of OpenLDAP, Remote Search, the Experience API, Content Composer, and Digital Asset Management components and services.","title":"Prerequisites"},{"location":"containerization/Amazon%20EKS/kubernetes_eks_cf191andearlier/#about-this-task","text":"Follow these steps to deploy HCL Digital Experience 9.5 CF183 and higher container release, along with Ambassador to Kubernetes, as verified in Amazon Elastic Kubernetes Service (Amazon EKS) . This deployment relies heavily on Kubernetes Operators for full functionality. Note: Reference the latest HCL DX 9.5 Container Release and Update file listings in the Docker deployment topic.","title":"About this task"},{"location":"containerization/Amazon%20EKS/kubernetes_eks_cf191andearlier/#dx-container-management","text":"Change directory Open a terminal window and change to the root directory of the extracted package. Docker load Load the containers into your Docker repository: ``` docker load < hcl-dx-core-image-v95_CF183_20200819-1159.tar.gz - ``` docker load < hcl-dx-ambassador-image-154.tar.gz - ``` docker load < hcl-dx-cloud-operator-image-v95_CF183_20200819-1711.tar.gz - ``` docker load < hcl-dx-redis-image-5.0.1.tar.gz **Note:** Either **`-i`** or **`<`** works for the load command. In case you encounter an error when using one, try running the command using the other. ![](../images/container_eks_load_01.png \"Loading containers into your Docker repository\") Docker tag and push Get the Docker images in your local Docker repository to your target Kubernetes system by tagging and pushing them appropriately. If you used docker load to get your images on the target environment, proceed to the next step. ``` docker tag - ``` docker push <image:tag>","title":"DX Container Management"},{"location":"containerization/Amazon%20EKS/kubernetes_eks_cf191andearlier/#dx-deployment","text":"Unzip Extract the HCL DX deployment scripts onto your environment as follows: unzip hcl-dx-cloud-scripts-v95_CF183_20200819-1711.zip Change directory Change to the extracted files directory, cd hcl-dx-cloud-scripts Custom resource definition Install the DxDeployment custom resource definition. Do not modify the git_v1_dxdeployment_crd.yaml file. Customize ./deploy/crds/git_v1_dxdeployment_cr.yaml, if required Use either of the following commands: ``` kubectl create -f hcl-dx-cloud-scripts/deploy/crds/git_v1_dxdeployment_crd.yaml - ``` ./scripts/deployCrd.sh Persistence volume Create (or have the Kubernetes Administrator create) a persistent volume and storage class where the AccessMode must be ReadWriteMany and the persistent volume reclaim policy must be Retain . For more details on storage class and volume, see Sample storage class and volume NFS server Provide the HCL DX 9.5 CF183 and higher Docker image access to the volume mount created in order to copy the profile. There are various ways to do this and NFS is one option. If NFS is used, here are the parameters that have been tested to work: rw, (Default) sync , (Default after NFS 1.0, means that the server does not reply until after the commit) insecure ,** (Requires requests originate on ports less than 1024) root_squash ,** (Map requests to the nobody user). hard ,** (Required because this means the system will keep trying to write until it works.) nfsvers=4.1 , rsize=8388608 , (Avoids dropped packages, default 8192) wsize=8388608 , (Avoids dropped packages, default 8192) timeo=600 , (60 seconds) retrans=2 , (Number of retries after a time out) noresvport ** (Tells the NFS client to use a new Transmission Control Protocol (TCP) source port when a network connection is reestablished. Doing this helps make sure that the EFS file system has uninterrupted availability after a network recovery event.) Note: Those marked with ** are critical and, in many cases, it is recommended to have the rsize and wsize set to 8388608 . Update YAML Replace the REPOSITORY NAME , IMAGE TAG , AMBASSADOR , and REDIS values in operator.yaml Deploy Create the DX container deployment. Run the deployDx.sh script to create the namespace, install the project scoped service account, role, role binding, operator, and deployment. ./scripts/deployDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE INGRESSIMAGE INGRESSTAG NAMESPACE - the project or the namespace to create or use for deployment. This namespace will be used in subsequent commands. REPLICAS - the number of initial instances for the deployment. REPOSITORY - your local repository, the repository used by Kubernetes. IMAGENAME - the name of the dxCore image, as added to the repository above. IMAGETAG - the tag for the target image as added to the repository above. VOLUMENAME - the volume to be used by the deployment for persistence, this must use AccessMode ReadWriteMany . STORAGECLASS - the storage class name used to create the persistent volume. DBTYPE - the database type. By default, and initially, this is Derby. HCL DX 9.5 uses Apache Derby, Oracle Database, IBM DB2, or Microsoft SQL Server. Acceptable values are derby , oracle , db2 , or msSql . INGRESSIMAGE - The image name to use for ambassador (Native K8s). INGRESSTAG - The image tag to use for ambassador (Native K8s). For example: ``` $cd hcl-dx-cloud-scripts $scripts/deployDx.sh dx-11 1 REPO_NAME dxen v95_CF183_20200818-1342 dx-pv-11 dx-deploy-stg derby ambassador 154 ```","title":"DX Deployment"},{"location":"containerization/Amazon%20EKS/kubernetes_eks_cf191andearlier/#generate-tls-certificate","text":"Create a TLS certification to be used by the deployment: For development purposes: Using OpenSSL, you can create a private key: openssl genrsa -out my-key.pem 2048 Using OpenSSL, you can create a certificate signed by the private key: openssl req -x509 -key my-key.pem -out my-cert.pem -days 365 -subj '/CN=my-cert Create a TLS certification: kubectl create secret tls dx-tls-cert --cert=my-cert.pem --key=my-key.pem -n aws-mynamespace Note: The default name is the dx-tls-cert this can be changed in the configuration. aws-mynamespace is your Kubernetes namespace. You can set your preferred namespace but you must consistently use this namespace in subsequent commands. See Customizing the Kubernetes deployment .","title":"Generate TLS Certificate"},{"location":"containerization/Amazon%20EKS/kubernetes_eks_cf191andearlier/#final-output","text":"External IP from Load balancer in the below example can be used to access PORTAL Output $ kubectl get all -n NAMESPACE https://EXTERNAL_IP/wps/portal","title":"Final Output"},{"location":"containerization/Amazon%20EKS/kubernetes_eks_cf192andlater/","text":"Deploy DX CF192 and later release Containers to Amazon EKS This section describes how to deploy HCL Digital Experience 9.5 CF192 and later release containers, along with the Ambassador, to Kubernetes as verified in Amazon Elastic Kubernetes Service (Amazon EKS) . Prerequisites Prior to using the following procedures, it is assumed that the HCL DX Administrator is generally experienced in using Kubernetes. Additionally, the DX Administrator must have the appropriate access to the target environment. If not, the following are some preliminary steps that must be taken. Setup KUBECONFIG to refer to the target server. This ensures any kubectl commands executed locally affect the target environment. Amazon EKS Cluster The following tools must be installed on a machine other than the Portal server: Docker AWS Command Line Interface (CLI) - used to get image details. dxctl tool - If deploying Digital Experience Container Update CF192 or later, the dcxtl tool is used to install and configure the deployment. Documentation resource: Deploy DX Container to Microsoft Azure Kubernetes Service (AKS) Volume requirement: Requires an AccessMode of ReadWriteMany Requires a minimum of 40 GB , with the default request set to 100 GB RECLAIM POLICY = Retain For DAM, additional volume is needed. For more details on storage class and volume, see Sample storage class and volume . Note: HCL Digital Experience is input-output (I/O) intensive which requires a high performing file system for optimization; NFS is an option for this. Hardware: 4 cores / 7 GB - - - 5 cores / 9 GB Amazon Elastic Container Registry (ECR) or any container registry access, for tagging and pushing. See the support topic for the HCL Digital Experience detailed system requirements . Download and extract the contents of the HCL DX 9.5 package to the local file system. This can be a local workstation or a local cloud platform. Note: Reference the latest HCL DX 9.5 Container Release and Update file listings in the Docker deployment topic. If deploying HCL Digital Experience 9.5 Container Update CF192 release, the image and package names are as follows: CF192-core.zip files: HCL DX notices V9.5 CF192.txt dxclient_v1.2.0_20210305-1758.zip hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF192_20210305-2309.tar.gz hcl-dx-cloud-scripts-v95_CF192_20210305-2309.zip hcl-dx-content-composer-image-v1.6.0_20210305-1756.tar.gz hcl-dx-core-image-v95_CF192_20210305-1758.tar.gz hcl-dx-digital-asset-management-operator-image-v95_CF192_20210305-1757.tar.gz hcl-dx-digital-asset-manager-image-v1.6.0_20210305-1802.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20210305-1805.zip hcl-dx-image-processor-image-v1.6.0_20210305-1758.tar.gz hcl-dx-openldap-image-v1.0.0-master_20210305_1614986151.tar.gz hcl-dx-postgres-image-v1.6.0_20210305-1800.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-remote-search-image-v95_CF192_20210305-1758.tar.gz hcl-dx-ringapi-image-v1.6.0_20210305-1802.tar.gz Note: Images included in the \u2018other\u2019 package released with CF192 are optional (in addition to HCL DX 9.5 core Portal and Web Content Manager) and are used to support the use of OpenLDAP, Remote Search, the Experience API, Content Composer, and Digital Asset Management components and services. About this task Follow these steps to deploy HCL Digital Experience 9.5 CF192 and later container release, along with Ambassador to Kubernetes, as verified in Amazon Elastic Kubernetes Service (Amazon EKS) . This deployment relies heavily on Kubernetes Operators for full functionality. If deploying a Container Update CF191 or earlier, view the instructions to deploy using script-based commands , instead of the dxctl tool commands used in this section. DX Container Management Follow these steps to deploy the HCL Digital Experience 9.5 CF192 and later container release in Amazon EKS Change directory Open a terminal window and change to the root directory of the extracted package. Docker load Load the containers into your Docker repository: ``` docker load < hcl-dx-core-image-v95_CF192_20210225-004909.tar.gz - ``` docker load < hcl-dx-ambassador-image-154.tar.gz - ``` docker load < hcl-dx-cloud-operator-image-v95_CF192_20210225-0546.tar.gz - ``` docker load < hcl-dx-image-processor-image-v1.6.0_20210226-0014.tar.gz - ``` docker load < hcl-dx-digital-asset-management-operator-image-v95_CF192_20210226-0040.tar.gz - ``` docker load < hcl-dx-postgres-image-v1.6.0_20210226-0009.tar.gz - ``` docker load < hcl-dx-ringapi-image-v1.6.0_20210226-0055.tar.gz - ``` docker load < hcl-dx-redis-image-5.0.1.tar.gz **Note:** Either **`-i`** or **`<`** works for the load command. In case you encounter an error when using one, try running the command using the other. Docker tag and push Get the Docker images in your local Docker repository to your target Kubernetes system by tagging and pushing them appropriately. If you used docker load to get your images on the target environment, proceed to the next step. ``` docker tag - ``` docker push <image:tag> Persistence volume Set the Persistent volume Create (or have the Kubernetes Administrator create) a persistent volume and storage class where the AccessMode must be ReadWriteMany and the persistent volume reclaim policy must be Retain . For more details on storage class and volume, see Sample storage class and volume NFS server Provide the HCL DX 9.5 CF192 and later Docker image access to the volume mount created in order to copy the profile. There are various ways to do this and NFS is one option. If NFS is used, following are the parameters that are tested to work: rw, (Default) sync , (Default after NFS 1.0, means that the server does not reply until after the commit) insecure ,** (Requires requests originate on ports less than 1024) root_squash ,** (Map requests to the nobody user). hard ,** (Required because this means the system will keep trying to write until it works.) nfsvers=4.1 , rsize=8388608 , (Avoids dropped packages, default 8192) wsize=8388608 , (Avoids dropped packages, default 8192) timeo=600 , (60 seconds) retrans=2 , (Number of retries after a time out) noresvport ** (Tells the NFS client to use a new Transmission Control Protocol (TCP) source port when a network connection is reestablished. Doing this helps make sure that the EFS file system has uninterrupted availability after a network recovery event.) Note: Those marked with ** are critical and, in many cases, it is recommended to have the rsize and wsize set to 8388608 . Login to the Kubernetes cluster Before using the dxctl tool to deploy, you must be logged in to the targeted cluster using the cloud platform-specific CLI (Red Hat OpenShift, Amazon EKS, Microsoft Azure AKS, or Google GKE). Example: aws eks update-kubeconfig --name <eks_cluster> --region <region> Configure and deploy Configure and deploy using HCL DX dxctl tool . Change directory Change to the extracted files directory: cd hcl-dx-cloud-scripts Using DX Container Update CF192 and later, the directory structure appears as follows: Configure dxctl Configure the dxctl properties for the HCL DX Container CF192 and later deployment. Copy one of the provided properties files to further modify for your deployment. The modified properties file can be used for the deployment and the same file must be used for further updates. mkdir -p /home/$USER/deployments/ cp dxctl/properties/full-deployment.properties /home/$USER/deployments/myfirst_deployment.properties Update the dxctl properties file values. See the following sample values: ``` dx.namespace: kube-eg-cf192-0225 - ``` dx.image: dxen - ``` dx.tag: v95_CF192_20210225-035822_xxxxxxx_95_CF192_60374773 ``` dx.storageclass:dx-deploy-stg ``` - ``` dx.volume: kube-eg-d2-core-pv - ``` dx.volume.size:60 - ``` remote.search.enabled:false - ``` openldap.enabled:false - ``` api.enabled: false - ``` composer.enabled: false - ``` dam.enabled: false - ``` ingress.image:dx-build-output/common/ambassador - ``` ingress.tag:1.5.4 - ``` ingress.redis.image:redis - ``` ingress.redis.tag:5.0.1 - ``` dx.operator.image: dx-build-output/hcldx-cloud-operator/hcldx-cloud-operator - ``` dx.operator.tag: v95_CF192_20210225-0546_xxxxxxxxx_95_CF192 **Important:** With HCL DX 9.5 Container Update CF197 and later, dam.features in full-deployment properties is added for use in a future container update release, and should not be modified except with direct guidance from HCL Support. **Note:** With HCL DX 9.5 Container Update CF193 and later, persist.force.read in full-deployment properties is added to enable a read-only Postgres pod for Digital Asset Management. This enables a failover capability for the Postgres service supporting DAM. Another option to enable a read-only pod is to set the persist.minreplicas: option set to greater than 1. Example: ![](../images/Container_deploy_amazon.png) Deploy using dxctl tool Run the dxctl command to deploy the HCL DX 9.5 CF192 and later release to Amazon EKS. For Windows: ./win/dxctl --deploy -p /home/$USER/deployments/myfirst_deployment.properties For Linux: ./linux/dxctl --deploy -p /home/$USER/deployments/myfirst_deployment.properties For Mac: ./mac/dxctl -\u2013update -p /home/$USER/deployments/myfirst_deployment.properties Note: These steps create the DX 9.5 CF192 and later deployment. Generate TLS Certificate Create a TLS certification to be used by the deployment: For development purposes: Using OpenSSL, you can create a private key: openssl genrsa -out my-key.pem 2048 Using OpenSSL, you can create a certificate signed by the private key: openssl req -x509 -key my-key.pem -out my-cert.pem -days 365 -subj '/CN=my-cert Create a TLS certification: kubectl create secret tls dx-tls-cert --cert=my-cert.pem --key=my-key.pem -n aws-mynamespace Note: The default name is the dx-tls-cert this can be changed in the configuration. aws-mynamespace is your Kubernetes namespace. You can set your preferred namespace but you must consistently use this namespace in subsequent commands. See Customizing the Kubernetes deployment . Access the deployment Obtain the external IP from the platform load balancer as shown in following example to access the HCL DX 9.5 CF192 and later deployment: Output: $ kubectl get all -n NAMESPACE Example: The deployed system can be accessed at: https://external-ip/wps/portal Update To update the deployment to later HCL DX 9.5 Container Update releases, follow these steps: If using HCL DX 9.5 Container Update CF191 and earlier release, update the deployment properties file with new image values and run the Update command. Example: On Mac: ./mac/dxctl -\u2013update -p properties/myfirst_deployment.properties On Windows: .\\win\\dxctl --update -p properties\\myfirst_deployment.properties On Linux: ./linux/dxctl -\u2013update -p properties/myfirst_deployment.properties Note: If using HCL DX 9.5 Container Update CF192 or later, the dxctl tool can be used to Update the deployment. The dxctl tool does not deploy or update the DxDeployment custom resource definition. Prior to running an Update process, administrators should check the DxDeployment custom resource definition ( hcl-dx-cloud-scripts/deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml ) for changes and update accordingly: Kubernetes command: kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com CAUTION: Since crd is a cluster-wide resource, the use of kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com causes a service outage for all the dx-deployment across the cluster. Kubernetes command: kubectl create -f deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml Additional considerations: For example, once the database is transferred, the DBTYPE must be updated so you can scale the instances higher. Additionally, once the database is transferred, the number of replicas could be increased. There are additional options to customize the deployment. Delete To delete the deployment, follow either of these methods: Method 1: Remove the deployment but allow for redeployment with the same volumes: Example: ../linux/dxctl --destroy -p properties/myfirst_deployment.properties Method 2: Remove the entire namespace/project: Example: ./linux/dxctl --destroy -p properties/myfirst_deployment.properties -all true If some deployment resources (such as services) are not deleted, run the following command: kubectl patch services $(kubectl get services -n $NAMESPACE | grep -v \"NAME\" |awk '{print $1}') -p '{\"metadata\":{\"finalizers\":null}}' -n $NAMESPACE","title":"Deploy DX CF192 and later release Containers to Amazon EKS"},{"location":"containerization/Amazon%20EKS/kubernetes_eks_cf192andlater/#deploy-dx-cf192-and-later-release-containers-to-amazon-eks","text":"This section describes how to deploy HCL Digital Experience 9.5 CF192 and later release containers, along with the Ambassador, to Kubernetes as verified in Amazon Elastic Kubernetes Service (Amazon EKS) .","title":"Deploy DX CF192 and later release Containers to Amazon EKS"},{"location":"containerization/Amazon%20EKS/kubernetes_eks_cf192andlater/#prerequisites","text":"Prior to using the following procedures, it is assumed that the HCL DX Administrator is generally experienced in using Kubernetes. Additionally, the DX Administrator must have the appropriate access to the target environment. If not, the following are some preliminary steps that must be taken. Setup KUBECONFIG to refer to the target server. This ensures any kubectl commands executed locally affect the target environment. Amazon EKS Cluster The following tools must be installed on a machine other than the Portal server: Docker AWS Command Line Interface (CLI) - used to get image details. dxctl tool - If deploying Digital Experience Container Update CF192 or later, the dcxtl tool is used to install and configure the deployment. Documentation resource: Deploy DX Container to Microsoft Azure Kubernetes Service (AKS) Volume requirement: Requires an AccessMode of ReadWriteMany Requires a minimum of 40 GB , with the default request set to 100 GB RECLAIM POLICY = Retain For DAM, additional volume is needed. For more details on storage class and volume, see Sample storage class and volume . Note: HCL Digital Experience is input-output (I/O) intensive which requires a high performing file system for optimization; NFS is an option for this. Hardware: 4 cores / 7 GB - - - 5 cores / 9 GB Amazon Elastic Container Registry (ECR) or any container registry access, for tagging and pushing. See the support topic for the HCL Digital Experience detailed system requirements . Download and extract the contents of the HCL DX 9.5 package to the local file system. This can be a local workstation or a local cloud platform. Note: Reference the latest HCL DX 9.5 Container Release and Update file listings in the Docker deployment topic. If deploying HCL Digital Experience 9.5 Container Update CF192 release, the image and package names are as follows: CF192-core.zip files: HCL DX notices V9.5 CF192.txt dxclient_v1.2.0_20210305-1758.zip hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF192_20210305-2309.tar.gz hcl-dx-cloud-scripts-v95_CF192_20210305-2309.zip hcl-dx-content-composer-image-v1.6.0_20210305-1756.tar.gz hcl-dx-core-image-v95_CF192_20210305-1758.tar.gz hcl-dx-digital-asset-management-operator-image-v95_CF192_20210305-1757.tar.gz hcl-dx-digital-asset-manager-image-v1.6.0_20210305-1802.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20210305-1805.zip hcl-dx-image-processor-image-v1.6.0_20210305-1758.tar.gz hcl-dx-openldap-image-v1.0.0-master_20210305_1614986151.tar.gz hcl-dx-postgres-image-v1.6.0_20210305-1800.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-remote-search-image-v95_CF192_20210305-1758.tar.gz hcl-dx-ringapi-image-v1.6.0_20210305-1802.tar.gz Note: Images included in the \u2018other\u2019 package released with CF192 are optional (in addition to HCL DX 9.5 core Portal and Web Content Manager) and are used to support the use of OpenLDAP, Remote Search, the Experience API, Content Composer, and Digital Asset Management components and services.","title":"Prerequisites"},{"location":"containerization/Amazon%20EKS/kubernetes_eks_cf192andlater/#about-this-task","text":"Follow these steps to deploy HCL Digital Experience 9.5 CF192 and later container release, along with Ambassador to Kubernetes, as verified in Amazon Elastic Kubernetes Service (Amazon EKS) . This deployment relies heavily on Kubernetes Operators for full functionality. If deploying a Container Update CF191 or earlier, view the instructions to deploy using script-based commands , instead of the dxctl tool commands used in this section.","title":"About this task"},{"location":"containerization/Amazon%20EKS/kubernetes_eks_cf192andlater/#dx-container-management","text":"Follow these steps to deploy the HCL Digital Experience 9.5 CF192 and later container release in Amazon EKS Change directory Open a terminal window and change to the root directory of the extracted package. Docker load Load the containers into your Docker repository: ``` docker load < hcl-dx-core-image-v95_CF192_20210225-004909.tar.gz - ``` docker load < hcl-dx-ambassador-image-154.tar.gz - ``` docker load < hcl-dx-cloud-operator-image-v95_CF192_20210225-0546.tar.gz - ``` docker load < hcl-dx-image-processor-image-v1.6.0_20210226-0014.tar.gz - ``` docker load < hcl-dx-digital-asset-management-operator-image-v95_CF192_20210226-0040.tar.gz - ``` docker load < hcl-dx-postgres-image-v1.6.0_20210226-0009.tar.gz - ``` docker load < hcl-dx-ringapi-image-v1.6.0_20210226-0055.tar.gz - ``` docker load < hcl-dx-redis-image-5.0.1.tar.gz **Note:** Either **`-i`** or **`<`** works for the load command. In case you encounter an error when using one, try running the command using the other. Docker tag and push Get the Docker images in your local Docker repository to your target Kubernetes system by tagging and pushing them appropriately. If you used docker load to get your images on the target environment, proceed to the next step. ``` docker tag - ``` docker push <image:tag>","title":"DX Container Management"},{"location":"containerization/Amazon%20EKS/kubernetes_eks_cf192andlater/#persistence-volume","text":"Set the Persistent volume Create (or have the Kubernetes Administrator create) a persistent volume and storage class where the AccessMode must be ReadWriteMany and the persistent volume reclaim policy must be Retain . For more details on storage class and volume, see Sample storage class and volume NFS server Provide the HCL DX 9.5 CF192 and later Docker image access to the volume mount created in order to copy the profile. There are various ways to do this and NFS is one option. If NFS is used, following are the parameters that are tested to work: rw, (Default) sync , (Default after NFS 1.0, means that the server does not reply until after the commit) insecure ,** (Requires requests originate on ports less than 1024) root_squash ,** (Map requests to the nobody user). hard ,** (Required because this means the system will keep trying to write until it works.) nfsvers=4.1 , rsize=8388608 , (Avoids dropped packages, default 8192) wsize=8388608 , (Avoids dropped packages, default 8192) timeo=600 , (60 seconds) retrans=2 , (Number of retries after a time out) noresvport ** (Tells the NFS client to use a new Transmission Control Protocol (TCP) source port when a network connection is reestablished. Doing this helps make sure that the EFS file system has uninterrupted availability after a network recovery event.) Note: Those marked with ** are critical and, in many cases, it is recommended to have the rsize and wsize set to 8388608 . Login to the Kubernetes cluster Before using the dxctl tool to deploy, you must be logged in to the targeted cluster using the cloud platform-specific CLI (Red Hat OpenShift, Amazon EKS, Microsoft Azure AKS, or Google GKE). Example: aws eks update-kubeconfig --name <eks_cluster> --region <region>","title":"Persistence volume"},{"location":"containerization/Amazon%20EKS/kubernetes_eks_cf192andlater/#configure-and-deploy","text":"Configure and deploy using HCL DX dxctl tool . Change directory Change to the extracted files directory: cd hcl-dx-cloud-scripts Using DX Container Update CF192 and later, the directory structure appears as follows: Configure dxctl Configure the dxctl properties for the HCL DX Container CF192 and later deployment. Copy one of the provided properties files to further modify for your deployment. The modified properties file can be used for the deployment and the same file must be used for further updates. mkdir -p /home/$USER/deployments/ cp dxctl/properties/full-deployment.properties /home/$USER/deployments/myfirst_deployment.properties Update the dxctl properties file values. See the following sample values: ``` dx.namespace: kube-eg-cf192-0225 - ``` dx.image: dxen - ``` dx.tag: v95_CF192_20210225-035822_xxxxxxx_95_CF192_60374773 ``` dx.storageclass:dx-deploy-stg ``` - ``` dx.volume: kube-eg-d2-core-pv - ``` dx.volume.size:60 - ``` remote.search.enabled:false - ``` openldap.enabled:false - ``` api.enabled: false - ``` composer.enabled: false - ``` dam.enabled: false - ``` ingress.image:dx-build-output/common/ambassador - ``` ingress.tag:1.5.4 - ``` ingress.redis.image:redis - ``` ingress.redis.tag:5.0.1 - ``` dx.operator.image: dx-build-output/hcldx-cloud-operator/hcldx-cloud-operator - ``` dx.operator.tag: v95_CF192_20210225-0546_xxxxxxxxx_95_CF192 **Important:** With HCL DX 9.5 Container Update CF197 and later, dam.features in full-deployment properties is added for use in a future container update release, and should not be modified except with direct guidance from HCL Support. **Note:** With HCL DX 9.5 Container Update CF193 and later, persist.force.read in full-deployment properties is added to enable a read-only Postgres pod for Digital Asset Management. This enables a failover capability for the Postgres service supporting DAM. Another option to enable a read-only pod is to set the persist.minreplicas: option set to greater than 1. Example: ![](../images/Container_deploy_amazon.png) Deploy using dxctl tool Run the dxctl command to deploy the HCL DX 9.5 CF192 and later release to Amazon EKS. For Windows: ./win/dxctl --deploy -p /home/$USER/deployments/myfirst_deployment.properties For Linux: ./linux/dxctl --deploy -p /home/$USER/deployments/myfirst_deployment.properties For Mac: ./mac/dxctl -\u2013update -p /home/$USER/deployments/myfirst_deployment.properties Note: These steps create the DX 9.5 CF192 and later deployment.","title":"Configure and deploy"},{"location":"containerization/Amazon%20EKS/kubernetes_eks_cf192andlater/#generate-tls-certificate","text":"Create a TLS certification to be used by the deployment: For development purposes: Using OpenSSL, you can create a private key: openssl genrsa -out my-key.pem 2048 Using OpenSSL, you can create a certificate signed by the private key: openssl req -x509 -key my-key.pem -out my-cert.pem -days 365 -subj '/CN=my-cert Create a TLS certification: kubectl create secret tls dx-tls-cert --cert=my-cert.pem --key=my-key.pem -n aws-mynamespace Note: The default name is the dx-tls-cert this can be changed in the configuration. aws-mynamespace is your Kubernetes namespace. You can set your preferred namespace but you must consistently use this namespace in subsequent commands. See Customizing the Kubernetes deployment .","title":"Generate TLS Certificate"},{"location":"containerization/Amazon%20EKS/kubernetes_eks_cf192andlater/#access-the-deployment","text":"Obtain the external IP from the platform load balancer as shown in following example to access the HCL DX 9.5 CF192 and later deployment: Output: $ kubectl get all -n NAMESPACE Example: The deployed system can be accessed at: https://external-ip/wps/portal","title":"Access the deployment"},{"location":"containerization/Amazon%20EKS/kubernetes_eks_cf192andlater/#update","text":"To update the deployment to later HCL DX 9.5 Container Update releases, follow these steps: If using HCL DX 9.5 Container Update CF191 and earlier release, update the deployment properties file with new image values and run the Update command. Example: On Mac: ./mac/dxctl -\u2013update -p properties/myfirst_deployment.properties On Windows: .\\win\\dxctl --update -p properties\\myfirst_deployment.properties On Linux: ./linux/dxctl -\u2013update -p properties/myfirst_deployment.properties Note: If using HCL DX 9.5 Container Update CF192 or later, the dxctl tool can be used to Update the deployment. The dxctl tool does not deploy or update the DxDeployment custom resource definition. Prior to running an Update process, administrators should check the DxDeployment custom resource definition ( hcl-dx-cloud-scripts/deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml ) for changes and update accordingly: Kubernetes command: kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com CAUTION: Since crd is a cluster-wide resource, the use of kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com causes a service outage for all the dx-deployment across the cluster. Kubernetes command: kubectl create -f deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml Additional considerations: For example, once the database is transferred, the DBTYPE must be updated so you can scale the instances higher. Additionally, once the database is transferred, the number of replicas could be increased. There are additional options to customize the deployment.","title":"Update"},{"location":"containerization/Amazon%20EKS/kubernetes_eks_cf192andlater/#delete","text":"To delete the deployment, follow either of these methods: Method 1: Remove the deployment but allow for redeployment with the same volumes: Example: ../linux/dxctl --destroy -p properties/myfirst_deployment.properties Method 2: Remove the entire namespace/project: Example: ./linux/dxctl --destroy -p properties/myfirst_deployment.properties -all true If some deployment resources (such as services) are not deleted, run the following command: kubectl patch services $(kubectl get services -n $NAMESPACE | grep -v \"NAME\" |awk '{print $1}') -p '{\"metadata\":{\"finalizers\":null}}' -n $NAMESPACE","title":"Delete"},{"location":"containerization/Management/","text":"Customizing your container deployment This section outlines the customization options when deploying HCL Digital Experience Container.","title":"Customizing your container deployment"},{"location":"containerization/Management/#customizing-your-container-deployment","text":"This section outlines the customization options when deploying HCL Digital Experience Container.","title":"Customizing your container deployment"},{"location":"containerization/Management/REST_APIs_remote_search/","text":"Configure Remote Search using REST APIs This section shows how to configure Remote Search for your HCL Digital Experience 9.5 environments on a traditional IBM WebSphere Application Server Network Deployment-based cluster DX deployment cluster, a Docker container, or on supported Red Hat OpenShift and Kubernetes container platforms using REST APIs. Introduction REST APIs are available to allow listing, deleting, modifying, and defining Portal Search Services (and their parameters). REST APIs may also be used to list, delete, and define Portal Search collections and Portal Search Content Providers (and their parameters). Reference the HCL DX 9.5 Help Center topic REST APIs for Search for additional information. Configuring Remote Search for DX Portal requires changes to the WebSphere configuration of both the remote search server, as well as the DX Portal server. In addition, Portal changes need to be made on the Portal server itself. Traditionally, these changes occurred via the Search Admin GUI on DX Portal. Starting HCL Digital Experience 9.5 CF199 and higher deployments, a new set of REST services also enables users to configure Remote Search. A REST service is implemented, and may be used to perform many of the same Remote Search configuration tasks in a selected environment. The environment can be a traditional IBM WebSphere Application Server Network Deployment-based cluster DX deployment, a set of Docker images, or a set of DX Kubernetes PODs. Prerequisites In general, at least one (1) DX Portal Server and exactly one (1) DX Remote Search Server instances must be running. This can be in Docker, in Kubernetes, or a cluster. The Portal Servers must have addressability to the Remote Search Server and vice-versa. Optimally, this is handled through a DNS server so each of the servers has an IP address statically assigned and resolvable via DNS. Note: Some services typically assign IP addresses dynamically and are NOT available in DNS. This is true (by default) for Docker. To resolve this issue in Docker, add parameters to the docker run command. Docker images can be started like it follows both - to statistically assign a DNS address, and for that DNS address to be in the /etc/hosts file on the servers: ``` !/bin/bash PORTALIP=\"172.18.0.10\" REMOTESEARCHIP=\"172.18.0.11\" DOCKERHOST=\"172.19.0.1\" PORTAL_DOCKER_IMAGE=\"quintana-docker.artifactory.cwp.pnp-hcl.com/dx-build-output/core/dxen:v95_CF192_20210206-022427_rohan_DXQ-14209_on_develop_601eaed4\" REMOTE_SEARCH_DOCKER_IMAGE=\"quintana-docker.artifactory.cwp.pnp-hcl.com/dxrs:v95_CF192_20210208-055522_rohan_develop_60215986\" echo \"Starting portal docker image with tag\" $PORTAL_DOCKER_IMAGE echo \"Starting remote search docker image with tag\" $REMOTE_SEARCH_DOCKER_IMAGE Start the two docker images Portal docker run -d --name portaldocker --net aDockerNetwork --ip=\"$PORTALIP\" -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200-10205:10200-10205 -p 7777:7777 --add-host=\"remotesearch:$REMOTESEARCHIP\" --add-host=\"remotesearch:$REMOTESEARCHIP2\" --add-host=\"dockerHost:$DOCKERHOST\" $PORTAL_DOCKER_IMAGE Remote Search docker run -d --name remotesearch --net aDockerNetwork --ip=\"$REMOTESEARCHIP\" -p 8880:8880 -p 2809:2809 -p 9043:9043 -p 9060:9060 -p 9080:9080 -p 9403:9403 --add-host=\"portaldocker:$PORTALIP\" --add-host=\"dockerHost:$DOCKERHOST\" $REMOTE_SEARCH_DOCKER_IMAGE ``` In order to use statically assigned IP addresses like in the example above, a private Docker subnet is created using the following command: docker network create --internal --subnet 172.18.0.0/16 aDockerNetwork The default network used in the example is 172.19.0.0/16. This is the address range used by the Docker host. The example will result to a new set of ConfigEngine tasks to exist for the WebSphere configuration portion. A new set of REST APIs are also now in place to support the command-line configuration of Portal Search. Note: The configuration commands used in the example configures remote search in a DX environment. However, the collections are empty even though they are defined. To populate the collections, the crawlers must be started. This can either be achieved by manually starting them, putting them on a schedule, or a combination of both. Access Rights For any attempted operation, the user that makes the request must first log into the Portal. The logged-in user is then checked for sufficient privileges before the requested action to any subsequent Remote Search REST API request is executed. If the logged-in user has no sufficient privileges, the Remote Search REST API request is rejected, and an appropriate response is returned. New ConfigEngine tasks ConfigEngine tasks on the DX Portal Server Complete configuration of WebSphere on the DX Portal Server is accomplished by executing the following command: ./ConfigEngine.sh configure-portal-for-remote-search -DWasPassword={Was Password} Parameters may be added to this command to customize it. The following are all given -D parameters, along with the default values for each: -Dremote.search.host.name default=\"remotesearch\" -Dremote.search.host.port default=\"9043\" -Dremote.search.cert.alias default=\"remotesearchalias\" -Dremote.search.iiop.url default=\"iiop://remotesearch:2809\" -Dremote.search.index.directory default=\"/opt/HCL/AppServer/profiles/prs_profile/SearchCollections\" The following takes place when the DX Portal Server ConfigEngine command is executed: Retrieve the remote SSL key from the remote search server. Export the LPTA key to a file for the Portal server. Suppress the automatic creation of the Default Search Server on Portal restart, if it doesn't already exist. Set all the Resource Environment Providers for the JCR for WCM Authoring search. ConfigEngine tasks on the DX Remote Search Server Complete configuration of WebSphere on the DX Remote Search Server may now be accomplished by executing the following command: ./ConfigEngine.sh configure-remote-search-server-for-remote-search -DWasPassword={Was Password} Note: Complete Remote search server configuration require deploying WebScannerEjbEar.ear, and copying and unzipping file PseLibs.zip on remote search server. Refer to the Preparing for remote service topic for steps. Parameters may be added to this command to customize it. The following are all given -D parameters, along with the default values for each: -Dremote.search.host.name default=\"remotesearch\" -Dportal.host.name default=\"portaldocker\" -Dportal.port.number default=\"10042\" -Dportal.cert.alias default=\"portaldockeralias\" The following takes place when the DX Remote Search Server ConfigEngine command is executed: Retrieve remote SSL key from the Portal Server. Import the LTPA key exported from the Portal Server in the previous step. Edit the serverindex.xml file to have the correct Remote Search server host name. Important: Both the remote search server and the portal server must both be restarted after the ConfigEngine tasks are complete. Since the changes are IBM WebSphere Application Server Network Deployment-based cluster DX deployment changes in the profile, the changes are not picked up until the restart. New REST APIs Like all REST services, the type of HTTP command ( GET , PUT , POST , DELETE ) dictates the type of operation. The format of the URL is very similar for each type. However, some of the types (e.g. POST ) require JSON input to define the add. Here are the HTTP mapping types: GET -> list POST -> add DELETE -> delete The following example illustrates the elements of a URL, which generally consists of the following: /wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB/collection/JCRCollection1/provider/JCR+Content The initial portal (/wps/mycontenthandler/!ut/p/searchadmin/service) is invariant and is present in all REST commands for remote search configuration. Remote+PSE+service+EJB presents the name of the service on which to perform an operation. Note that in a URL, the space character is NOT allowed. You can either replace the space character with the \" + \" character, or replace the space character with \" %20 \". Both forms are equivalent. The collection character sequence is only required when operating on a collection or providers within a collection. In the example, the collection name is JCRCollection1. This happens to be the required collection name for searches of artifacts by the WCM Authoring GUI. If the URL is malformed for whatever reason, an error will be returned in response to the request. Lastly, and only required when doing operations on a content provider for a particular service and collection, is you need to add the required character sequence provider, followed by the name of the provider in question. In our example, the provider is called JCR Content. Note that a \" + \" replaces a space character in the URL. Thus, the actual provider name is JCR Content. For all commands, the HTTP response code is useful. For example, if the HTTP response code is 401 , then it is likely that the one has NOT used the REST login before the REST configure command. All these commands require an \"Authenticated\" status. The POST and DEL commands require administrator access rights on the search configuration objects. In all cases, a combination of the HTTP response code along with a potential error message in the response payload indicates a variety of potential issues. Some of these issues may include a lack of access rights for the intended operation, the fact that the resource already exists (for example, trying to create/POST a service name that already exists), and more. Otherwise, a successful returns an HTTP response code of 20x List The following command list details of various remote search resources. No JSON body is required on the request. The HTTP response is the JSON which matches the type of the request. If the requested resource to \" LIST \" doesn't exist, the returned JSON will be empty (e.g. \" {} \"). http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/services http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/ http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/collection/{collection name} http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/collection/{collection name}/provider/{content provider name} Sample command and output: http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/services { \"services\": [ { \"name\": \"Remote PSE service EJB\", \"link\": \"/wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB\" } ] } Note that each service name is followed by a relative link, which can be used to get more details of the service. The next command shows an example of this: http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB/ { \"RESOURCE_ENVIRONMENT_PROVIDER_NAME\": \"SearchPropertiesService\", \"facetedFields\": \"null\", \"WORK_MANAGER_DEPLOY\": \"wps/searchIndexWM\", \"EJB_Example\": \"ejb/com/ibm/hrl/portlets/WsPse/WebScannerLiteEJBHome\", \"DefaultCollectionsDirectory\": \"null\", \"CONTENT_SOURCE_TYPE_FEATURE_NAME\": \"ContentSourceType\", \"EJB\": \"ejb/com/ibm/hrl/portlets/WsPse/WebScannerLiteEJBHome\", \"MAX_BUILD_BATCH_SIZE\": \"10000\", \"fieldTypes\": \"null\", \"WORK_MANAGER_NATIVE\": \"force.hrl.work.manager.use.native.threads\", \"WORK_MANAGER\": \"wps/searchIndexWM\", \"PSE_TYPE_option_3\": \"soap\", \"PSE_TYPE_option_2\": \"ejb\", \"PSE_TYPE_option_1\": \"localhost\", \"IIOP_URL\": \"iiop://remotesearch:2809\", \"VALIDATE_COOKIE\": \"123\", \"PortalCollectionSourceName\": \"Remote PSE service EJB\", \"WORK_MANAGER_NAME\": \"wps/searchIndexWM\", \"PSE_TYPE\": \"ejb\", \"CONTENT_SOURCE_TYPE_FEATURE_VAL_PORTAL\": \"Portal\", \"HTTP_MAX_BODY_SIZE_MB\": \"20\", \"MAX_BUILD_INTERVAL_TIME_SECONDS\": \"300\", \"SetProperties\": \"on\", \"PortalCollectionName\": \"TestGood\", \"IIOP_URL_Example\": \"iiop://localhost:2811\", \"CLEAN_UP_TIME_OF_DAY_HOURS\": \"0\", \"SOAP_URL_Example\": \"http://localhost:10000/WebScannerSOAP/servlet/rpcrouter\", \"mappedFields\": \"null\", \"OPEN_WCM_WINDOW\": \"/wps/myportal/wcmContent?WCM_GLOBAL_CONTEXT=\", \"SOAP_URL\": \"null\", \"DEFAULT_acls_FIELDINFO\": \"contentSearchable=false, fieldSearchable=true, returnable=true, sortable=false, supportsExactMatch=true, parametric=false, typeAhead=false\", \"SecurityResolverId\": \"com.ibm.lotus.search.plugins.provider.core.PortalSecurityResolverFactory\", \"CONTENT_SOURCE_TYPE_FEATURE_VAL_UPLOAD\": \"Upload\", \"CONTENT_SOURCE_TYPE_FEATURE_VAL_WEB\": \"Web\", \"OpenResultMode\": \"new\", \"SEARCH_SECURITY_MODE\": \"SECURITY_MODE_PRE_POST_FILTER\", \"collections\": [ { \"name\": \"JCRCollection1\", \"link\": \"/wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB/collection/JCRCollection1\" }, { \"name\": \"Portal Search Collection\", \"link\": \"/wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB/collection/Portal+Search+Collection\" } ] } Again, do note that the end of the list shows two collections, and the URLs that can be used to gather more information regarding those collections. http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB/collection/JCRCollection1 { \"location\": \"/opt/HCL/AppServer/profiles/prs_profile/SearchCollections/JCRCollection1\", \"IndexTitleKey\": \"JCRCollection1\", \"IndexNameKey\": \"JCRCollection1\", \"IndexLanguageKey\": \"en_US\", \"location\": \"/opt/HCL/AppServer/profiles/prs_profile/SearchCollections/JCRCollection1\", \"CollectionStatus\": \"true\", \"IndexDescriptionKey\": \"JCRCollection1\", \"DictionaryAnalysis\": \"true\", \"providers\": [ { \"name\": \"JCR Content\", \"link\": \"/wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB/collection/JCRCollection1/provider/JCR+Content\" } ] } Delete If a resource to be deleted does not exist, then the returned JSON will return null (e.g. \" {} \"), which is the same as the returned JSON if the request is successful. http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/ http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/collection/{collection name} http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/collection/{collection name}/provider/{content provider name} In general, after a successful delete operation (HTTP 200), expect that the response JSON payload is null (e.g. \" {} \"). Add http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/ http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/collection/{collection name} http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/collection/{collection name}/provider/{content provider name} If the resource to be added already exists, then an error message is returned like the following: Error 400: {resource} Already Exists where {resource} is one of \"service\", \"collection\" or \"content provider\" as is appropriate for the invalid request URL. The JSON returned as a result of an add REST call is exactly that, which is returned for the same GET call. Effectively, the returned JSON echoes the input add JSON request. Starting a Crawler Once the Service/Collection/Content Provider is configured, the crawlers will still not populate the indexes. To populate the indexes, the crawlers must be started. Crawlers can be started in one of two different ways: The first is via a scheduler, which automatically runs the crawler on a set schedule. Currently this schedule can only be configured in the search GUI. The second method is to immediately start the crawler either from the GUI or via a REST service. The REST service to start a crawler looks as follows: http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB/collection/Portal+Search+Collection/provider/WCMContentSource/crawl This URL looks like very much a POST command to add a content provider. The only difference is that the crawl command is located at the end of the URL. This command will start an immediate crawl on the content provider in the previous portion of the URL. The output of the command is an HTTP 201 return code, along with a JSON body that is exactly like this: { \"crawl\": \"started\" } Use of API on Main Virtual Portal versus all other Virtual Portals On a Virtual Portal, the \u201c!ut/p/digest\u201d portal of the URL must be included as the contenthandler cannot issue the redirect when using the URL format without the portion mentioned. As such, referring to the example URLs above, the \u201c!ut/p/digest\u201d portal of the URL is NOT included. This implies that this URL is issued in the \"main\" VP of the DX Portal. A 302 redirect will take place, and the \u201c!ut/p/digest\u201d will be inserted in the final URL. This portion of the URL can also be used for the VP URL request.","title":"Configure Remote Search using REST APIs"},{"location":"containerization/Management/REST_APIs_remote_search/#configure-remote-search-using-rest-apis","text":"This section shows how to configure Remote Search for your HCL Digital Experience 9.5 environments on a traditional IBM WebSphere Application Server Network Deployment-based cluster DX deployment cluster, a Docker container, or on supported Red Hat OpenShift and Kubernetes container platforms using REST APIs.","title":"Configure Remote Search using REST APIs"},{"location":"containerization/Management/REST_APIs_remote_search/#introduction","text":"REST APIs are available to allow listing, deleting, modifying, and defining Portal Search Services (and their parameters). REST APIs may also be used to list, delete, and define Portal Search collections and Portal Search Content Providers (and their parameters). Reference the HCL DX 9.5 Help Center topic REST APIs for Search for additional information. Configuring Remote Search for DX Portal requires changes to the WebSphere configuration of both the remote search server, as well as the DX Portal server. In addition, Portal changes need to be made on the Portal server itself. Traditionally, these changes occurred via the Search Admin GUI on DX Portal. Starting HCL Digital Experience 9.5 CF199 and higher deployments, a new set of REST services also enables users to configure Remote Search. A REST service is implemented, and may be used to perform many of the same Remote Search configuration tasks in a selected environment. The environment can be a traditional IBM WebSphere Application Server Network Deployment-based cluster DX deployment, a set of Docker images, or a set of DX Kubernetes PODs.","title":"Introduction"},{"location":"containerization/Management/REST_APIs_remote_search/#prerequisites","text":"In general, at least one (1) DX Portal Server and exactly one (1) DX Remote Search Server instances must be running. This can be in Docker, in Kubernetes, or a cluster. The Portal Servers must have addressability to the Remote Search Server and vice-versa. Optimally, this is handled through a DNS server so each of the servers has an IP address statically assigned and resolvable via DNS. Note: Some services typically assign IP addresses dynamically and are NOT available in DNS. This is true (by default) for Docker. To resolve this issue in Docker, add parameters to the docker run command. Docker images can be started like it follows both - to statistically assign a DNS address, and for that DNS address to be in the /etc/hosts file on the servers: ```","title":"Prerequisites"},{"location":"containerization/Management/REST_APIs_remote_search/#binbash","text":"PORTALIP=\"172.18.0.10\" REMOTESEARCHIP=\"172.18.0.11\" DOCKERHOST=\"172.19.0.1\" PORTAL_DOCKER_IMAGE=\"quintana-docker.artifactory.cwp.pnp-hcl.com/dx-build-output/core/dxen:v95_CF192_20210206-022427_rohan_DXQ-14209_on_develop_601eaed4\" REMOTE_SEARCH_DOCKER_IMAGE=\"quintana-docker.artifactory.cwp.pnp-hcl.com/dxrs:v95_CF192_20210208-055522_rohan_develop_60215986\" echo \"Starting portal docker image with tag\" $PORTAL_DOCKER_IMAGE echo \"Starting remote search docker image with tag\" $REMOTE_SEARCH_DOCKER_IMAGE","title":"!/bin/bash"},{"location":"containerization/Management/REST_APIs_remote_search/#start-the-two-docker-images","text":"","title":"Start the two docker images"},{"location":"containerization/Management/REST_APIs_remote_search/#portal","text":"docker run -d --name portaldocker --net aDockerNetwork --ip=\"$PORTALIP\" -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200-10205:10200-10205 -p 7777:7777 --add-host=\"remotesearch:$REMOTESEARCHIP\" --add-host=\"remotesearch:$REMOTESEARCHIP2\" --add-host=\"dockerHost:$DOCKERHOST\" $PORTAL_DOCKER_IMAGE","title":"Portal"},{"location":"containerization/Management/REST_APIs_remote_search/#remote-search","text":"docker run -d --name remotesearch --net aDockerNetwork --ip=\"$REMOTESEARCHIP\" -p 8880:8880 -p 2809:2809 -p 9043:9043 -p 9060:9060 -p 9080:9080 -p 9403:9403 --add-host=\"portaldocker:$PORTALIP\" --add-host=\"dockerHost:$DOCKERHOST\" $REMOTE_SEARCH_DOCKER_IMAGE ``` In order to use statically assigned IP addresses like in the example above, a private Docker subnet is created using the following command: docker network create --internal --subnet 172.18.0.0/16 aDockerNetwork The default network used in the example is 172.19.0.0/16. This is the address range used by the Docker host. The example will result to a new set of ConfigEngine tasks to exist for the WebSphere configuration portion. A new set of REST APIs are also now in place to support the command-line configuration of Portal Search. Note: The configuration commands used in the example configures remote search in a DX environment. However, the collections are empty even though they are defined. To populate the collections, the crawlers must be started. This can either be achieved by manually starting them, putting them on a schedule, or a combination of both.","title":"Remote Search"},{"location":"containerization/Management/REST_APIs_remote_search/#access-rights","text":"For any attempted operation, the user that makes the request must first log into the Portal. The logged-in user is then checked for sufficient privileges before the requested action to any subsequent Remote Search REST API request is executed. If the logged-in user has no sufficient privileges, the Remote Search REST API request is rejected, and an appropriate response is returned.","title":"Access Rights"},{"location":"containerization/Management/REST_APIs_remote_search/#new-configengine-tasks","text":"ConfigEngine tasks on the DX Portal Server Complete configuration of WebSphere on the DX Portal Server is accomplished by executing the following command: ./ConfigEngine.sh configure-portal-for-remote-search -DWasPassword={Was Password} Parameters may be added to this command to customize it. The following are all given -D parameters, along with the default values for each: -Dremote.search.host.name default=\"remotesearch\" -Dremote.search.host.port default=\"9043\" -Dremote.search.cert.alias default=\"remotesearchalias\" -Dremote.search.iiop.url default=\"iiop://remotesearch:2809\" -Dremote.search.index.directory default=\"/opt/HCL/AppServer/profiles/prs_profile/SearchCollections\" The following takes place when the DX Portal Server ConfigEngine command is executed: Retrieve the remote SSL key from the remote search server. Export the LPTA key to a file for the Portal server. Suppress the automatic creation of the Default Search Server on Portal restart, if it doesn't already exist. Set all the Resource Environment Providers for the JCR for WCM Authoring search. ConfigEngine tasks on the DX Remote Search Server Complete configuration of WebSphere on the DX Remote Search Server may now be accomplished by executing the following command: ./ConfigEngine.sh configure-remote-search-server-for-remote-search -DWasPassword={Was Password} Note: Complete Remote search server configuration require deploying WebScannerEjbEar.ear, and copying and unzipping file PseLibs.zip on remote search server. Refer to the Preparing for remote service topic for steps. Parameters may be added to this command to customize it. The following are all given -D parameters, along with the default values for each: -Dremote.search.host.name default=\"remotesearch\" -Dportal.host.name default=\"portaldocker\" -Dportal.port.number default=\"10042\" -Dportal.cert.alias default=\"portaldockeralias\" The following takes place when the DX Remote Search Server ConfigEngine command is executed: Retrieve remote SSL key from the Portal Server. Import the LTPA key exported from the Portal Server in the previous step. Edit the serverindex.xml file to have the correct Remote Search server host name. Important: Both the remote search server and the portal server must both be restarted after the ConfigEngine tasks are complete. Since the changes are IBM WebSphere Application Server Network Deployment-based cluster DX deployment changes in the profile, the changes are not picked up until the restart.","title":"New ConfigEngine tasks"},{"location":"containerization/Management/REST_APIs_remote_search/#new-rest-apis","text":"Like all REST services, the type of HTTP command ( GET , PUT , POST , DELETE ) dictates the type of operation. The format of the URL is very similar for each type. However, some of the types (e.g. POST ) require JSON input to define the add. Here are the HTTP mapping types: GET -> list POST -> add DELETE -> delete The following example illustrates the elements of a URL, which generally consists of the following: /wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB/collection/JCRCollection1/provider/JCR+Content The initial portal (/wps/mycontenthandler/!ut/p/searchadmin/service) is invariant and is present in all REST commands for remote search configuration. Remote+PSE+service+EJB presents the name of the service on which to perform an operation. Note that in a URL, the space character is NOT allowed. You can either replace the space character with the \" + \" character, or replace the space character with \" %20 \". Both forms are equivalent. The collection character sequence is only required when operating on a collection or providers within a collection. In the example, the collection name is JCRCollection1. This happens to be the required collection name for searches of artifacts by the WCM Authoring GUI. If the URL is malformed for whatever reason, an error will be returned in response to the request. Lastly, and only required when doing operations on a content provider for a particular service and collection, is you need to add the required character sequence provider, followed by the name of the provider in question. In our example, the provider is called JCR Content. Note that a \" + \" replaces a space character in the URL. Thus, the actual provider name is JCR Content. For all commands, the HTTP response code is useful. For example, if the HTTP response code is 401 , then it is likely that the one has NOT used the REST login before the REST configure command. All these commands require an \"Authenticated\" status. The POST and DEL commands require administrator access rights on the search configuration objects. In all cases, a combination of the HTTP response code along with a potential error message in the response payload indicates a variety of potential issues. Some of these issues may include a lack of access rights for the intended operation, the fact that the resource already exists (for example, trying to create/POST a service name that already exists), and more. Otherwise, a successful returns an HTTP response code of 20x","title":"New REST APIs"},{"location":"containerization/Management/REST_APIs_remote_search/#list","text":"The following command list details of various remote search resources. No JSON body is required on the request. The HTTP response is the JSON which matches the type of the request. If the requested resource to \" LIST \" doesn't exist, the returned JSON will be empty (e.g. \" {} \"). http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/services http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/ http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/collection/{collection name} http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/collection/{collection name}/provider/{content provider name} Sample command and output: http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/services { \"services\": [ { \"name\": \"Remote PSE service EJB\", \"link\": \"/wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB\" } ] } Note that each service name is followed by a relative link, which can be used to get more details of the service. The next command shows an example of this: http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB/ { \"RESOURCE_ENVIRONMENT_PROVIDER_NAME\": \"SearchPropertiesService\", \"facetedFields\": \"null\", \"WORK_MANAGER_DEPLOY\": \"wps/searchIndexWM\", \"EJB_Example\": \"ejb/com/ibm/hrl/portlets/WsPse/WebScannerLiteEJBHome\", \"DefaultCollectionsDirectory\": \"null\", \"CONTENT_SOURCE_TYPE_FEATURE_NAME\": \"ContentSourceType\", \"EJB\": \"ejb/com/ibm/hrl/portlets/WsPse/WebScannerLiteEJBHome\", \"MAX_BUILD_BATCH_SIZE\": \"10000\", \"fieldTypes\": \"null\", \"WORK_MANAGER_NATIVE\": \"force.hrl.work.manager.use.native.threads\", \"WORK_MANAGER\": \"wps/searchIndexWM\", \"PSE_TYPE_option_3\": \"soap\", \"PSE_TYPE_option_2\": \"ejb\", \"PSE_TYPE_option_1\": \"localhost\", \"IIOP_URL\": \"iiop://remotesearch:2809\", \"VALIDATE_COOKIE\": \"123\", \"PortalCollectionSourceName\": \"Remote PSE service EJB\", \"WORK_MANAGER_NAME\": \"wps/searchIndexWM\", \"PSE_TYPE\": \"ejb\", \"CONTENT_SOURCE_TYPE_FEATURE_VAL_PORTAL\": \"Portal\", \"HTTP_MAX_BODY_SIZE_MB\": \"20\", \"MAX_BUILD_INTERVAL_TIME_SECONDS\": \"300\", \"SetProperties\": \"on\", \"PortalCollectionName\": \"TestGood\", \"IIOP_URL_Example\": \"iiop://localhost:2811\", \"CLEAN_UP_TIME_OF_DAY_HOURS\": \"0\", \"SOAP_URL_Example\": \"http://localhost:10000/WebScannerSOAP/servlet/rpcrouter\", \"mappedFields\": \"null\", \"OPEN_WCM_WINDOW\": \"/wps/myportal/wcmContent?WCM_GLOBAL_CONTEXT=\", \"SOAP_URL\": \"null\", \"DEFAULT_acls_FIELDINFO\": \"contentSearchable=false, fieldSearchable=true, returnable=true, sortable=false, supportsExactMatch=true, parametric=false, typeAhead=false\", \"SecurityResolverId\": \"com.ibm.lotus.search.plugins.provider.core.PortalSecurityResolverFactory\", \"CONTENT_SOURCE_TYPE_FEATURE_VAL_UPLOAD\": \"Upload\", \"CONTENT_SOURCE_TYPE_FEATURE_VAL_WEB\": \"Web\", \"OpenResultMode\": \"new\", \"SEARCH_SECURITY_MODE\": \"SECURITY_MODE_PRE_POST_FILTER\", \"collections\": [ { \"name\": \"JCRCollection1\", \"link\": \"/wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB/collection/JCRCollection1\" }, { \"name\": \"Portal Search Collection\", \"link\": \"/wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB/collection/Portal+Search+Collection\" } ] } Again, do note that the end of the list shows two collections, and the URLs that can be used to gather more information regarding those collections. http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB/collection/JCRCollection1 { \"location\": \"/opt/HCL/AppServer/profiles/prs_profile/SearchCollections/JCRCollection1\", \"IndexTitleKey\": \"JCRCollection1\", \"IndexNameKey\": \"JCRCollection1\", \"IndexLanguageKey\": \"en_US\", \"location\": \"/opt/HCL/AppServer/profiles/prs_profile/SearchCollections/JCRCollection1\", \"CollectionStatus\": \"true\", \"IndexDescriptionKey\": \"JCRCollection1\", \"DictionaryAnalysis\": \"true\", \"providers\": [ { \"name\": \"JCR Content\", \"link\": \"/wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB/collection/JCRCollection1/provider/JCR+Content\" } ] }","title":"List"},{"location":"containerization/Management/REST_APIs_remote_search/#delete","text":"If a resource to be deleted does not exist, then the returned JSON will return null (e.g. \" {} \"), which is the same as the returned JSON if the request is successful. http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/ http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/collection/{collection name} http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/collection/{collection name}/provider/{content provider name} In general, after a successful delete operation (HTTP 200), expect that the response JSON payload is null (e.g. \" {} \").","title":"Delete"},{"location":"containerization/Management/REST_APIs_remote_search/#add","text":"http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/ http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/collection/{collection name} http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/collection/{collection name}/provider/{content provider name} If the resource to be added already exists, then an error message is returned like the following: Error 400: {resource} Already Exists where {resource} is one of \"service\", \"collection\" or \"content provider\" as is appropriate for the invalid request URL. The JSON returned as a result of an add REST call is exactly that, which is returned for the same GET call. Effectively, the returned JSON echoes the input add JSON request.","title":"Add"},{"location":"containerization/Management/REST_APIs_remote_search/#starting-a-crawler","text":"Once the Service/Collection/Content Provider is configured, the crawlers will still not populate the indexes. To populate the indexes, the crawlers must be started. Crawlers can be started in one of two different ways: The first is via a scheduler, which automatically runs the crawler on a set schedule. Currently this schedule can only be configured in the search GUI. The second method is to immediately start the crawler either from the GUI or via a REST service. The REST service to start a crawler looks as follows: http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB/collection/Portal+Search+Collection/provider/WCMContentSource/crawl This URL looks like very much a POST command to add a content provider. The only difference is that the crawl command is located at the end of the URL. This command will start an immediate crawl on the content provider in the previous portion of the URL. The output of the command is an HTTP 201 return code, along with a JSON body that is exactly like this: { \"crawl\": \"started\" }","title":"Starting a Crawler"},{"location":"containerization/Management/REST_APIs_remote_search/#use-of-api-on-main-virtual-portal-versus-all-other-virtual-portals","text":"On a Virtual Portal, the \u201c!ut/p/digest\u201d portal of the URL must be included as the contenthandler cannot issue the redirect when using the URL format without the portion mentioned. As such, referring to the example URLs above, the \u201c!ut/p/digest\u201d portal of the URL is NOT included. This implies that this URL is issued in the \"main\" VP of the DX Portal. A 302 redirect will take place, and the \u201c!ut/p/digest\u201d will be inserted in the final URL. This portion of the URL can also be used for the VP URL request.","title":"Use of API on Main Virtual Portal versus all other Virtual Portals"},{"location":"containerization/Management/credentialvaultslot/","text":"Create or update credential vault slot This topic describes the commands that are used to create or update credential vault slot in the DX server. Credential vault slot Command description Use the create-credential-vault command to create or update a credential vault slot. dxclient create-credential-vault Help command This command shows the help information for create-credential-vault command usage: dxclient create-credential-vault -h Command options Use this attribute to specify the protocol to connect to the server: -dxProtocol <value> Use this attribute to specify the hostname of the target server: -hostname <value> Use this attribute to specify the port on which to connect to the server (for Kubernetes Environment, dxPort is 443): -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server: -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server: -dxPassword <value> Use this attribute to specify the path to DX configuration endpoint: -xmlConfigPath <value> Use this attribute to specify the credential vault segment slot name: -credentialSlotName <value> Use this attribute to specify the credential vault Username: -vaultUsername <value> Use this attribute to specify the credential vault UserGroup: -vaultUserGroup <value> Use this attribute to specify the credential vault shared userid password: -vaultPassword <value> Use this attribute to specify the credential vault segment name and the default is set to DefaultAdminSegment : -vaultSegmentName <value> Use this attribute to specify the credential vault segment description: -vaultDescription <value> All the command options are configured in the config.json file of the tool, which is read by default. The configuration file is located at <working-directory>/store/config.json . The options that are passed through the command line overrides the default values. Example Usage: dxclient create-credential-vault -dxProtocol <dxProtocol> -hostname <hostname> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -xmlConfigPath <xmlConfigPath> -credentialSlotName <credentialSlotName> -vaultUsername <vaultUsername> -vaultPassword <vaultPassword>","title":"Create or update credential vault slot"},{"location":"containerization/Management/credentialvaultslot/#create-or-update-credential-vault-slot","text":"This topic describes the commands that are used to create or update credential vault slot in the DX server.","title":"Create or update credential vault slot"},{"location":"containerization/Management/credentialvaultslot/#credential-vault-slot","text":"Command description Use the create-credential-vault command to create or update a credential vault slot. dxclient create-credential-vault Help command This command shows the help information for create-credential-vault command usage: dxclient create-credential-vault -h Command options Use this attribute to specify the protocol to connect to the server: -dxProtocol <value> Use this attribute to specify the hostname of the target server: -hostname <value> Use this attribute to specify the port on which to connect to the server (for Kubernetes Environment, dxPort is 443): -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server: -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server: -dxPassword <value> Use this attribute to specify the path to DX configuration endpoint: -xmlConfigPath <value> Use this attribute to specify the credential vault segment slot name: -credentialSlotName <value> Use this attribute to specify the credential vault Username: -vaultUsername <value> Use this attribute to specify the credential vault UserGroup: -vaultUserGroup <value> Use this attribute to specify the credential vault shared userid password: -vaultPassword <value> Use this attribute to specify the credential vault segment name and the default is set to DefaultAdminSegment : -vaultSegmentName <value> Use this attribute to specify the credential vault segment description: -vaultDescription <value> All the command options are configured in the config.json file of the tool, which is read by default. The configuration file is located at <working-directory>/store/config.json . The options that are passed through the command line overrides the default values. Example Usage: dxclient create-credential-vault -dxProtocol <dxProtocol> -hostname <hostname> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -xmlConfigPath <xmlConfigPath> -credentialSlotName <credentialSlotName> -vaultUsername <vaultUsername> -vaultPassword <vaultPassword>","title":"Credential vault slot"},{"location":"containerization/Management/deployapplication/","text":"Digital Experience applications This section provides information about the deployment of DX application artifacts by using the DXClient tool. Deploy Application The deploy-application command is used to deploy the EAR file into the WebSphere Application Server. Command description This command invokes the deploy-application tool inside DXClient. This command uses the provided files and execute the deploy application task. dxclient deploy-application Required files The following EAR file will be deployed into the WebSphere Application Server: Deployable EAR Help command This command shows the help information for deploy-application command usage: dxclient deploy-application -h Command options Use this attribute to specify the hostname of the target server -hostname <value> Use this attribute to specify the protocol with which to connect to the server -dxProtocol <value> Use this attribute to specify the port on which to connect to the server(for Kubernetes Environment dxPort is 443) -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server -dxPassword <value> Required options for application deployment: Use this attribute to specify the config wizard home (route change only in case of Open Shift Kubernetes Enviornment, otherwise same as hostname) that is required for authenticating to the cw_profile -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile (for Kubernetes Environment dxConnectPort is 443) -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile -dxConnectPassword <value> Use this attribute to specify Soap port of the DX server -dxSoapPort <Soap port of the DX server> Specify either the dxProfileName or dxProfilePath of the DX core server: Use this attribute to specify the profile name of the DX core server (for example: wp_profile ) -dxProfileName <Profile name of the DX core server> OR Use this attribute to specify the profile path of the DX server (for example: /opt/HCL/wp_profile ) -dxProfilePath <Path of the DX core server profile> Use this attribute to specify the EAR file path that is required while executing the deploy application task \u2013applicationFile <Absolute or relative path to deployable ear file> Use this attribute to specify the application name -applicationName <value> Use this attribute to specify the path to the contenthandler servlet on the DX server (e.g. /wps/mycontenthandler) -contenthandlerPath <value> All the above command options can also be configured inside the config.json configuration file of the DXClient tool, available in the <working-directory>/store directory of the DXClient installation. Note: If you have installed DXClient using the node package file, then you can find the config.json file in the following path: dist/src/configuration. The values passed through the command line command override the default values. Example Usage: dxclient deploy-application -dxProtocol <http/https> -hostname <host-name> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -dxSoapPort <dxSoapPort> -dxConnectHostname <hostname> -dxConnectPort <dxConnectPort> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -applicationFile <application-file-with-path> -applicationName <application name> -dxProfileName <Profile name of the DX core server>","title":"Digital Experience applications"},{"location":"containerization/Management/deployapplication/#digital-experience-applications","text":"This section provides information about the deployment of DX application artifacts by using the DXClient tool.","title":"Digital Experience applications"},{"location":"containerization/Management/deployapplication/#deploy-application","text":"The deploy-application command is used to deploy the EAR file into the WebSphere Application Server. Command description This command invokes the deploy-application tool inside DXClient. This command uses the provided files and execute the deploy application task. dxclient deploy-application Required files The following EAR file will be deployed into the WebSphere Application Server: Deployable EAR Help command This command shows the help information for deploy-application command usage: dxclient deploy-application -h Command options Use this attribute to specify the hostname of the target server -hostname <value> Use this attribute to specify the protocol with which to connect to the server -dxProtocol <value> Use this attribute to specify the port on which to connect to the server(for Kubernetes Environment dxPort is 443) -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server -dxPassword <value> Required options for application deployment: Use this attribute to specify the config wizard home (route change only in case of Open Shift Kubernetes Enviornment, otherwise same as hostname) that is required for authenticating to the cw_profile -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile (for Kubernetes Environment dxConnectPort is 443) -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile -dxConnectPassword <value> Use this attribute to specify Soap port of the DX server -dxSoapPort <Soap port of the DX server> Specify either the dxProfileName or dxProfilePath of the DX core server: Use this attribute to specify the profile name of the DX core server (for example: wp_profile ) -dxProfileName <Profile name of the DX core server> OR Use this attribute to specify the profile path of the DX server (for example: /opt/HCL/wp_profile ) -dxProfilePath <Path of the DX core server profile> Use this attribute to specify the EAR file path that is required while executing the deploy application task \u2013applicationFile <Absolute or relative path to deployable ear file> Use this attribute to specify the application name -applicationName <value> Use this attribute to specify the path to the contenthandler servlet on the DX server (e.g. /wps/mycontenthandler) -contenthandlerPath <value> All the above command options can also be configured inside the config.json configuration file of the DXClient tool, available in the <working-directory>/store directory of the DXClient installation. Note: If you have installed DXClient using the node package file, then you can find the config.json file in the following path: dist/src/configuration. The values passed through the command line command override the default values. Example Usage: dxclient deploy-application -dxProtocol <http/https> -hostname <host-name> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -dxSoapPort <dxSoapPort> -dxConnectHostname <hostname> -dxConnectPort <dxConnectPort> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -applicationFile <application-file-with-path> -applicationName <application name> -dxProfileName <Profile name of the DX core server>","title":"Deploy Application"},{"location":"containerization/Management/personalization/","text":"Personalization rules This topic contains the commands that the administrators can use to export and import the personalization (PZN) rules from the source server to the target server as specified by the user. Export PZN rules Command description The pzn-export command is used to export the rules from the source server location specified by the user. dxclient pzn-export Help command This command shows the help information for pzn-export command usage: dxclient pzn-export -h Command options Use this attribute to specify the protocol with which to connect to the DX server: -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server,for Kubernetes Environment dxPort is 443: -dxPort <value> Use this attribute to specify the username to authenticate with the DX server: -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the location in the target workspace, which is the parent for the published nodes. The target path must exist before publishing: -targetPath <value> Use this attribute to specify the name of the workspace containing the rules in DX server (default targetWorkspace is 'ROOTWORKSPACE'): -targetWorkspace <value> Note: The targetPath and targetWorkspace parameters are optional. If the user does not pass the respective parameters, then the default values are taken. Use this attribute to specify the path to the context root on the DX server (for example, /wps): -dxContextRoot <value> All the above command options can be configured in the config.json file of the tool, which is read by default. The configuration file is located at <working-directory>/store/config.json . The options that are passed through the command line override the default values. Log files from command execution can be found in the logs directory of the DXClient installation. Example Usage: ``` dxclient pzn-export -dxProtocol -hostname -dxPort -dxUsername -dxPassword ``` The outputfile for pzn export is generated in the following path: store/outputFiles/pznrules Import PZN rules Command description The pzn-import command is used to import the rules into the target server. dxclient pzn-import Required files Rules file: This file should contain the configuration XML representation of all the currently selected personalization objects. Help command This command shows the help information for pzn-import command usage: dxclient pzn-import -h Command options Use this attribute to specify the protocol with which to connect to the DX server -dxProtocol <value> Use this attribute to specify the hostname of the target DX server -hostname <value> Use this attribute to specify the port on which to connect to the DX server,for Kubernetes Environment dxPort is 443 -dxPort <value> Use this attribute to specify the username to authenticate with the DX server -dxUsername <value> Use this attribute to specify the password for the user in the \"dxUsername\" attribute -dxPassword <value> Use this attribute to specify the config wizard home (route change only in case of Open Shift Kubernetes Environment, otherwise same as hostname) that is required for authenticating to the cw_profile -dxConnectHostname <value> Use this attribute to specify the username that is required for authenticating to the cw_profile -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile -dxConnectPassword <value> Use this attribute to specify the port number of the cw_profile(for Kubernetes Environment dxConnectPort is 443) -dxConnectPort <value> Use this attribute to specify the import file path that is required while executing the pzn import task -rulesFilePath <Absolute or relative path to import nodes file> Use this attribute to specify the location in the target workspace, which is the parent for the published nodes. The target path must exist before publishing -targetPath <value> Use this attribute to specify the name of the workspace containing the rules in DX server (default targetWorkspace is 'ROOTWORKSPACE') -targetWorkspace <value> Use this attribute to specify the path to the context root on the DX server (for example, /wps): -dxContextRoot <value> Notes: For Kubernetes environments, dxProtocol should be http , hostname should be localhost, dxPort should be 10039 as DXConnect doesn't support https due to SSL Handshake challenges at this time. The dxProtocol , hostname , dxPort , targetWorkspace , and targetPath parameters are optional. If the user does not pass the respective parameters, then the default values are taken. All the above command options can be configured in the config.json file of the tool, which is read by default. Location of the configuration file - <working-directory>/store/config.json . The options passed through command line will override these default values. Log files from command execution can be found in the logs directory of the DXClient installation. Example Usage: dxclient pzn-import -dxProtocol <dxProtocol> -hostname <hostname> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -dxConnectHostname <dxConnectHostname> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -dxConnectPort <dxConnectPort> -rulesFilePath <rulesFilePath>","title":"Personalization rules"},{"location":"containerization/Management/personalization/#personalization-rules","text":"This topic contains the commands that the administrators can use to export and import the personalization (PZN) rules from the source server to the target server as specified by the user.","title":"Personalization rules"},{"location":"containerization/Management/personalization/#export-pzn-rules","text":"Command description The pzn-export command is used to export the rules from the source server location specified by the user. dxclient pzn-export Help command This command shows the help information for pzn-export command usage: dxclient pzn-export -h Command options Use this attribute to specify the protocol with which to connect to the DX server: -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server,for Kubernetes Environment dxPort is 443: -dxPort <value> Use this attribute to specify the username to authenticate with the DX server: -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the location in the target workspace, which is the parent for the published nodes. The target path must exist before publishing: -targetPath <value> Use this attribute to specify the name of the workspace containing the rules in DX server (default targetWorkspace is 'ROOTWORKSPACE'): -targetWorkspace <value> Note: The targetPath and targetWorkspace parameters are optional. If the user does not pass the respective parameters, then the default values are taken. Use this attribute to specify the path to the context root on the DX server (for example, /wps): -dxContextRoot <value> All the above command options can be configured in the config.json file of the tool, which is read by default. The configuration file is located at <working-directory>/store/config.json . The options that are passed through the command line override the default values. Log files from command execution can be found in the logs directory of the DXClient installation. Example Usage: ``` dxclient pzn-export -dxProtocol -hostname -dxPort -dxUsername -dxPassword ``` The outputfile for pzn export is generated in the following path: store/outputFiles/pznrules","title":"Export PZN rules"},{"location":"containerization/Management/personalization/#import-pzn-rules","text":"Command description The pzn-import command is used to import the rules into the target server. dxclient pzn-import Required files Rules file: This file should contain the configuration XML representation of all the currently selected personalization objects. Help command This command shows the help information for pzn-import command usage: dxclient pzn-import -h Command options Use this attribute to specify the protocol with which to connect to the DX server -dxProtocol <value> Use this attribute to specify the hostname of the target DX server -hostname <value> Use this attribute to specify the port on which to connect to the DX server,for Kubernetes Environment dxPort is 443 -dxPort <value> Use this attribute to specify the username to authenticate with the DX server -dxUsername <value> Use this attribute to specify the password for the user in the \"dxUsername\" attribute -dxPassword <value> Use this attribute to specify the config wizard home (route change only in case of Open Shift Kubernetes Environment, otherwise same as hostname) that is required for authenticating to the cw_profile -dxConnectHostname <value> Use this attribute to specify the username that is required for authenticating to the cw_profile -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile -dxConnectPassword <value> Use this attribute to specify the port number of the cw_profile(for Kubernetes Environment dxConnectPort is 443) -dxConnectPort <value> Use this attribute to specify the import file path that is required while executing the pzn import task -rulesFilePath <Absolute or relative path to import nodes file> Use this attribute to specify the location in the target workspace, which is the parent for the published nodes. The target path must exist before publishing -targetPath <value> Use this attribute to specify the name of the workspace containing the rules in DX server (default targetWorkspace is 'ROOTWORKSPACE') -targetWorkspace <value> Use this attribute to specify the path to the context root on the DX server (for example, /wps): -dxContextRoot <value> Notes: For Kubernetes environments, dxProtocol should be http , hostname should be localhost, dxPort should be 10039 as DXConnect doesn't support https due to SSL Handshake challenges at this time. The dxProtocol , hostname , dxPort , targetWorkspace , and targetPath parameters are optional. If the user does not pass the respective parameters, then the default values are taken. All the above command options can be configured in the config.json file of the tool, which is read by default. Location of the configuration file - <working-directory>/store/config.json . The options passed through command line will override these default values. Log files from command execution can be found in the logs directory of the DXClient installation. Example Usage: dxclient pzn-import -dxProtocol <dxProtocol> -hostname <hostname> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -dxConnectHostname <dxConnectHostname> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -dxConnectPort <dxConnectPort> -rulesFilePath <rulesFilePath>","title":"Import PZN rules"},{"location":"containerization/Management/portlets/","text":"Portlets This topic provides information about the deployment and undeployment of portlets. Deploy Portlets The deploy-portlet command is used to deploy one or more new or updated portlets from a source client or server environment to target HCL DX 9.5 CF19 or later server using a provided input XMLAccess file and deployable Portlet WAR file. Note: The synchronization mode of all nodes in a clustered DX environment must be consistently set for a newly deployed portlet to be automatically started; otherwise redeployment or a manual start is required. Required files XMLAccess file This xml file should contain the definition of the web application along with the details of the portlet(s) to be deployed. The web archive file path referred to in this file inside the URL element is ignored, but the URL element itself must exist as it is dynamically replaced when the command is executed. A sample XML file for deploying portlet(s) can be found in the samples directory of DXClient (samples/DeployPortlet.xml) or in DX server located in the following directory: PortalServer_root/doc/xml-samples/DeployPortlet.xml. Portlet Application web archive file This web archive .war file should contain the necessary portlet artifacts for deployment, as per the JSR 286 portlet standard. Refer to Importing WAR files Command dxclient deploy-portlet -xmlFile <path> -warFile <path> Help command This command shows the help document on the deploy-portlet command usage: dxclient deploy-portlet -h Command options Use this attribute to specify the protocol with which to connect to the DX server ( wp_profile ): -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server ( wp_profile ): -dxPort <value> Use this attribute to specify the path to DX configuration endpoint (e.g. /wps/config): -xmlConfigPath <value> Use this attribute to specify the username to authenticate with the DX server ( wp_profile ): -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the local path to the XMLAccess file: -xmlFile <Absolute or relative path to XMLAccess input file> Use this attribute to specify the local path to the WAR file to be deployed: -warFile <Absolute or relative path to deployable war file> Use this attribute to specify the Configuration Wizard Console port number: -dxConnectPort <value> Use this attribute to specify the config wizard home (change to the appropriate route in the case of an OpenShift Kubernetes Environment, otherwise the value will typically be the same as the hostname) that is required for authenticating with the DXConnect application: -dxConnectHostname <value> Use this attribute to specify the Configuration Wizard Administrator username that is required for authenticating with the DXConnect application: -dxConnectUsername <value> Use this attribute to specify the Configuration Wizard Administrator password that is required for authenticating with the DXConnect application: -dxConnectPassword <value> All of the above command options can also be configured inside the config.json configuration file of the DXClient tool, available in the dist/src/configuration directory of the DXClient installation. Command options passed through the command line will override values set in the config.json file. Log files from command execution can be found in the logs directory of the DXClient installation. Undeploy portlets The undeploy-portlet command is used to undeploy the portlets in the target DX servers. Note: Undeploy-portlet command takes a backup of the XML file of the deployed portlet application and application (EAR) if user has given enableBackup as true. By default, enableBackup is set to true and placed in the store/outputFiles/portlets/backup/undeploy-portlet/ . In case, if the undeployed portlet is required again, then the user can restore the portlet WAR file from the downloaded portlet application EAR file along with the exported deployable portlet application XML file. Command description This command invokes the undeploy-portlet tool inside the DXClient. The undeploy-portlet dxtool uses the provided files and executes the undeploy portlet task. dxclient undeploy-portlet Help command This command shows the help information for undeploy-portlet command usage: dxclient undeploy-portlet -h Required files This file should contain the definition of the web application along with the undeploy portlet. dxclient undeploy-portlet -xmlFile <path> Command options Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the protocol with which to connect to the DX server (wp_profile): -dxProtocol <value> Use this attribute to specify the port on which to connect to the DX server ( wp_profile ): -dxPort <value> Use this attribute to specify the path to DX configuration endpoint (e.g. /wps/configwps/config): -xmlConfigPath <value> Use this attribute to specify the username to authenticate with the DX server ( wp_profile ): -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the local path to the XMLAccess file: -xmlFile <xml file name with absolute path of the xmlaccess input file> Use this attribute to take the backup of portlet application before undeploying it: -enableBackup <value> Commands required when enableBackup is set to true Use this attribute to specify the config wizard home (route change only in case of Open Shift Kubernetes Environment, otherwise same as hostname) that is required for authenticating to the cw_profile -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile(for Kubernetes Environment dxConnectPort is 443) -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile -dxConnectPassword <value> Use this attribute to specify Soap port of the DX server -dxSoapPort <Soap port of the DX server> Specify either the dxProfileName or dxProfilePath of the DX core server: Use this attribute to specify the profile name of the DX core server (for example: wp_profile ) -dxProfileName <Profile name of the DX core server> OR Use this attribute to specify the profile path of the DX server (for example: /opt/HCL/wp_profile ) -dxProfilePath <Path of the DX core server profile> All the above command options can also be configured in the config.json configuration file of the DXClient tool, available in the <working-directory>/store directory of the DXClient installation. Note: If you have installed DXClient using the node package file, then you can find the config.json file in the following path: dist/src/configuration. The values that are passed through the command line override the default values. Example usage: dxclient undeploy-portlet -dxProtocol <dxProtocol> -hostname <host-name> -dxPort <dxPort> -xmlConfigPath <xmlConfigPath> -dxUsername <dxUsername> -dxPassword <dxPassword> -xmlFile <xml-file-with-path> Example Usage when enableBackup is set to true: dxclient undeploy-portlet -dxProtocol <dxProtocol> -hostname <host-name> -dxPort <dxPort> -xmlConfigPath <xmlConfigPath> -dxUsername <dxUsername> -dxPassword <dxPassword> -xmlFile <xml-file-with-path> -enableBackup true -dxSoapPort <dxSoapPort> -dxConnectHostname <hostname> -dxConnectPort <dxConnectPort> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -dxProfileName <Profile name of the DX core server profile>","title":"Portlets"},{"location":"containerization/Management/portlets/#portlets","text":"This topic provides information about the deployment and undeployment of portlets.","title":"Portlets"},{"location":"containerization/Management/portlets/#deploy-portlets","text":"The deploy-portlet command is used to deploy one or more new or updated portlets from a source client or server environment to target HCL DX 9.5 CF19 or later server using a provided input XMLAccess file and deployable Portlet WAR file. Note: The synchronization mode of all nodes in a clustered DX environment must be consistently set for a newly deployed portlet to be automatically started; otherwise redeployment or a manual start is required. Required files XMLAccess file This xml file should contain the definition of the web application along with the details of the portlet(s) to be deployed. The web archive file path referred to in this file inside the URL element is ignored, but the URL element itself must exist as it is dynamically replaced when the command is executed. A sample XML file for deploying portlet(s) can be found in the samples directory of DXClient (samples/DeployPortlet.xml) or in DX server located in the following directory: PortalServer_root/doc/xml-samples/DeployPortlet.xml. Portlet Application web archive file This web archive .war file should contain the necessary portlet artifacts for deployment, as per the JSR 286 portlet standard. Refer to Importing WAR files Command dxclient deploy-portlet -xmlFile <path> -warFile <path> Help command This command shows the help document on the deploy-portlet command usage: dxclient deploy-portlet -h Command options Use this attribute to specify the protocol with which to connect to the DX server ( wp_profile ): -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server ( wp_profile ): -dxPort <value> Use this attribute to specify the path to DX configuration endpoint (e.g. /wps/config): -xmlConfigPath <value> Use this attribute to specify the username to authenticate with the DX server ( wp_profile ): -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the local path to the XMLAccess file: -xmlFile <Absolute or relative path to XMLAccess input file> Use this attribute to specify the local path to the WAR file to be deployed: -warFile <Absolute or relative path to deployable war file> Use this attribute to specify the Configuration Wizard Console port number: -dxConnectPort <value> Use this attribute to specify the config wizard home (change to the appropriate route in the case of an OpenShift Kubernetes Environment, otherwise the value will typically be the same as the hostname) that is required for authenticating with the DXConnect application: -dxConnectHostname <value> Use this attribute to specify the Configuration Wizard Administrator username that is required for authenticating with the DXConnect application: -dxConnectUsername <value> Use this attribute to specify the Configuration Wizard Administrator password that is required for authenticating with the DXConnect application: -dxConnectPassword <value> All of the above command options can also be configured inside the config.json configuration file of the DXClient tool, available in the dist/src/configuration directory of the DXClient installation. Command options passed through the command line will override values set in the config.json file. Log files from command execution can be found in the logs directory of the DXClient installation.","title":"Deploy Portlets"},{"location":"containerization/Management/portlets/#undeploy-portlets","text":"The undeploy-portlet command is used to undeploy the portlets in the target DX servers. Note: Undeploy-portlet command takes a backup of the XML file of the deployed portlet application and application (EAR) if user has given enableBackup as true. By default, enableBackup is set to true and placed in the store/outputFiles/portlets/backup/undeploy-portlet/ . In case, if the undeployed portlet is required again, then the user can restore the portlet WAR file from the downloaded portlet application EAR file along with the exported deployable portlet application XML file. Command description This command invokes the undeploy-portlet tool inside the DXClient. The undeploy-portlet dxtool uses the provided files and executes the undeploy portlet task. dxclient undeploy-portlet Help command This command shows the help information for undeploy-portlet command usage: dxclient undeploy-portlet -h Required files This file should contain the definition of the web application along with the undeploy portlet. dxclient undeploy-portlet -xmlFile <path> Command options Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the protocol with which to connect to the DX server (wp_profile): -dxProtocol <value> Use this attribute to specify the port on which to connect to the DX server ( wp_profile ): -dxPort <value> Use this attribute to specify the path to DX configuration endpoint (e.g. /wps/configwps/config): -xmlConfigPath <value> Use this attribute to specify the username to authenticate with the DX server ( wp_profile ): -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the local path to the XMLAccess file: -xmlFile <xml file name with absolute path of the xmlaccess input file> Use this attribute to take the backup of portlet application before undeploying it: -enableBackup <value> Commands required when enableBackup is set to true Use this attribute to specify the config wizard home (route change only in case of Open Shift Kubernetes Environment, otherwise same as hostname) that is required for authenticating to the cw_profile -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile(for Kubernetes Environment dxConnectPort is 443) -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile -dxConnectPassword <value> Use this attribute to specify Soap port of the DX server -dxSoapPort <Soap port of the DX server> Specify either the dxProfileName or dxProfilePath of the DX core server: Use this attribute to specify the profile name of the DX core server (for example: wp_profile ) -dxProfileName <Profile name of the DX core server> OR Use this attribute to specify the profile path of the DX server (for example: /opt/HCL/wp_profile ) -dxProfilePath <Path of the DX core server profile> All the above command options can also be configured in the config.json configuration file of the DXClient tool, available in the <working-directory>/store directory of the DXClient installation. Note: If you have installed DXClient using the node package file, then you can find the config.json file in the following path: dist/src/configuration. The values that are passed through the command line override the default values. Example usage: dxclient undeploy-portlet -dxProtocol <dxProtocol> -hostname <host-name> -dxPort <dxPort> -xmlConfigPath <xmlConfigPath> -dxUsername <dxUsername> -dxPassword <dxPassword> -xmlFile <xml-file-with-path> Example Usage when enableBackup is set to true: dxclient undeploy-portlet -dxProtocol <dxProtocol> -hostname <host-name> -dxPort <dxPort> -xmlConfigPath <xmlConfigPath> -dxUsername <dxUsername> -dxPassword <dxPassword> -xmlFile <xml-file-with-path> -enableBackup true -dxSoapPort <dxSoapPort> -dxConnectHostname <hostname> -dxConnectPort <dxConnectPort> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -dxProfileName <Profile name of the DX core server profile>","title":"Undeploy portlets"},{"location":"containerization/Management/resourceenvironments/","text":"Resource environment provider This topic describes the commands that are used to create, update, or delete custom properties from an existing resource environment provider. It also provides the commands to export or import multiple resource environment providers. Resource environment commands Command description: The resource-env-provider command is used to create, update or delete a custom property from an existing Resource Environment Provider, and to export or import multiple resource environment providers. dxclient resource-env-provider Help command: This command shows the help information for resource-env-provider command usage: dxclient resource-env-provider -h Help command for creating the resource environment property: ``` dxclient resource-env-provider create-property -h ``` Help command for updating the resource environment property: dxclient resource-env-provider update-property -h Help command for deleting the resource environment property: dxclient resource-env-provider delete-property -h Help command for exporting the resource environment property: dxclient resource-env-provider export-properties -h Help command for importing the resource environment property: dxclient resource-env-provider import-properties -h Commands: Create a custom property from an existing resource environment: ``` resource-env-provider create-property [OPTIONS] ``` Update a custom property from an existing resource environment: resource-env-provider update-property [OPTIONS] Delete a custom property from an existing resource environment: resource-env-provider delete-property[OPTIONS] Export all the existing resource environment providers: resource-env-provider export-properties [OPTIONS] Import all the existing resource environment providers provided in the input file containing the resource environment providers: resource-env-provider import-properties [OPTIONS] Command options required to create, update, and delete resource environment providers: Use this attribute to specify the protocol with which to connect to the server: -dxProtocol <value> Use this attribute to specify the hostname of the target server: -hostname <value> Use this attribute to specify the port on which to connect to the server (for Kubernetes Environment, dxPort is 443): -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server: -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server: -dxPassword <value> Use this attribute to specify the config wizard home (route change is only in case of Open Shift Kubernetes Environment, otherwise it is the same as hostname) that is required for authenticating to the cw_profile: -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile (for Kubernetes Environment, dxConnectPort is 443): -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile: -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile: -dxConnectPassword <value> Use this attribute to specify the name of the Resource Environment Provider: -providerName <value> Use this attribute to specify the name of the Custom Property: -propertyName <value> Use this attribute to specify the value of the Custom Property: -propertyValue <value> Use this attribute to specify the description of the Custom Property: -propertyDesc <value> Command options required to export and import resource environment providers: Use this attribute to specify the configuration wizard home (route change is only in the case of Open Shift Kubernetes Environment, otherwise it is same as hostname) that is required for authenticating to the cw_profile: -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile(for Kubernetes Environment, dxConnectPort is 443): -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile: -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile: -dxConnectPassword <value> Use this attribute to specify the username of the DX WAS server: -dxWASUsername <value> Use this attribute to specify the password of the DX WAS server: -dxWASPassword <value> Use this attribute to specify the profile name of the DX core server: -dxProfileName <Profile name of the DX core server> For importing resource environment properties, use this attribute to specify the File path: -filePath <value> Example Usage: For creating property: ``` dxclient resource-env-provider create-property -providerName -propertyName -propertyValue ``` For updating property: dxclient resource-env-provider update-property -providerName <providerName> -propertyName <propertyName> -propertyValue <modifiedpropertyValue> For deleting property: dxclient resource-env-provider delete-property -providerName <providerName> -propertyName <propertyName> -propertyValue <modifiedpropertyValue> For exporting property: dxclient resource-env-provider export-properties -dxProfileName <dxProfileName> For importing property: dxclient resource-env-provider import-properties -dxProfileName <dxProfileName> -filePath <filePath>","title":"Resource environment provider"},{"location":"containerization/Management/resourceenvironments/#resource-environment-provider","text":"This topic describes the commands that are used to create, update, or delete custom properties from an existing resource environment provider. It also provides the commands to export or import multiple resource environment providers.","title":"Resource environment provider"},{"location":"containerization/Management/resourceenvironments/#resource-environment-commands","text":"Command description: The resource-env-provider command is used to create, update or delete a custom property from an existing Resource Environment Provider, and to export or import multiple resource environment providers. dxclient resource-env-provider Help command: This command shows the help information for resource-env-provider command usage: dxclient resource-env-provider -h Help command for creating the resource environment property: ``` dxclient resource-env-provider create-property -h ``` Help command for updating the resource environment property: dxclient resource-env-provider update-property -h Help command for deleting the resource environment property: dxclient resource-env-provider delete-property -h Help command for exporting the resource environment property: dxclient resource-env-provider export-properties -h Help command for importing the resource environment property: dxclient resource-env-provider import-properties -h Commands: Create a custom property from an existing resource environment: ``` resource-env-provider create-property [OPTIONS] ``` Update a custom property from an existing resource environment: resource-env-provider update-property [OPTIONS] Delete a custom property from an existing resource environment: resource-env-provider delete-property[OPTIONS] Export all the existing resource environment providers: resource-env-provider export-properties [OPTIONS] Import all the existing resource environment providers provided in the input file containing the resource environment providers: resource-env-provider import-properties [OPTIONS] Command options required to create, update, and delete resource environment providers: Use this attribute to specify the protocol with which to connect to the server: -dxProtocol <value> Use this attribute to specify the hostname of the target server: -hostname <value> Use this attribute to specify the port on which to connect to the server (for Kubernetes Environment, dxPort is 443): -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server: -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server: -dxPassword <value> Use this attribute to specify the config wizard home (route change is only in case of Open Shift Kubernetes Environment, otherwise it is the same as hostname) that is required for authenticating to the cw_profile: -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile (for Kubernetes Environment, dxConnectPort is 443): -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile: -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile: -dxConnectPassword <value> Use this attribute to specify the name of the Resource Environment Provider: -providerName <value> Use this attribute to specify the name of the Custom Property: -propertyName <value> Use this attribute to specify the value of the Custom Property: -propertyValue <value> Use this attribute to specify the description of the Custom Property: -propertyDesc <value> Command options required to export and import resource environment providers: Use this attribute to specify the configuration wizard home (route change is only in the case of Open Shift Kubernetes Environment, otherwise it is same as hostname) that is required for authenticating to the cw_profile: -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile(for Kubernetes Environment, dxConnectPort is 443): -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile: -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile: -dxConnectPassword <value> Use this attribute to specify the username of the DX WAS server: -dxWASUsername <value> Use this attribute to specify the password of the DX WAS server: -dxWASPassword <value> Use this attribute to specify the profile name of the DX core server: -dxProfileName <Profile name of the DX core server> For importing resource environment properties, use this attribute to specify the File path: -filePath <value> Example Usage: For creating property: ``` dxclient resource-env-provider create-property -providerName -propertyName -propertyValue ``` For updating property: dxclient resource-env-provider update-property -providerName <providerName> -propertyName <propertyName> -propertyValue <modifiedpropertyValue> For deleting property: dxclient resource-env-provider delete-property -providerName <providerName> -propertyName <propertyName> -propertyValue <modifiedpropertyValue> For exporting property: dxclient resource-env-provider export-properties -dxProfileName <dxProfileName> For importing property: dxclient resource-env-provider import-properties -dxProfileName <dxProfileName> -filePath <filePath>","title":"Resource environment commands"},{"location":"containerization/Management/sample_storage_class_volume/","text":"Sample storage class and volume for HCL Digital Experience 9.5 containers Learn how to set storage class and volume using a sample storage class and volume scripts for HCL Digital Experience 9.5 CF171 and higher container releases deployed to Amazon Elastic Container Service (EKS) or Red Hat OpenShift environment. It is recommended to set a separate storage class and volume for production, especially if you have more than one project in the Kubernetes (Amazon EKS or Red Hat OpenShift) environment. This is a good practice because it prevents projects from overlapping storage volumes. See video: Understanding the Core Persistent Volumes in HCL Digital Experience Container Update CF194 Follow these steps to create a new persistent volume and storage class, in either Amazon EKS or OpenShift. Use and save the following as your storage class file: kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: dx-deploy-stg provisioner: kubernetes.io/no-provisioner reclaimPolicy: Retain volumeBindingMode: WaitForFirstConsumer Use and save the following as your storage volume file: kind: PersistentVolume apiVersion: v1 metadata: name: wp-profile-volume spec: capacity: storage: 100Gi nfs: server: your_nfs_server.com path: /exports/volume_name accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain storageClassName: dx-deploy-stg mountOptions: - hard - nfsvers=4.1 - rsize=8388608 - wsize=8388608 - timeo=600 - retrans=2 - noresvport volumeMode: Filesystem Copy both files to your local file system. Change at least the server and path in your sample volume .yaml file to an appropriate NFS server and volume. To create the storage class, run the following command: kubectl apply -f subclass.yaml To create the storage volume, run the following command: kubectl apply -f SampleZVolume.yaml Continue with deployment. Note: In these examples, NFS volumes have been used. You can use the following sample yaml to create the volume in Amazon EKS OpenShift with the corrected values: nfs: \u2028 server: your_nfs_server.com \u2028 path: /exports/volume_name","title":"Sample\u00a0storage class and volume for HCL Digital Experience 9.5 containers"},{"location":"containerization/Management/sample_storage_class_volume/#sample-storage-class-and-volume-for-hcl-digital-experience-95-containers","text":"Learn how to set storage class and volume using a sample storage class and volume scripts for HCL Digital Experience 9.5 CF171 and higher container releases deployed to Amazon Elastic Container Service (EKS) or Red Hat OpenShift environment. It is recommended to set a separate storage class and volume for production, especially if you have more than one project in the Kubernetes (Amazon EKS or Red Hat OpenShift) environment. This is a good practice because it prevents projects from overlapping storage volumes. See video: Understanding the Core Persistent Volumes in HCL Digital Experience Container Update CF194 Follow these steps to create a new persistent volume and storage class, in either Amazon EKS or OpenShift. Use and save the following as your storage class file: kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: dx-deploy-stg provisioner: kubernetes.io/no-provisioner reclaimPolicy: Retain volumeBindingMode: WaitForFirstConsumer Use and save the following as your storage volume file: kind: PersistentVolume apiVersion: v1 metadata: name: wp-profile-volume spec: capacity: storage: 100Gi nfs: server: your_nfs_server.com path: /exports/volume_name accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain storageClassName: dx-deploy-stg mountOptions: - hard - nfsvers=4.1 - rsize=8388608 - wsize=8388608 - timeo=600 - retrans=2 - noresvport volumeMode: Filesystem Copy both files to your local file system. Change at least the server and path in your sample volume .yaml file to an appropriate NFS server and volume. To create the storage class, run the following command: kubectl apply -f subclass.yaml To create the storage volume, run the following command: kubectl apply -f SampleZVolume.yaml Continue with deployment. Note: In these examples, NFS volumes have been used. You can use the following sample yaml to create the volume in Amazon EKS OpenShift with the corrected values: nfs: \u2028 server: your_nfs_server.com \u2028 path: /exports/volume_name","title":"Sample\u00a0storage class and volume for HCL Digital Experience 9.5 containers"},{"location":"containerization/Management/scriptapplications/","text":"Script applications This topic provides information about the deployment, undeployment, and restoration of script applications. Deploy script applications The deploy-scriptapplication command is used with the DXClient tool to push or pull Script Applications between a local development workstation or automation server and DX 9.5 CF19 or later servers. The command will push or pull the files that make up a script application to or from a Script Application instance stored in a Web Content Manager library on the server. Required Files : The script application push command in the DXClient tool requires a Script Application zip file or an extracted folder of the same (identified by the prebuiltZip or contentRoot attributes respectively). For more information on Script Applications, refer to the Script Application topics in the HCL DX Help Center. Command This command invokes the deploy-scriptapplication command inside the DXClient tool to either push or pull a script application: dxclient deploy-scriptapplication Subcommands Use this command to create or update the content of a Script Application on the HCL DX server: push [options] Use this command to download the content of a Script Application from the HCL DX server: pull [options] Help command This command shows the help document on the deploy-scriptapplication command: dxclient deploy-scriptapplication pull -h dxclient deploy-scriptapplication push -h Options for the pull subcommand Use this attribute to specify the protocol with which to connect to the DX server: -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server: -dxPort <value> Use this attribute to specify the path to the content handler servlet on DX server (example: /wps/mycontenthandler ): -contenthandlerPath <value> Use this attribute to specify the context of the virtual portal that contains the Script Application instance that you want to retrieve, if any: -virtualPortalContext <value> Use this attribute to specify the context of the portal project that manages the publication of changes to the Script Application instance, if any: -projectContext <value> Use this attribute to specify the username to authenticate with the DX server: -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the WCM content ID of the Script Application content item: -wcmContentId <value> Options for the push subcommand Use this attribute to specify the protocol with which to connect to the DX server: -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server: -dxPort <value> Use this attribute to specify the path to the content handler servlet on the DX server (e.g. /wps/mycontenthandler): -contenthandlerPath <value> Use this attribute to specify the context of the virtual portal that should receive the Script Application instance being pushed, if any: -virtualPortalContext <value> Use this attribute to specify the context of the portal project that manages the publication of changes to the Script Application instance, if any: -projectContext <value> Use this attribute to specify the username to authenticate with the DX server: -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the WCM ID of the Script Application content item: -wcmContentId <value> Use this attribute to specify the SiteArea containing the Script Application content item: -wcmSiteArea <value> Use this attribute to specify the name of the Script Application content item to be created or updated: -wcmContentName <value> Use this attribute to specify the full WCM path of the Script Application content item to be created or updated: -wcmContentPath <value> Use this attribute to set or update the title of the Script Application content item: -wcmContentTitle <value> Use this attribute to specify the path to the main HTML file within the Script Application: -mainHtmlFile <value> Use this attribute to specify the absolute or relative path to the Script Application's content as a ZIP file: -prebuiltZip <value> Use this attribute to specify the absolute or relative path to the Script Application's content in a directory: contentRoot <value> Notes: At least one of (a) wcmContentId , (b) wcmContentPath or (c) both wcmContentName and wcmSiteArea must be specified. If multiple options are provided, then the priority order goes as follows: (a), then (b), and then (c). Use wcmContentId only if you are updating an existing Script Application instance - for new Script Applications specify either (a) wcmContentPath or (b) both wcmContentName and wcmSiteArea . mainHtmlFile is mandatory. The outputfile for pull will be generated inside store/outputFiles/sp-pull-output. When prebuiltZip is specified, the main HTML file path must be relative to the top-level directory in the compressed file. All of the above command options can also be configured inside the config.json configuration file of the DXClient tool, available in the <working-directory>/store directory of the DXClient installation. Note: If you have installed DXClient using the node package file, then you can find the config.json file in the following path: dist/src/configuration. Command options passed through the command line will override values set in the config.json file. Example Usage: For Script Application Pull: dxclient deploy-scriptapplication pull -wcmContentId <wcmContentId> If all required options are configured in config.json of the DX Client tool, then execute: dxclient deploy-scriptapplication pull For Script Application Push, if the Script Application is extracted to a folder named temp at the root of the DXClient machine: dxclient deploy-scriptapplication push -contentRoot /temp -wcmSiteArea \"Script Application Library/Script Applications/\" -wcmContentName DemoScriptApplication If the Script Application is available as a .zip file in a folder named temp on the DXClient tool location, execute: dxclient deploy-scriptapplication push -prebuiltZip /temp/DemoScriptApplication.zip -wcmSiteArea \"Script Application Library/Script Applications/\" -wcmContentName DemoScriptApplication If all required options are configured in the config.json at the /dist/src/configuration path of the DXClient tool, then execute: dxclient deploy-scriptapplication push Undeploy script applications The undeploy-scriptapplication command is used to remove a script application from a target HCL DX 9.5 CF192 or later servers. Required file This command invokes the undeploy-scriptapplication tool inside the DXClient. The undeploy-scriptapplication dxtool uses the provided files and execute the undeploy scriptapplication task. Command dxclient undeploy-scriptapplication -wcmContentId <value> Help command This command shows the help information for undeploy-scriptapplication command usage: dxclient undeploy-scriptapplication -h Command options Use this attribute to specify the protocol with which to connect to the DX server: -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server: -dxPort <value> Use this attribute to specify the path to the content handler servlet on DX server (example, /wps/mycontenthandler ): -contenthandlerPath <value> Use this attribute to specify the context of the virtual portal that contains the Script Application instance that you want to retrieve, if any: -virtualPortalContext <value> Use this attribute to specify the context of the portal project that manages the publication of changes to the Script Application instance, if any: -projectContext <value> Use this attribute to specify the username to authenticate with the DX server: -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the WCM content ID of the Script Application content item: -wcmContentId <value> Use this tag to forcefully delete the Script Application. -f All the above command options can also be configured inside the config.json configuration file of the DXClient tool, available in the <working-directory>/store directory of the DXClient installation. Note: If you have installed DXClient using the node package file, then you can find the config.json file in the following path: dist/src/configuration. Command options passed through the command line will override values set in the config.json file. Log files from command execution can be found in the logs directory of the DXClient installation. Example usage: dxclient undeploy-scriptapplication -wcmContentId <wcm-content-id> dxclient undeploy-scriptapplication -wcmContentId <wcm-content-id> -f Restore Script Application The restore-scriptapplication command is used to restore a script application into one of its previous versions present in the target HCL DX 9.5 CF 19 or later servers. Required file This command invokes the restore-scriptapplication tool inside the DXClient. The restore-scriptapplication dxtool uses the provided files and execute the restore scriptapplication task. Command dxclient restore-scriptapplication -wcmContentId <value> -versionName <version-name> Help command This command shows the help information for restore-scriptapplication command usage: dxclient restore-scriptapplication -h Command options Use this attribute to specify the protocol with which to connect to the DX server: -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server: -dxPort <value> Use this attribute to specify the path to the content handler servlet on DX server (example, /wps/mycontenthandler ): -contenthandlerPath <value> Use this attribute to specify the context of the virtual portal that contains the Script Application instance that you want to retrieve, if any: -virtualPortalContext <value> Use this attribute to specify the context of the portal project that manages the publication of changes to the Script Application instance, if any: -projectContext <value> Use this attribute to specify the username to authenticate with the DX server: -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the WCM content ID of the Script Application content item: -wcmContentId <value> Use this attribute to specify the versionName for the Script Application. -versionName <value> Use this attribute to specify the restore as a draft or replace the published version of Script Application. -restoreAsPublished <value> All the above command options can also be configured inside the config.json configuration file of the DXClient tool, available in the <working-directory>/store directory of the DXClient installation. Note: If you have installed DXClient using the node package file, then you can find the config.json file in the following path: dist/src/configuration. Command options passed through the command line will override values set in the config.json file. Log files from command execution can be found in the logs directory of the DXClient installation. Example usage: dxclient restore-scriptapplication -wcmContentID <wcm-content-id> -versionName <version-name> -restoreAsPublished <restore-as-published>","title":"Script applications"},{"location":"containerization/Management/scriptapplications/#script-applications","text":"This topic provides information about the deployment, undeployment, and restoration of script applications.","title":"Script applications"},{"location":"containerization/Management/scriptapplications/#deploy-script-applications","text":"The deploy-scriptapplication command is used with the DXClient tool to push or pull Script Applications between a local development workstation or automation server and DX 9.5 CF19 or later servers. The command will push or pull the files that make up a script application to or from a Script Application instance stored in a Web Content Manager library on the server. Required Files : The script application push command in the DXClient tool requires a Script Application zip file or an extracted folder of the same (identified by the prebuiltZip or contentRoot attributes respectively). For more information on Script Applications, refer to the Script Application topics in the HCL DX Help Center. Command This command invokes the deploy-scriptapplication command inside the DXClient tool to either push or pull a script application: dxclient deploy-scriptapplication Subcommands Use this command to create or update the content of a Script Application on the HCL DX server: push [options] Use this command to download the content of a Script Application from the HCL DX server: pull [options] Help command This command shows the help document on the deploy-scriptapplication command: dxclient deploy-scriptapplication pull -h dxclient deploy-scriptapplication push -h Options for the pull subcommand Use this attribute to specify the protocol with which to connect to the DX server: -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server: -dxPort <value> Use this attribute to specify the path to the content handler servlet on DX server (example: /wps/mycontenthandler ): -contenthandlerPath <value> Use this attribute to specify the context of the virtual portal that contains the Script Application instance that you want to retrieve, if any: -virtualPortalContext <value> Use this attribute to specify the context of the portal project that manages the publication of changes to the Script Application instance, if any: -projectContext <value> Use this attribute to specify the username to authenticate with the DX server: -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the WCM content ID of the Script Application content item: -wcmContentId <value> Options for the push subcommand Use this attribute to specify the protocol with which to connect to the DX server: -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server: -dxPort <value> Use this attribute to specify the path to the content handler servlet on the DX server (e.g. /wps/mycontenthandler): -contenthandlerPath <value> Use this attribute to specify the context of the virtual portal that should receive the Script Application instance being pushed, if any: -virtualPortalContext <value> Use this attribute to specify the context of the portal project that manages the publication of changes to the Script Application instance, if any: -projectContext <value> Use this attribute to specify the username to authenticate with the DX server: -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the WCM ID of the Script Application content item: -wcmContentId <value> Use this attribute to specify the SiteArea containing the Script Application content item: -wcmSiteArea <value> Use this attribute to specify the name of the Script Application content item to be created or updated: -wcmContentName <value> Use this attribute to specify the full WCM path of the Script Application content item to be created or updated: -wcmContentPath <value> Use this attribute to set or update the title of the Script Application content item: -wcmContentTitle <value> Use this attribute to specify the path to the main HTML file within the Script Application: -mainHtmlFile <value> Use this attribute to specify the absolute or relative path to the Script Application's content as a ZIP file: -prebuiltZip <value> Use this attribute to specify the absolute or relative path to the Script Application's content in a directory: contentRoot <value> Notes: At least one of (a) wcmContentId , (b) wcmContentPath or (c) both wcmContentName and wcmSiteArea must be specified. If multiple options are provided, then the priority order goes as follows: (a), then (b), and then (c). Use wcmContentId only if you are updating an existing Script Application instance - for new Script Applications specify either (a) wcmContentPath or (b) both wcmContentName and wcmSiteArea . mainHtmlFile is mandatory. The outputfile for pull will be generated inside store/outputFiles/sp-pull-output. When prebuiltZip is specified, the main HTML file path must be relative to the top-level directory in the compressed file. All of the above command options can also be configured inside the config.json configuration file of the DXClient tool, available in the <working-directory>/store directory of the DXClient installation. Note: If you have installed DXClient using the node package file, then you can find the config.json file in the following path: dist/src/configuration. Command options passed through the command line will override values set in the config.json file. Example Usage: For Script Application Pull: dxclient deploy-scriptapplication pull -wcmContentId <wcmContentId> If all required options are configured in config.json of the DX Client tool, then execute: dxclient deploy-scriptapplication pull For Script Application Push, if the Script Application is extracted to a folder named temp at the root of the DXClient machine: dxclient deploy-scriptapplication push -contentRoot /temp -wcmSiteArea \"Script Application Library/Script Applications/\" -wcmContentName DemoScriptApplication If the Script Application is available as a .zip file in a folder named temp on the DXClient tool location, execute: dxclient deploy-scriptapplication push -prebuiltZip /temp/DemoScriptApplication.zip -wcmSiteArea \"Script Application Library/Script Applications/\" -wcmContentName DemoScriptApplication If all required options are configured in the config.json at the /dist/src/configuration path of the DXClient tool, then execute: dxclient deploy-scriptapplication push","title":"Deploy script applications"},{"location":"containerization/Management/scriptapplications/#undeploy-script-applications","text":"The undeploy-scriptapplication command is used to remove a script application from a target HCL DX 9.5 CF192 or later servers. Required file This command invokes the undeploy-scriptapplication tool inside the DXClient. The undeploy-scriptapplication dxtool uses the provided files and execute the undeploy scriptapplication task. Command dxclient undeploy-scriptapplication -wcmContentId <value> Help command This command shows the help information for undeploy-scriptapplication command usage: dxclient undeploy-scriptapplication -h Command options Use this attribute to specify the protocol with which to connect to the DX server: -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server: -dxPort <value> Use this attribute to specify the path to the content handler servlet on DX server (example, /wps/mycontenthandler ): -contenthandlerPath <value> Use this attribute to specify the context of the virtual portal that contains the Script Application instance that you want to retrieve, if any: -virtualPortalContext <value> Use this attribute to specify the context of the portal project that manages the publication of changes to the Script Application instance, if any: -projectContext <value> Use this attribute to specify the username to authenticate with the DX server: -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the WCM content ID of the Script Application content item: -wcmContentId <value> Use this tag to forcefully delete the Script Application. -f All the above command options can also be configured inside the config.json configuration file of the DXClient tool, available in the <working-directory>/store directory of the DXClient installation. Note: If you have installed DXClient using the node package file, then you can find the config.json file in the following path: dist/src/configuration. Command options passed through the command line will override values set in the config.json file. Log files from command execution can be found in the logs directory of the DXClient installation. Example usage: dxclient undeploy-scriptapplication -wcmContentId <wcm-content-id> dxclient undeploy-scriptapplication -wcmContentId <wcm-content-id> -f","title":"Undeploy script applications"},{"location":"containerization/Management/scriptapplications/#restore-script-application","text":"The restore-scriptapplication command is used to restore a script application into one of its previous versions present in the target HCL DX 9.5 CF 19 or later servers. Required file This command invokes the restore-scriptapplication tool inside the DXClient. The restore-scriptapplication dxtool uses the provided files and execute the restore scriptapplication task. Command dxclient restore-scriptapplication -wcmContentId <value> -versionName <version-name> Help command This command shows the help information for restore-scriptapplication command usage: dxclient restore-scriptapplication -h Command options Use this attribute to specify the protocol with which to connect to the DX server: -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server: -dxPort <value> Use this attribute to specify the path to the content handler servlet on DX server (example, /wps/mycontenthandler ): -contenthandlerPath <value> Use this attribute to specify the context of the virtual portal that contains the Script Application instance that you want to retrieve, if any: -virtualPortalContext <value> Use this attribute to specify the context of the portal project that manages the publication of changes to the Script Application instance, if any: -projectContext <value> Use this attribute to specify the username to authenticate with the DX server: -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the WCM content ID of the Script Application content item: -wcmContentId <value> Use this attribute to specify the versionName for the Script Application. -versionName <value> Use this attribute to specify the restore as a draft or replace the published version of Script Application. -restoreAsPublished <value> All the above command options can also be configured inside the config.json configuration file of the DXClient tool, available in the <working-directory>/store directory of the DXClient installation. Note: If you have installed DXClient using the node package file, then you can find the config.json file in the following path: dist/src/configuration. Command options passed through the command line will override values set in the config.json file. Log files from command execution can be found in the logs directory of the DXClient installation. Example usage: dxclient restore-scriptapplication -wcmContentID <wcm-content-id> -versionName <version-name> -restoreAsPublished <restore-as-published>","title":"Restore Script Application"},{"location":"containerization/Management/sharedlibrary/","text":"Shared library Shared libraries are jar files representing code that is shared across multiple components of the customer, for example, portlets, themes, preprocessors, and others. Shared library The shared-library command is used to manage the jar files in the provided default shared library location. Default shared Library: DXCLib Default shared library location: <dx-server-profile>/PortalServer/sharedLibrary Note: For Shared Library artifact, the DX Server needs to be at HCL DX 9.5 CF196 or higher. The default shared library DXCLib is already configured and associated to application server. The shared-library command uses two sub-commands upload and delete to manage files in the DX server. The sub-command upload is used to upload jar files and sub-command delete is used to delete the files from the default shared library location provided below. Command Description This command invokes the shared library upload task inside the DXClient. This is used to upload jar files into the default shared library location. dxclient shared-library upload This command invokes the shared library delete task inside the DXClient. This is used to delete jar files from the default shared library location. dxclient shared-library delete Help command This command shows the help information for shared-library upload command usage: dxclient shared-library upload -h This command shows the help information for shared-library delete command usage: dxclient shared-library delete -h Common Command options Use this attribute to specify the username that is required for authenticating with the server -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server -dxPassword <value> Use this attribute to specify the config wizard home (route change only in case of Open Shift Kubernetes Environment, otherwise same as hostname) that is required for authenticating to the cw_profile -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile(for Kubernetes Environment dxConnectPort is 443) -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile. -dxConnectPassword <value> Use this attribute to specify the profile name of the DX Core -dxProfileName <Name of the DX server profile> Command option for upload Use this attribute to specify the path to a jar/zip file or folder containing jars in it. -libFilePath <value> Command option for delete Use this attribute to specify the names of the jar files present in the shared library location on the server. -libFileNames <value> Note: For upload, the folder or zip file should contain only jars files that are to be uploaded to the default shared library location. All the above command options can also be configured inside the config.json configuration file of the DXClient tool, available in the <working-directory>/store directory of the DXClient installation. Note: If you have installed DXClient using the node package file, then you can find the config.json file in the following path: dist/src/configuration Example usage Use this attribute to specify the path to a jar/zip file or folder containing jars in it. ``` dxclient shared-library upload -dxUsername -dxPassword -dxConnectHostname -dxConnectPort -dxConnectUsername -dxConnectPassword -dxProfileName -libFilePath dxclient shared-library delete -dxUsername -dxPassword -dxConnectHostname -dxConnectPort -dxConnectUsername -dxConnectPassword -dxProfileName -libFileNames -libFilePath ```","title":"Shared library"},{"location":"containerization/Management/sharedlibrary/#shared-library","text":"Shared libraries are jar files representing code that is shared across multiple components of the customer, for example, portlets, themes, preprocessors, and others.","title":"Shared library"},{"location":"containerization/Management/sharedlibrary/#shared-library_1","text":"The shared-library command is used to manage the jar files in the provided default shared library location. Default shared Library: DXCLib Default shared library location: <dx-server-profile>/PortalServer/sharedLibrary Note: For Shared Library artifact, the DX Server needs to be at HCL DX 9.5 CF196 or higher. The default shared library DXCLib is already configured and associated to application server. The shared-library command uses two sub-commands upload and delete to manage files in the DX server. The sub-command upload is used to upload jar files and sub-command delete is used to delete the files from the default shared library location provided below. Command Description This command invokes the shared library upload task inside the DXClient. This is used to upload jar files into the default shared library location. dxclient shared-library upload This command invokes the shared library delete task inside the DXClient. This is used to delete jar files from the default shared library location. dxclient shared-library delete Help command This command shows the help information for shared-library upload command usage: dxclient shared-library upload -h This command shows the help information for shared-library delete command usage: dxclient shared-library delete -h Common Command options Use this attribute to specify the username that is required for authenticating with the server -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server -dxPassword <value> Use this attribute to specify the config wizard home (route change only in case of Open Shift Kubernetes Environment, otherwise same as hostname) that is required for authenticating to the cw_profile -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile(for Kubernetes Environment dxConnectPort is 443) -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile. -dxConnectPassword <value> Use this attribute to specify the profile name of the DX Core -dxProfileName <Name of the DX server profile> Command option for upload Use this attribute to specify the path to a jar/zip file or folder containing jars in it. -libFilePath <value> Command option for delete Use this attribute to specify the names of the jar files present in the shared library location on the server. -libFileNames <value> Note: For upload, the folder or zip file should contain only jars files that are to be uploaded to the default shared library location. All the above command options can also be configured inside the config.json configuration file of the DXClient tool, available in the <working-directory>/store directory of the DXClient installation. Note: If you have installed DXClient using the node package file, then you can find the config.json file in the following path: dist/src/configuration Example usage Use this attribute to specify the path to a jar/zip file or folder containing jars in it. ``` dxclient shared-library upload -dxUsername -dxPassword -dxConnectHostname -dxConnectPort -dxConnectUsername -dxConnectPassword -dxProfileName -libFilePath dxclient shared-library delete -dxUsername -dxPassword -dxConnectHostname -dxConnectPort -dxConnectUsername -dxConnectPassword -dxProfileName -libFileNames -libFilePath ```","title":"Shared library"},{"location":"containerization/Management/syndicatorsandsubscribers/","text":"Managing Web Content Syndicators and Subscribers using DXClient The section provides information about using the DXClient process to automate the management of Web Content Manager Syndicators, Subscribers, and get-syndication reports. For more information on the process and settings of the Web Content Manager Syndicators and Subscribers, see How to manage syndicators and subscribers . Managing syndicators The manage-syndicator command is used to enable or disable the syndicator using the provided input. Command description This command invokes the manage-syndicator tool inside the DXClient. It is used to enable or disable the syndicator. dxclient manage-syndicator Help command This command shows the help document on the manage-syndicator command usage: dxclient manage-syndicator -h Command options Use this attribute to specify the hostname of the target server. -hostname <value> Use this attribute to specify the protocol with which to connect to the server. -dxProtocol <value> Use this attribute to specify the port on which to connect to the server(for Kubernetes Environment dxPort is 443) -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server. -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server. -dxPassword <value> Use this attribute to specify the path to the contenthandler servlet on the DX server (example: /wps/mycontenthandler). -contenthandlerPath <value> Use this attribute to specify the UUID of the syndicator instance. -UUID <value> Use true or false to enable or disable the syndicator. -enable <value> All the above command options can also be configured inside the config.json configuration file of the DXClient tool, available in the <working-directory>/store directory of the DXClient installation. Note: If you have installed DXClient using the node package file, then you can find the config.json file in the following path: dist/src/configuration. The options that are passed through the command line override the default values. Example Usage: dxclient manage-syndicator -dxProtocol <dxProtocol> -hostname <host-name> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -contenthandlerPath <contenthandlerPath> -UUID <UUID> -enable <enable> Manage-syndicator get-syndication-report The manage-syndicator get-syndication-report command is used to fetch the failed reports of the syndicator. Command description This command invokes the syndicator-faileditems tool inside the DXClient, which is used to fetch the failed reports. dxclient manage-syndicator get-syndication-report Help command This command shows the help document on the manage-syndicator get-syndication-report command usage: dxclient manage-syndicator get-syndication-report -h Command options Use this attribute to specify the hostname of the target server -hostname <value> Use this attribute to specify the protocol with which to connect to the server -dxProtocol <value> Use this attribute to specify the port on which to connect to the server (for Kubernetes Environment, dxPort is 443) -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server -dxPassword <value> Use this attribute to specify the path to the contenthandler servlet on the DX server (for example, /wps/mycontenthandler) -contenthandlerPath <value> Use this attribute to specify the UUID of the syndicator instance -UUID <value> All the above command options can also be configured inside the config.json configuration file of the DXClient tool, available in the <working-directory>/store directory of the DXClient installation. This file is read by default. Note: If you have installed DXClient using the node package file, then you can find the config.json file in the following path: dist/src/configuration The options that are passed through the command line override the default values. Example usage with UUID: dxclient manage-syndicator get-syndication-report -dxProtocol <dxProtocol> -hostname <host-name> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -contenthandlerPath <contenthandlerPath> -UUID <UUID> Example usage without UUID: dxclient manage-syndicator get-syndication-report -dxProtocol <dxProtocol> -hostname <host-name> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -contenthandlerPath <contenthandlerPath> Note: If UUID of a syndicator is specified, then the command provides the report for only the particular syndicator that is present in the target DX Server; otherwise, it provides the failure report for all syndicators. Managing subscribers The manage-subscriber command is used to enable or disable the subscriber using the provided input. Command description This command invokes the manage-subscriber tool inside the DXClient. It is used to enable/disable the subscriber. dxclient manage-subscriber Help command This command shows the help document on the manage-syndicator command usage: dxclient manage-subscriber -h Command options Use this attribute to specify the hostname of the target server. -hostname <value> Use this attribute to specify the protocol with which to connect to the server. -dxProtocol <value> Use this attribute to specify the port on which to connect to the server(for Kubernetes Environment dxPort is 443). -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server. -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server. -dxPassword <value> Use this attribute to specify the path to the contenthandler servlet on the DX server (for example, /wps/mycontenthandler). -contenthandlerPath <value> Use this attribute to specify the UUID of the subscriber instance. -UUID <value> Use this attribute to specify the enable or disable the subscriber instance. Use true or false to enable or disable the subscriber. -enable <value> All the above command options can also be configured inside the config.json configuration file of the DXClient tool, available in the <working-directory>/store directory of the DXClient installation. Note: If you have installed DXClient using the node package file, then you can find the config.json file in the following path: dist/src/configuration The options passed through command line overrides the default values. Example Usage: dxclient manage-subscriber -dxProtocol <dxProtocol> -hostname <host-name> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -contenthandlerPath <contenthandlerPath> -UUID <UUID> -enable <enable> Create Syndication Relation The create-syndication-relation command is used to create the syndication relation between syndicator and subscriber in the DX server. Command description Use the create-syndication-relation to create syndication relation: dxclient create-syndication-relation Help command This command shows the help information for create-syndication-relation command usage: dxclient create-syndication-relation -h Command options Use this attribute to specify the protocol with which to connect to the DX server: -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server (for Kubernetes Environment, dxPort is 443): -dxPort <value> Use this attribute to specify the username to authenticate with the DX server -dxUsername <value> Use this attribute to specify the password for the user in the \"dxUsername\" attribute -dxPassword <value> The path to the contenthandler servlet on the Script Application server: -contenthandlerPath <value> Syndicator URL of target server, for example, http(s)://host:port/wps/wcm: -syndicatorUrl <value> Use this attribute to specify the new syndicator name: -syndicatorName <value> Use this attribute to specify the new subscriber name: -subscriberName <value> Use this attribute to specify the Credential Vault Name of source server: -vaultSlotName <value> Whether the syndicator/subscriber pair is enabled on creation: isEnabled (default is true): -isEnabled <value> Whether the syndicator/subscriber pair is updateAfterCreation : updateAfterCreation (default is true): -updateAfterCreation <value> The libraries to syndicate eg. all-items,liveItems,liveProjectsItem,all-items,published-items and all-items-and-versions: -syndicationType <value> Use this attribute to specify the Libraries Name of target Server: -webContentLibraries <value> Use this attribute to specify the Subscriber URL, for example, http(s)://host:port/wps/wcm: -subscriberURL <value> Use this attribute to specify the Syndicator/subscriber mode: -mode <value> Use this attribute to specify the Credential Vault Name: -syndicatorVaultSlotName <value> Use this attribute to specify the path to the context root on the DX server (for example, /wps): -dxContextRoot <value> Use this attribute to specify the path to the Virtual portal Context: -virtualPortalContext <value> All the command options are configured in the config.json file of the tool, which is read by default. The configuration file is located at <working-directory>/store/config.json . The options that are passed through the command line overrides the default values. Log files from command execution can be found in the logs directory of the DXClient installation. Example usage: ``` dxclient create-syndication-relation -dxProtocol -hostname -dxPort -contenthandlerPath -dxUsername -dxPassword -syndicatorUrl -syndicatorName -subscriberName -vaultSlotName -isEnabled -updateAfterCreation -syndicationType -webContentLibraries -subscriberURL -mode -syndicatorVaultSlotName -dxContextRoot -virtualPortalContext ```","title":"Managing Web Content Syndicators and Subscribers using DXClient"},{"location":"containerization/Management/syndicatorsandsubscribers/#managing-web-content-syndicators-and-subscribers-using-dxclient","text":"The section provides information about using the DXClient process to automate the management of Web Content Manager Syndicators, Subscribers, and get-syndication reports. For more information on the process and settings of the Web Content Manager Syndicators and Subscribers, see How to manage syndicators and subscribers .","title":"Managing Web Content Syndicators and Subscribers using DXClient"},{"location":"containerization/Management/syndicatorsandsubscribers/#managing-syndicators","text":"The manage-syndicator command is used to enable or disable the syndicator using the provided input. Command description This command invokes the manage-syndicator tool inside the DXClient. It is used to enable or disable the syndicator. dxclient manage-syndicator Help command This command shows the help document on the manage-syndicator command usage: dxclient manage-syndicator -h Command options Use this attribute to specify the hostname of the target server. -hostname <value> Use this attribute to specify the protocol with which to connect to the server. -dxProtocol <value> Use this attribute to specify the port on which to connect to the server(for Kubernetes Environment dxPort is 443) -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server. -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server. -dxPassword <value> Use this attribute to specify the path to the contenthandler servlet on the DX server (example: /wps/mycontenthandler). -contenthandlerPath <value> Use this attribute to specify the UUID of the syndicator instance. -UUID <value> Use true or false to enable or disable the syndicator. -enable <value> All the above command options can also be configured inside the config.json configuration file of the DXClient tool, available in the <working-directory>/store directory of the DXClient installation. Note: If you have installed DXClient using the node package file, then you can find the config.json file in the following path: dist/src/configuration. The options that are passed through the command line override the default values. Example Usage: dxclient manage-syndicator -dxProtocol <dxProtocol> -hostname <host-name> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -contenthandlerPath <contenthandlerPath> -UUID <UUID> -enable <enable>","title":"Managing syndicators"},{"location":"containerization/Management/syndicatorsandsubscribers/#manage-syndicator-get-syndication-report","text":"The manage-syndicator get-syndication-report command is used to fetch the failed reports of the syndicator. Command description This command invokes the syndicator-faileditems tool inside the DXClient, which is used to fetch the failed reports. dxclient manage-syndicator get-syndication-report Help command This command shows the help document on the manage-syndicator get-syndication-report command usage: dxclient manage-syndicator get-syndication-report -h Command options Use this attribute to specify the hostname of the target server -hostname <value> Use this attribute to specify the protocol with which to connect to the server -dxProtocol <value> Use this attribute to specify the port on which to connect to the server (for Kubernetes Environment, dxPort is 443) -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server -dxPassword <value> Use this attribute to specify the path to the contenthandler servlet on the DX server (for example, /wps/mycontenthandler) -contenthandlerPath <value> Use this attribute to specify the UUID of the syndicator instance -UUID <value> All the above command options can also be configured inside the config.json configuration file of the DXClient tool, available in the <working-directory>/store directory of the DXClient installation. This file is read by default. Note: If you have installed DXClient using the node package file, then you can find the config.json file in the following path: dist/src/configuration The options that are passed through the command line override the default values. Example usage with UUID: dxclient manage-syndicator get-syndication-report -dxProtocol <dxProtocol> -hostname <host-name> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -contenthandlerPath <contenthandlerPath> -UUID <UUID> Example usage without UUID: dxclient manage-syndicator get-syndication-report -dxProtocol <dxProtocol> -hostname <host-name> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -contenthandlerPath <contenthandlerPath> Note: If UUID of a syndicator is specified, then the command provides the report for only the particular syndicator that is present in the target DX Server; otherwise, it provides the failure report for all syndicators.","title":"Manage-syndicator get-syndication-report"},{"location":"containerization/Management/syndicatorsandsubscribers/#managing-subscribers","text":"The manage-subscriber command is used to enable or disable the subscriber using the provided input. Command description This command invokes the manage-subscriber tool inside the DXClient. It is used to enable/disable the subscriber. dxclient manage-subscriber Help command This command shows the help document on the manage-syndicator command usage: dxclient manage-subscriber -h Command options Use this attribute to specify the hostname of the target server. -hostname <value> Use this attribute to specify the protocol with which to connect to the server. -dxProtocol <value> Use this attribute to specify the port on which to connect to the server(for Kubernetes Environment dxPort is 443). -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server. -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server. -dxPassword <value> Use this attribute to specify the path to the contenthandler servlet on the DX server (for example, /wps/mycontenthandler). -contenthandlerPath <value> Use this attribute to specify the UUID of the subscriber instance. -UUID <value> Use this attribute to specify the enable or disable the subscriber instance. Use true or false to enable or disable the subscriber. -enable <value> All the above command options can also be configured inside the config.json configuration file of the DXClient tool, available in the <working-directory>/store directory of the DXClient installation. Note: If you have installed DXClient using the node package file, then you can find the config.json file in the following path: dist/src/configuration The options passed through command line overrides the default values. Example Usage: dxclient manage-subscriber -dxProtocol <dxProtocol> -hostname <host-name> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -contenthandlerPath <contenthandlerPath> -UUID <UUID> -enable <enable>","title":"Managing subscribers"},{"location":"containerization/Management/syndicatorsandsubscribers/#create-syndication-relation","text":"The create-syndication-relation command is used to create the syndication relation between syndicator and subscriber in the DX server. Command description Use the create-syndication-relation to create syndication relation: dxclient create-syndication-relation Help command This command shows the help information for create-syndication-relation command usage: dxclient create-syndication-relation -h Command options Use this attribute to specify the protocol with which to connect to the DX server: -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server (for Kubernetes Environment, dxPort is 443): -dxPort <value> Use this attribute to specify the username to authenticate with the DX server -dxUsername <value> Use this attribute to specify the password for the user in the \"dxUsername\" attribute -dxPassword <value> The path to the contenthandler servlet on the Script Application server: -contenthandlerPath <value> Syndicator URL of target server, for example, http(s)://host:port/wps/wcm: -syndicatorUrl <value> Use this attribute to specify the new syndicator name: -syndicatorName <value> Use this attribute to specify the new subscriber name: -subscriberName <value> Use this attribute to specify the Credential Vault Name of source server: -vaultSlotName <value> Whether the syndicator/subscriber pair is enabled on creation: isEnabled (default is true): -isEnabled <value> Whether the syndicator/subscriber pair is updateAfterCreation : updateAfterCreation (default is true): -updateAfterCreation <value> The libraries to syndicate eg. all-items,liveItems,liveProjectsItem,all-items,published-items and all-items-and-versions: -syndicationType <value> Use this attribute to specify the Libraries Name of target Server: -webContentLibraries <value> Use this attribute to specify the Subscriber URL, for example, http(s)://host:port/wps/wcm: -subscriberURL <value> Use this attribute to specify the Syndicator/subscriber mode: -mode <value> Use this attribute to specify the Credential Vault Name: -syndicatorVaultSlotName <value> Use this attribute to specify the path to the context root on the DX server (for example, /wps): -dxContextRoot <value> Use this attribute to specify the path to the Virtual portal Context: -virtualPortalContext <value> All the command options are configured in the config.json file of the tool, which is read by default. The configuration file is located at <working-directory>/store/config.json . The options that are passed through the command line overrides the default values. Log files from command execution can be found in the logs directory of the DXClient installation. Example usage: ``` dxclient create-syndication-relation -dxProtocol -hostname -dxPort -contenthandlerPath -dxUsername -dxPassword -syndicatorUrl -syndicatorName -subscriberName -vaultSlotName -isEnabled -updateAfterCreation -syndicationType -webContentLibraries -subscriberURL -mode -syndicatorVaultSlotName -dxContextRoot -virtualPortalContext ```","title":"Create Syndication Relation"},{"location":"containerization/Management/t_customize_dx_url/","text":"Customizing the HCL DX URL when deployed to container platforms This section describes the procedures to define custom context root URLs, or no context root URL definitions, when deploying your HCL DX 9.5 software to the supported container platforms. Note: Defining the custom context root URL feature is available in HCL DX 9.5 Container Update CF193 and later. Prerequisites and Notes: The following configuration procedure is supported for DX container deployments to Kubernetes and OpenShift platforms. It is not yet supported for DX deployments in a hybrid container deployment and on-premise deployment. The dxctl tool is used for this configuration process. Before running the dxctl tool, the administrator must log on to the targeted DX cluster using the cloud-specific CLI login commands for the supported Kubernetes and OpenShift platforms; such as Microsoft Azure Kubernetes Services (AKS), Google Kubernetes Engine (GKE), Amazon Elastic Kubernetes Service (EKS), or Red Hat OpenShift. For example, to log in to your DX container cluster on the Red Hat OpenShift platform, use the oc login command. Ensure that you have updated the DxDeployment custom resource definition to the HCL DX 9.5 Container Update CF192 and later releases. For more information, see Customizing the container deployment . For more information on the custom URI management for HCL Digital Experience, refer to the following topic: Changing the portal URI after an installation . If you have already defined a custom Digital Experience URL in your existing container deployment, it is mandatory to configure the following properties with the existing values. Otherwise, the properties are updated with the default values. Customize the context root in your Digital Experience container deployment: The following are the default configuration property values for the context root changes. ``` Path dx.path.contextroot: wps dx.path.home: portal dx.path.personalized: myportal ``` To change the default values to your custom requirements, update the following properties. For example: ``` Path dx.path.contextroot: hcl dx.path.home: dx dx.path.personalized: mydx ``` Important: Do not use the same value for the dx.path.home and dx.path.personalized properties. To apply the HCL Digital Experience custom URI during a new DX Container deployment, run the following command: $ dxctl --deploy -p properties/full-deployment.properties To change the custom URI of a previous DX 9.5 Container deployment, then update the properties as specified in Step 1 , and then run the following command: $ dxctl --update -p properties/full-deployment.properties Additional Considerations and Example: Review the following manual, required and optional steps to complete the context root customization updates. Perform the steps that are related to your DX deployment details. (Some optional steps may not apply to your deployment). Optional step: If your DX deployment includes custom themes that use Dojo, update those themes to refer to the correct Dojo context root. The default Dojo context root in HCL Digital Experience is /wps/portal_dojo. After you run the modify-servlet-path and modify-servlet-path-portlets tasks, the Dojo context root is changed to include the new value in the WpsContextRoot parameter as the prefix. For example, if the new WpsContextRoot value is myco, then the new Dojo context root becomes /myco/portal_dojo. If your theme includes hard-coded references to /wps/portal_dojo, update those references to the new context root. If you migrated a custom theme, you might find that it has references to /portal_dojo without the /wps prefix. Look for these references in both the WAR file and in the WebDAV storage for your theme. Required step: Refresh your search collection and select Regather to update the documents. Log on to the Digital Experience platform as the administrator. Navigate to the Practitioner Studio menu. Select Search: Open the Manage Search portlet. Click Search Collections. Click the search collection that you want to update. For example: Default Search Collection. Start the Digital Experience search collection crawler service for each content collection source: Notes: If the documents are not stored in the search collection but a schedule is defined for the crawler, then the crawler automatically runs at the scheduled time. You can also start the crawler manually. If the documents are already collected, then select Regather documents to update the documents with the new context root information. Click Collections from All Services in the breadcrumb trail and select the next search collection to modify. Optional step: From the Web Content interface of Practitioner Studio, update the Web Content Manager syndicator and subscriber servers that reference your modified DX Container site URL. If you do not use syndication, skip this step. Log on to the site that syndicates to this instance. Open the Syndicators page. Click the edit icon by the syndicator that you want to edit. Update the URL with the new context root information. Log on to the site that subscribes to this instance. Open the Subscribers page. Click the edit icon of the subscriber that you want to edit. Update the URL with the new context root information. Configure no context root in your Digital Experience container deployment To configure no context root, update the following property values. For example: ``` Path dx.path.contextroot: \" \" dx.path.home: \" \" dx.path.personalized: mydx dx.ready.path: / dx.live.path: / ``` Note: If the context root is removed, the home path must be removed as well. It may take more time than usual for the DX-Core pod to get to a running state during the update process. Before log in (no context root): https://dx-cr-demo-service-dx-cr-01-dx-cr-01.apps.sample.domain.net/!ut/p/z1/04_Sj9CPykssy0xPLMnMz0vMAfljo8ziDVCAo4FTkJGTsYGBu7OJfjhYgbmHi7u7oYFhgL-bu4BoJmrt7e After log in (personalized context root): After log in (personalized context root): https://dx-cr-demo-service-dx-cr-01-dx-cr-01.apps.sample.domain.net/mydx/woodburnstudio/home/!ut/p/z1/04_Sj9CPykssy0xPLMnMz0vMAfljo8ziDVCAo4FTkJGTsYGBu7OJfjhYgbmHi7u7oYFhgL-bu4BoJmrt7e Important: Do not use the same value for the dx.path.home and dx.path.personalized properties. To apply the HCL Digital Experience custom URI during a new DX container deployment, run the following command: $ dxctl --deploy -p properties/full-deployment.properties To change the custom URI of a previous HCL DX 9.5 container deployment, update the property values as specified in step 1, and run the following command: $ dxctl --update -p properties/full-deployment.properties","title":"Customizing the HCL DX URL when deployed to container platforms"},{"location":"containerization/Management/t_customize_dx_url/#customizing-the-hcl-dx-url-when-deployed-to-container-platforms","text":"This section describes the procedures to define custom context root URLs, or no context root URL definitions, when deploying your HCL DX 9.5 software to the supported container platforms. Note: Defining the custom context root URL feature is available in HCL DX 9.5 Container Update CF193 and later. Prerequisites and Notes: The following configuration procedure is supported for DX container deployments to Kubernetes and OpenShift platforms. It is not yet supported for DX deployments in a hybrid container deployment and on-premise deployment. The dxctl tool is used for this configuration process. Before running the dxctl tool, the administrator must log on to the targeted DX cluster using the cloud-specific CLI login commands for the supported Kubernetes and OpenShift platforms; such as Microsoft Azure Kubernetes Services (AKS), Google Kubernetes Engine (GKE), Amazon Elastic Kubernetes Service (EKS), or Red Hat OpenShift. For example, to log in to your DX container cluster on the Red Hat OpenShift platform, use the oc login command. Ensure that you have updated the DxDeployment custom resource definition to the HCL DX 9.5 Container Update CF192 and later releases. For more information, see Customizing the container deployment . For more information on the custom URI management for HCL Digital Experience, refer to the following topic: Changing the portal URI after an installation . If you have already defined a custom Digital Experience URL in your existing container deployment, it is mandatory to configure the following properties with the existing values. Otherwise, the properties are updated with the default values. Customize the context root in your Digital Experience container deployment: The following are the default configuration property values for the context root changes. ```","title":"Customizing the HCL DX URL when deployed to container platforms"},{"location":"containerization/Management/t_customize_dx_url/#path","text":"dx.path.contextroot: wps dx.path.home: portal dx.path.personalized: myportal ``` To change the default values to your custom requirements, update the following properties. For example: ```","title":"Path"},{"location":"containerization/Management/t_customize_dx_url/#path_1","text":"dx.path.contextroot: hcl dx.path.home: dx dx.path.personalized: mydx ``` Important: Do not use the same value for the dx.path.home and dx.path.personalized properties. To apply the HCL Digital Experience custom URI during a new DX Container deployment, run the following command: $ dxctl --deploy -p properties/full-deployment.properties To change the custom URI of a previous DX 9.5 Container deployment, then update the properties as specified in Step 1 , and then run the following command: $ dxctl --update -p properties/full-deployment.properties","title":"Path"},{"location":"containerization/Management/t_customize_dx_url/#additional-considerations-and-example","text":"Review the following manual, required and optional steps to complete the context root customization updates. Perform the steps that are related to your DX deployment details. (Some optional steps may not apply to your deployment). Optional step: If your DX deployment includes custom themes that use Dojo, update those themes to refer to the correct Dojo context root. The default Dojo context root in HCL Digital Experience is /wps/portal_dojo. After you run the modify-servlet-path and modify-servlet-path-portlets tasks, the Dojo context root is changed to include the new value in the WpsContextRoot parameter as the prefix. For example, if the new WpsContextRoot value is myco, then the new Dojo context root becomes /myco/portal_dojo. If your theme includes hard-coded references to /wps/portal_dojo, update those references to the new context root. If you migrated a custom theme, you might find that it has references to /portal_dojo without the /wps prefix. Look for these references in both the WAR file and in the WebDAV storage for your theme. Required step: Refresh your search collection and select Regather to update the documents. Log on to the Digital Experience platform as the administrator. Navigate to the Practitioner Studio menu. Select Search: Open the Manage Search portlet. Click Search Collections. Click the search collection that you want to update. For example: Default Search Collection. Start the Digital Experience search collection crawler service for each content collection source: Notes: If the documents are not stored in the search collection but a schedule is defined for the crawler, then the crawler automatically runs at the scheduled time. You can also start the crawler manually. If the documents are already collected, then select Regather documents to update the documents with the new context root information. Click Collections from All Services in the breadcrumb trail and select the next search collection to modify. Optional step: From the Web Content interface of Practitioner Studio, update the Web Content Manager syndicator and subscriber servers that reference your modified DX Container site URL. If you do not use syndication, skip this step. Log on to the site that syndicates to this instance. Open the Syndicators page. Click the edit icon by the syndicator that you want to edit. Update the URL with the new context root information. Log on to the site that subscribes to this instance. Open the Subscribers page. Click the edit icon of the subscriber that you want to edit. Update the URL with the new context root information. Configure no context root in your Digital Experience container deployment To configure no context root, update the following property values. For example: ```","title":"Additional Considerations and Example:"},{"location":"containerization/Management/t_customize_dx_url/#path_2","text":"dx.path.contextroot: \" \" dx.path.home: \" \" dx.path.personalized: mydx dx.ready.path: / dx.live.path: / ``` Note: If the context root is removed, the home path must be removed as well. It may take more time than usual for the DX-Core pod to get to a running state during the update process. Before log in (no context root): https://dx-cr-demo-service-dx-cr-01-dx-cr-01.apps.sample.domain.net/!ut/p/z1/04_Sj9CPykssy0xPLMnMz0vMAfljo8ziDVCAo4FTkJGTsYGBu7OJfjhYgbmHi7u7oYFhgL-bu4BoJmrt7e After log in (personalized context root): After log in (personalized context root): https://dx-cr-demo-service-dx-cr-01-dx-cr-01.apps.sample.domain.net/mydx/woodburnstudio/home/!ut/p/z1/04_Sj9CPykssy0xPLMnMz0vMAfljo8ziDVCAo4FTkJGTsYGBu7OJfjhYgbmHi7u7oYFhgL-bu4BoJmrt7e Important: Do not use the same value for the dx.path.home and dx.path.personalized properties. To apply the HCL Digital Experience custom URI during a new DX container deployment, run the following command: $ dxctl --deploy -p properties/full-deployment.properties To change the custom URI of a previous HCL DX 9.5 container deployment, update the property values as specified in step 1, and run the following command: $ dxctl --update -p properties/full-deployment.properties","title":"Path"},{"location":"containerization/Management/themes/","text":"Themes This topic provides information about the deployment and undeployment of themes artifacts. Deploy theme The deploy-theme command is used to deploy a theme (EAR and WebDAV based) from a source client or server environment to a target HCL DX 9.5 CF192 or later server using the provided theme registration XML file, deployable EAR file, and WebDAV theme collection. Required files Theme Registration XML file: This XML file is required to register the theme into DX Server. Theme deployable EAR file: This EAR file containing theme data is used for deploying into the WebSphere Application Server. WebDAV theme collection: The theme collection folder/zip is used to create or update the collection in WebDAV file store of the DX Server. Notes: This command can execute below one or more tasks together: Theme Registration Theme EAR deployment WebDAV theme collection Command dxclient deploy-theme -xmlFile <path> -applicationFile <path> -applicationName <application name> -themeName <theme collection name> -themePath <folder/zip path of theme collection> Help command This command shows the help document on the deploy-theme command usage: dxclient deploy-theme -h Common Command options Use this attribute to specify the protocol with which to connect to the DX server ( wp_profile ): -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server ( wp_profile ): -dxPort <value> Use this attribute to specify the username to authenticate with the DX server ( wp_profile ): -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Required options for Theme Registration Use this attribute to specify the local path to the theme registration XML file: -xmlFile <Absolute or relative path to theme registration xml input file> Use this attribute to specify the path to DX configuration endpoint (for example, /wps/config): -xmlConfigPath <value> Note: For theme registration, a backup of the complete DX configuration export (not including users) is taken and placed in store/outputFiles/themes/backup/foldername folder. Required options for Theme EAR deployment Use this attribute to specify the Configuration Wizard Console port number: -dxConnectPort <value> Use this attribute to specify the config wizard home (change to the appropriate route in the case of an OpenShift Kubernetes Environment, otherwise the value is typically the same as the hostname) that is required for authenticating with the DXConnect application: -dxConnectHostname <value> Use this attribute to specify the Configuration Wizard Administrator username that is required for authenticating with the DXConnect application: -dxConnectUsername <value> Use this attribute to specify the Configuration Wizard Administrator password that is required for authenticating with the DXConnect application: -dxConnectPassword <value> Use this attribute to specify Soap port of the DX server: -dxSoapPort <Soap port of the DX server> Specify either the dxProfileName or dxProfilePath of the DX core server: Use this attribute to specify the profile name of the DX core server (for example: wp_profile ) -dxProfileName <Profile name of the DX core server> OR Use this attribute to specify the profile path of the DX server (for example: /opt/HCL/wp_profile ) -dxProfilePath <Path of the DX core server profile> Use this attribute to specify the theme EAR file path that is required while executing the deploy theme task: \u2013applicationFile <Absolute or relative path to deployable theme ear file> Use this attribute to specify the theme application name: -applicationName <value> Required options for creating/updating WebDAV theme collection Use this attribute to specify the theme name of the collection created under WebDAV server in DX: -themeName <value> Use this attribute to specify the theme file path that contains all static files to be pushed into DX theme, it accepts either folder or zip file path of the WebDAV theme collection: -themePath <value> Use this attribute to specify the path to the contenthandler servlet on the DX server (e.g. /wps/mycontenthandler): -contenthandlerPath <value> Notes: For new WebDAV theme collection, DXClient tool adds the provided collection (folder/zip) to the WebDAV file store. For existing WebDAV theme collection, the existing theme collection is replaced by the provided theme collection during the update. To get the latest theme collection from the DX server, see Exporting content from the filestore and make modifications on the same folder to get it updated in the DX Server WebDAV file store. For WebDAV theme collection update, a backup of the existing theme collection is taken and placed in store/outputFiles/themes/backup/foldername folder. All the above command options can also be configured inside the config.json configuration file of the DXClient tool, available in the <working-directory>/store directory of the DXClient installation. Note: If you have installed DXClient using the node package file, then you can find the config.json file in the following path: dist/src/configuration/config.json Log files from command execution can be found in the logs directory of the DXClient installation. Example usage: dxclient deploy-theme -dxProtocol <http/https> -hostname <host-name> -dxPort <dxPort> -xmlConfigPath <xmlConfigPath> -dxUsername <dxUsername> -dxPassword <dxPassword> -dxSoapPort <dxSoapPort> -dxConnectHostname <hostname> -dxConnectPort <dxConnectPort> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -xmlFile <xml-file-with-path> -applicationFile <application-file-with-path> -applicationName <application name> -themeName <theme-name> -themePath <theme-path> -dxProfileName <Profile name of the DX core server> Undeploy theme The undeploy-theme command can be used to undeploy a theme, including the EAR application and WebDAV files, and it also unregisters the registered theme from the target DX server. Note: If enableBackup is set to true, then undeploy-theme takes a backup of the deployed EAR theme, WebDAV theme collection, and completes DX configuration export (without users) and place it in the backup folder. When the user is downloading EAR, WeDAV, and XML to backup, we must separate it by the folder names store/outputFiles/themes/backup/foldername . The backup of EAR is placed in store/outputFiles/themes/backup/application . Users can restore the theme by using the backup files. Note: Pages might lose the applied theme references in the restored themes. Command description This command invokes the undeploy-theme tool inside the DXClient. This command uses the unregistered theme XML file, theme EAR application name and WebDAV theme collection name, and executes the undeploy theme task. ``` dxclient undeploy-theme ``` Help command This command shows the help document on the undeploy-theme command usage: dxclient undeploy-theme -h Required files Theme Unregistration XML file: This XML file is required to unregister the theme from target DX Server and must contain the details of the theme. The XML file must be provided when executing the undeploy theme task. This command can execute one or more of the following tasks at the same time: Theme unregistration Undeploy theme EAR application Undeploy WebDAV theme collection Common commands Use this attribute to specify the hostname of the target server -hostname <value> Use this attribute to specify the protocol with which to connect to the server -dxProtocol <value> Use this attribute to specify the port on which to connect to the server (for Kubernetes Environment, dxPort is 443 ) -dxPort <value> Use this attribute to specify the username that is required for server authentication -dxUsername <value> Use this attribute to specify the password that is required for server authentication -dxPassword <value> Use this attribute to specify the path to the contenthandler servlet on the DX server (e.g. /wps/mycontenthandler) -contenthandlerPath <value> Use this attribute to take the backup before undeploying theme -enableBackup <value> Note: User can set the enableBackup parameter as true to take backup before undeploying theme. The value is set to false by default. The options passed through command line override the default values. Required options for Theme Unregistration: Use this attribute to specify the theme registration xml file that is used while executing the undeploy theme task. For example, see the Theme-registration.xml file in the directory dxclient/samples/ -xmlFile <xml file name with absolute path of the xmlaccess input file> Use this attribute to specify the path to DX configuration endpoint -xmlConfigPath <value> Required options for undeploying theme EAR application: Use this attribute to specify the configuration wizard home (change of route is only in case of Open Shift Kubernetes Enviornment, otherwise the route remains the same as hostname) that is required for authenticating to the cw_profile -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile (for Kubernetes Environment dxConnectPort is 443) -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile -dxConnectPassword <value> Use this attribute to specify Soap port of the DX server -dxSoapPort <Soap port of the DX server> Specify either the dxProfileName or dxProfilePath of the DX core server: Use this attribute to specify the profile name of the DX core server (for example: wp_profile ) -dxProfileName <Profile name of the DX core server> OR Use this attribute to specify the profile path of the DX server (for example: /opt/HCL/wp_profile ) -dxProfilePath <Path of the DX core server profile> Use this attribute to specify the EAR application name -applicationName <value> Required options for undeploying WebDAV theme collection: Use this attribute to specify the theme name of the collection created under WebDAV -themeName <value> All the above command options can also be configured inside the config.json configuration file of the DXClient tool, available in the <working-directory>/store directory of the DXClient installation. Note: If you have installed DXClient using the node package file, then you can find the config.json file in the following path: dist/src/configuration/config.json Example usage: dxclient undeploy-theme -dxProtocol <http/https> -hostname <host-name> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -dxSoapPort <dxSoapPort> -dxConnectHostname <hostname> -dxConnectPort <dxConnectPort> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -xmlFile <xml-file-with-path> -applicationName <application name> -themeName <theme-name> -enableBackup <enable-backup> -dxProfileName <Profile name of the DX core server profile>","title":"Themes"},{"location":"containerization/Management/themes/#themes","text":"This topic provides information about the deployment and undeployment of themes artifacts.","title":"Themes"},{"location":"containerization/Management/themes/#deploy-theme","text":"The deploy-theme command is used to deploy a theme (EAR and WebDAV based) from a source client or server environment to a target HCL DX 9.5 CF192 or later server using the provided theme registration XML file, deployable EAR file, and WebDAV theme collection. Required files Theme Registration XML file: This XML file is required to register the theme into DX Server. Theme deployable EAR file: This EAR file containing theme data is used for deploying into the WebSphere Application Server. WebDAV theme collection: The theme collection folder/zip is used to create or update the collection in WebDAV file store of the DX Server. Notes: This command can execute below one or more tasks together: Theme Registration Theme EAR deployment WebDAV theme collection Command dxclient deploy-theme -xmlFile <path> -applicationFile <path> -applicationName <application name> -themeName <theme collection name> -themePath <folder/zip path of theme collection> Help command This command shows the help document on the deploy-theme command usage: dxclient deploy-theme -h Common Command options Use this attribute to specify the protocol with which to connect to the DX server ( wp_profile ): -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server ( wp_profile ): -dxPort <value> Use this attribute to specify the username to authenticate with the DX server ( wp_profile ): -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Required options for Theme Registration Use this attribute to specify the local path to the theme registration XML file: -xmlFile <Absolute or relative path to theme registration xml input file> Use this attribute to specify the path to DX configuration endpoint (for example, /wps/config): -xmlConfigPath <value> Note: For theme registration, a backup of the complete DX configuration export (not including users) is taken and placed in store/outputFiles/themes/backup/foldername folder. Required options for Theme EAR deployment Use this attribute to specify the Configuration Wizard Console port number: -dxConnectPort <value> Use this attribute to specify the config wizard home (change to the appropriate route in the case of an OpenShift Kubernetes Environment, otherwise the value is typically the same as the hostname) that is required for authenticating with the DXConnect application: -dxConnectHostname <value> Use this attribute to specify the Configuration Wizard Administrator username that is required for authenticating with the DXConnect application: -dxConnectUsername <value> Use this attribute to specify the Configuration Wizard Administrator password that is required for authenticating with the DXConnect application: -dxConnectPassword <value> Use this attribute to specify Soap port of the DX server: -dxSoapPort <Soap port of the DX server> Specify either the dxProfileName or dxProfilePath of the DX core server: Use this attribute to specify the profile name of the DX core server (for example: wp_profile ) -dxProfileName <Profile name of the DX core server> OR Use this attribute to specify the profile path of the DX server (for example: /opt/HCL/wp_profile ) -dxProfilePath <Path of the DX core server profile> Use this attribute to specify the theme EAR file path that is required while executing the deploy theme task: \u2013applicationFile <Absolute or relative path to deployable theme ear file> Use this attribute to specify the theme application name: -applicationName <value> Required options for creating/updating WebDAV theme collection Use this attribute to specify the theme name of the collection created under WebDAV server in DX: -themeName <value> Use this attribute to specify the theme file path that contains all static files to be pushed into DX theme, it accepts either folder or zip file path of the WebDAV theme collection: -themePath <value> Use this attribute to specify the path to the contenthandler servlet on the DX server (e.g. /wps/mycontenthandler): -contenthandlerPath <value> Notes: For new WebDAV theme collection, DXClient tool adds the provided collection (folder/zip) to the WebDAV file store. For existing WebDAV theme collection, the existing theme collection is replaced by the provided theme collection during the update. To get the latest theme collection from the DX server, see Exporting content from the filestore and make modifications on the same folder to get it updated in the DX Server WebDAV file store. For WebDAV theme collection update, a backup of the existing theme collection is taken and placed in store/outputFiles/themes/backup/foldername folder. All the above command options can also be configured inside the config.json configuration file of the DXClient tool, available in the <working-directory>/store directory of the DXClient installation. Note: If you have installed DXClient using the node package file, then you can find the config.json file in the following path: dist/src/configuration/config.json Log files from command execution can be found in the logs directory of the DXClient installation. Example usage: dxclient deploy-theme -dxProtocol <http/https> -hostname <host-name> -dxPort <dxPort> -xmlConfigPath <xmlConfigPath> -dxUsername <dxUsername> -dxPassword <dxPassword> -dxSoapPort <dxSoapPort> -dxConnectHostname <hostname> -dxConnectPort <dxConnectPort> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -xmlFile <xml-file-with-path> -applicationFile <application-file-with-path> -applicationName <application name> -themeName <theme-name> -themePath <theme-path> -dxProfileName <Profile name of the DX core server>","title":"Deploy theme"},{"location":"containerization/Management/themes/#undeploy-theme","text":"The undeploy-theme command can be used to undeploy a theme, including the EAR application and WebDAV files, and it also unregisters the registered theme from the target DX server. Note: If enableBackup is set to true, then undeploy-theme takes a backup of the deployed EAR theme, WebDAV theme collection, and completes DX configuration export (without users) and place it in the backup folder. When the user is downloading EAR, WeDAV, and XML to backup, we must separate it by the folder names store/outputFiles/themes/backup/foldername . The backup of EAR is placed in store/outputFiles/themes/backup/application . Users can restore the theme by using the backup files. Note: Pages might lose the applied theme references in the restored themes. Command description This command invokes the undeploy-theme tool inside the DXClient. This command uses the unregistered theme XML file, theme EAR application name and WebDAV theme collection name, and executes the undeploy theme task. ``` dxclient undeploy-theme ``` Help command This command shows the help document on the undeploy-theme command usage: dxclient undeploy-theme -h Required files Theme Unregistration XML file: This XML file is required to unregister the theme from target DX Server and must contain the details of the theme. The XML file must be provided when executing the undeploy theme task. This command can execute one or more of the following tasks at the same time: Theme unregistration Undeploy theme EAR application Undeploy WebDAV theme collection Common commands Use this attribute to specify the hostname of the target server -hostname <value> Use this attribute to specify the protocol with which to connect to the server -dxProtocol <value> Use this attribute to specify the port on which to connect to the server (for Kubernetes Environment, dxPort is 443 ) -dxPort <value> Use this attribute to specify the username that is required for server authentication -dxUsername <value> Use this attribute to specify the password that is required for server authentication -dxPassword <value> Use this attribute to specify the path to the contenthandler servlet on the DX server (e.g. /wps/mycontenthandler) -contenthandlerPath <value> Use this attribute to take the backup before undeploying theme -enableBackup <value> Note: User can set the enableBackup parameter as true to take backup before undeploying theme. The value is set to false by default. The options passed through command line override the default values. Required options for Theme Unregistration: Use this attribute to specify the theme registration xml file that is used while executing the undeploy theme task. For example, see the Theme-registration.xml file in the directory dxclient/samples/ -xmlFile <xml file name with absolute path of the xmlaccess input file> Use this attribute to specify the path to DX configuration endpoint -xmlConfigPath <value> Required options for undeploying theme EAR application: Use this attribute to specify the configuration wizard home (change of route is only in case of Open Shift Kubernetes Enviornment, otherwise the route remains the same as hostname) that is required for authenticating to the cw_profile -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile (for Kubernetes Environment dxConnectPort is 443) -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile -dxConnectPassword <value> Use this attribute to specify Soap port of the DX server -dxSoapPort <Soap port of the DX server> Specify either the dxProfileName or dxProfilePath of the DX core server: Use this attribute to specify the profile name of the DX core server (for example: wp_profile ) -dxProfileName <Profile name of the DX core server> OR Use this attribute to specify the profile path of the DX server (for example: /opt/HCL/wp_profile ) -dxProfilePath <Path of the DX core server profile> Use this attribute to specify the EAR application name -applicationName <value> Required options for undeploying WebDAV theme collection: Use this attribute to specify the theme name of the collection created under WebDAV -themeName <value> All the above command options can also be configured inside the config.json configuration file of the DXClient tool, available in the <working-directory>/store directory of the DXClient installation. Note: If you have installed DXClient using the node package file, then you can find the config.json file in the following path: dist/src/configuration/config.json Example usage: dxclient undeploy-theme -dxProtocol <http/https> -hostname <host-name> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -dxSoapPort <dxSoapPort> -dxConnectHostname <hostname> -dxConnectPort <dxConnectPort> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -xmlFile <xml-file-with-path> -applicationName <application name> -themeName <theme-name> -enableBackup <enable-backup> -dxProfileName <Profile name of the DX core server profile>","title":"Undeploy theme"},{"location":"containerization/Management/update_dx_core_kubernetes_container_deployment/","text":"Update the Digital Experience 9.5 Core Kubernetes or Red Hat OpenShift container deployment Update the Digital Experience 9.5 Core Kubernetes container deployment. Follow the processes below to create a backup, then update the Digital Experience 9.5 Core Kubernetes or Red Hat OpenShift Container Deployment to a later Container Update release. New HCL Digital Experience 9.5 CFxxx container images are released on a regular cadence, through the HCL DX 9.5 Container Update deliveries . Consult the Digital Experience 9.5 Container Deployment topic for the latest list of DX 9.5 Container images that are available. HCL DX administrators should not apply maintenance to an HCL Digital Experience 9.5 container image. Instead, they should run the update process as described below. Note: HCL DX 9.5 CF191 images are available and may be installed. HCL DX 9.5 CF191 is supported for new deployments only . DX administrators should not upgrade DX 9.5 container deployments to this release. HCL DX administrators should not extend the HCL Digital Experience 9.5 container images. They are not intended to be used in the FROM instruction as a parent image. It is recommended you create a backup of your DX 9.5 deployment before managing the update processes using the following steps: Create a backup of the wp_profile . To backup the wp_profile , it is recommended that the number of instances is 1 instance, and your Digital Experience 9.5 Container deployment is stopped using the following command: /opt/HCL/wp_profile/stopServer.sh Next, ensure the entire /opt/HCL/wp_profile directory is backed up. A method to generate this backup is shown using the following commands: cd /opt/HCL/wp_profile\u2028 tar -cvpzf backup.tar.gz --exclude=/backup.tar.gz --one-file-system /opt/HCL/wp_profile Note: Before starting the tar ensure that your file system has ~50% free capacity. \u2028Once complete, it is recommended that you copy the backup.tar.gz file that is generated to alternate long term storage. In addition, as outlined in the Backup and Recovery topic, the Digital Experience 9.5 database should be backed up at the same time as the wp_profile . Create a backup of the Persistence layer using the following example commands: pg_dump name_of_database > name_of_backup_file. We recommend backing up the system on a remote system: pg_dump -U user_name -h remote_host -p remote_port name_of_database > name_of_backup_file. If it is done locally, you need to execute into the POD. Once you have completed the command, it is recommended that a copy of the resulting file is created and placed to an alternate long-term storage. Update the HCL Digital Experience 9.5 core deployment files: Note: Beginning with HCL DX 9.5 Container Update CF192, the dxctl process is used to manage updates to later update releases. See the following deployment topics below for the update instructions details. Documentation resource: Deploy HCL Digital Experience 9.5 Container to Red Hat OpenShift Documentation resource: Deploy HCL Digital Experience 9.5 Container to Amazon EKS Documentation resource: Deploy HCL Digital Experience 9.5 Container to Microsoft Azure AKS Documentation resource: Deploy HCL Digital Experience 9.5 Container to Google Kubernetes Engine (GKE) Video: Using dxctl to update HCL DX 9.5 on Red Hat OpenShift to Container Update CF192 Video: Updating the HCL DX 9.5 Portal & IBM WebSphere Application Server Administrator Secrets in OpenShift and Kubernetes Download the later version of the HCL DX 9.5 Container Update packages to update from the HCL Software License Portal. Consult the Digital Experience 9.5 Container Deployment topic for the latest list. Load, tag, and push the later version DX 9.5 Container images to your supported Kubernetes or OpenShift platform (similar to steps followed with your original HCL DX 9.5 image detailed in the HCL DX 9.5 Help Center Deployment topics). Use the following guidance and steps if managing an update to Container Update versions prior to CF192: To upgrade the deployment to a new version, update the IMAGETAG value that was used in the original deployDx.sh execution to the new IMAGETAG . Note: If you performed a database transfer, please ensure the <database>.DbUser and <database>.DbPassword for all Portal databases reflect the current user and password in opt/HCL/wp_profile/ConfigEngine/properties/wkplc_dbdomain.properties prior to updating the Portal Core image. Update the tag in the operator.yaml file to the later HCL DX 9.5 Container Update versions tag. Note: If updating your DX 9.5 container deployment to CF19, complete the following steps before proceeding to step 7. Delete the DxDeployment CRD. This terminates all deployments. OpenShift command: oc delete crd dxdeployments.git.cwp.pnp-hcl.com Kubernetes command: kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com Release claim on the wp_profile persistent volume to make it available: OpenShift command: oc edit pv VOLUME Kubernetes command: kubectl edit pv VOLUME Delete the claimRef section: ``` Example: claimRef: kind: PersistentVolumeClaim namespace: dx-ns name: dx-deployment-pvc uid: 633c67f9-89fe-4ac8-8db1-929ccbb8a657 apiVersion: v1 resourceVersion: '658831' ``` Create the CF19 DxDeployment CRD: OpenShift command: oc create -f deploy/crd/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml Kubernetes command: kubectl create -f deploy/crd/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml To update the HCL Digital Experience 9.5 core deployment files to the later Container Update version, run the updateDx.sh script with updated values, as shown in the following examples: Kubernetes command: ./scripts/updateDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE INGRESSIMAGE INGRESSTAG OpenShift command: ./scripts/updateDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE NAMESPACE - the project or the namespace to create or use for deployment. REPLICAS - the number of initial instances for the deployment. REPOSITORY - your local repository, the repository used by OpenShift/ or supported Kubernetes platforms, Amazon EKS or MS Azure AKs IMAGENAME - the name of the dxCore image, as added to the repository above. IMAGETAG - the tag for the target image as added to the above repository. VOLUMENAME - the volume to be used by the deployment for persistence, this must use AccessMode ReadWriteMany . STORAGECLASS - the storage class name used to create the persistent volume. DBTYPE - the database type. By default, and initially, this is Derby. INGRESSIMAGE - The image name to use for ambassador. INGRESSTAG - The image tag to use for ambassador. After the updateDx.sh script has completed, DX administrators are encouraged to check the following log files to ensure the CF update completed successfully. Check the ConfigTrace.log located under the /opt/HCL/wp_profile/ConfigEngine/log/ directory to ensure the update task was successful, as shown in this example: [2020-08-26 19:53:28.658] Target finished: action-apply-cf BUILD SUCCESSFUL Review for any exceptions in the SystemOut.log located under the /opt/HCL/wp_profile/logs/WebSphere_Portal directory. Additional steps required for HCL DX 9.5 deployments to supported Kubernetes platforms: Amazon EKS or Microsoft Azure AKS. Administrators must perform delete/deploy or redeploy the ambassador definitions. This can be done by performing these commands, operating on an MS Azure AKS environment in these examples: kubectl delete crd tracingservices.getambassador.io -n az-demo kubectl delete crd tlscontexts.getambassador.io -n az-demo kubectl delete crd tcpmappings.getambassador.io -n az-demo kubectl delete crd ratelimitservices.getambassador.io -n az-demo kubectl delete crd ratelimits.getambassador.io -n az-demo kubectl delete crd projectversions.getambassador.io -n az-demo kubectl delete crd projects.getambassador.io -n az-demo kubectl delete crd projectsrevisions.getambassador.io -n az-demo kubectl delete crd modules.getambassador.io -n az-demo kubectl delete crd mappings.getambassador.io -n az-demo kubectl delete crd logservices.getambassador.io -n az-demo kubectl delete crd authservices.getambassador.io -n az-demo kubectl delete crd consulresolvers.getambassador.io -n az-demo kubectl delete crd hosts.getambassador.io -n az-demo kubectl delete crd kubernetesserviceresolvers.getambassador.io -n az-demo kubectl delete crd kubernetesendpointresolvers.getambassador.io -n az-demo Upon completion, these are automatically redeployed at version 1 and version 2, provided you have an active deployment. If not, they are redeployed once the HCL DX 9.5 DX is deployed. The previous ambassador version, prior to CF183 at level 0.85.0, is deployed and uses the ambassador version 1 APIs. There are additional options to customize the deployment . For example, once the database is transferred to a non-Derby database, the DBTYPE must updated so you can scale the instances higher. Additionally, once the database is transferred, the number of replicas can be increased. During the Update process, the deployment automatically restarts a few times and make appropriate configuration changes during these restarts. Once complete the deployment is upgraded. For instructions to update the Content Composer, Digital Asset Management, and Experience API container deployment images, see the following topics. Documentation resource: Install the Experience API, Content Composer, and DAM Components Documentation resource: Update the Experience API, Content Composer, and DAM Components Instructions to Delete a DX 9.5 Container Deployment Removing the entire deployment requires several steps, this is by design. To remove the deployment in a specific namespace, run the following: ./scripts/removeDx.sh NAMESPACE NAMESPACE - the project or the namespace created or used for deployment. To remove a namespace, use any of the following commands: OpenShift commands: 'oc delete project **<project\\_name\\>**' 'oc delete -f dxNameSpace_**NAMESPACE**.yaml' where **NAMESPACE** is the namespace to be removed Kubernetes command: 'kubectl delete -f dxNameSpace_**NAMESPACE**.yaml' where **NAMESPACE** is the namespace to be removed The persistent volume associated to the deployment needs to be cleaned up by your Administrator. To reuse a persistent volume, see the following steps: Open the persistent volume in a visual editor (vi) using the: OpenShift command: oc edit pv your_namespace Kubernetes command: kubectl edit pv your_namespace Remove the claimRef section: claimRef: apiVersion: v1 kind: PersistentVolumeClaim name: dx-deploy-pvc namespace: your_namespace resourceVersion: \"488931120\" uid: ebd58361-0e2a-11ea-b02e-02f8fe687954 Ensure you get the 'persistentvolume/your_namespace edited' message. You may need to manually remove any data remaining from the previous deployment.","title":"Update the Digital Experience 9.5 Core Kubernetes or Red Hat OpenShift container deployment"},{"location":"containerization/Management/update_dx_core_kubernetes_container_deployment/#update-the-digital-experience-95-core-kubernetes-or-red-hat-openshift-container-deployment","text":"Update the Digital Experience 9.5 Core Kubernetes container deployment. Follow the processes below to create a backup, then update the Digital Experience 9.5 Core Kubernetes or Red Hat OpenShift Container Deployment to a later Container Update release. New HCL Digital Experience 9.5 CFxxx container images are released on a regular cadence, through the HCL DX 9.5 Container Update deliveries . Consult the Digital Experience 9.5 Container Deployment topic for the latest list of DX 9.5 Container images that are available. HCL DX administrators should not apply maintenance to an HCL Digital Experience 9.5 container image. Instead, they should run the update process as described below. Note: HCL DX 9.5 CF191 images are available and may be installed. HCL DX 9.5 CF191 is supported for new deployments only . DX administrators should not upgrade DX 9.5 container deployments to this release. HCL DX administrators should not extend the HCL Digital Experience 9.5 container images. They are not intended to be used in the FROM instruction as a parent image. It is recommended you create a backup of your DX 9.5 deployment before managing the update processes using the following steps: Create a backup of the wp_profile . To backup the wp_profile , it is recommended that the number of instances is 1 instance, and your Digital Experience 9.5 Container deployment is stopped using the following command: /opt/HCL/wp_profile/stopServer.sh Next, ensure the entire /opt/HCL/wp_profile directory is backed up. A method to generate this backup is shown using the following commands: cd /opt/HCL/wp_profile\u2028 tar -cvpzf backup.tar.gz --exclude=/backup.tar.gz --one-file-system /opt/HCL/wp_profile Note: Before starting the tar ensure that your file system has ~50% free capacity. \u2028Once complete, it is recommended that you copy the backup.tar.gz file that is generated to alternate long term storage. In addition, as outlined in the Backup and Recovery topic, the Digital Experience 9.5 database should be backed up at the same time as the wp_profile . Create a backup of the Persistence layer using the following example commands: pg_dump name_of_database > name_of_backup_file. We recommend backing up the system on a remote system: pg_dump -U user_name -h remote_host -p remote_port name_of_database > name_of_backup_file. If it is done locally, you need to execute into the POD. Once you have completed the command, it is recommended that a copy of the resulting file is created and placed to an alternate long-term storage. Update the HCL Digital Experience 9.5 core deployment files: Note: Beginning with HCL DX 9.5 Container Update CF192, the dxctl process is used to manage updates to later update releases. See the following deployment topics below for the update instructions details. Documentation resource: Deploy HCL Digital Experience 9.5 Container to Red Hat OpenShift Documentation resource: Deploy HCL Digital Experience 9.5 Container to Amazon EKS Documentation resource: Deploy HCL Digital Experience 9.5 Container to Microsoft Azure AKS Documentation resource: Deploy HCL Digital Experience 9.5 Container to Google Kubernetes Engine (GKE) Video: Using dxctl to update HCL DX 9.5 on Red Hat OpenShift to Container Update CF192 Video: Updating the HCL DX 9.5 Portal & IBM WebSphere Application Server Administrator Secrets in OpenShift and Kubernetes Download the later version of the HCL DX 9.5 Container Update packages to update from the HCL Software License Portal. Consult the Digital Experience 9.5 Container Deployment topic for the latest list. Load, tag, and push the later version DX 9.5 Container images to your supported Kubernetes or OpenShift platform (similar to steps followed with your original HCL DX 9.5 image detailed in the HCL DX 9.5 Help Center Deployment topics). Use the following guidance and steps if managing an update to Container Update versions prior to CF192: To upgrade the deployment to a new version, update the IMAGETAG value that was used in the original deployDx.sh execution to the new IMAGETAG . Note: If you performed a database transfer, please ensure the <database>.DbUser and <database>.DbPassword for all Portal databases reflect the current user and password in opt/HCL/wp_profile/ConfigEngine/properties/wkplc_dbdomain.properties prior to updating the Portal Core image. Update the tag in the operator.yaml file to the later HCL DX 9.5 Container Update versions tag. Note: If updating your DX 9.5 container deployment to CF19, complete the following steps before proceeding to step 7. Delete the DxDeployment CRD. This terminates all deployments. OpenShift command: oc delete crd dxdeployments.git.cwp.pnp-hcl.com Kubernetes command: kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com Release claim on the wp_profile persistent volume to make it available: OpenShift command: oc edit pv VOLUME Kubernetes command: kubectl edit pv VOLUME Delete the claimRef section: ``` Example: claimRef: kind: PersistentVolumeClaim namespace: dx-ns name: dx-deployment-pvc uid: 633c67f9-89fe-4ac8-8db1-929ccbb8a657 apiVersion: v1 resourceVersion: '658831' ``` Create the CF19 DxDeployment CRD: OpenShift command: oc create -f deploy/crd/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml Kubernetes command: kubectl create -f deploy/crd/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml To update the HCL Digital Experience 9.5 core deployment files to the later Container Update version, run the updateDx.sh script with updated values, as shown in the following examples: Kubernetes command: ./scripts/updateDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE INGRESSIMAGE INGRESSTAG OpenShift command: ./scripts/updateDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE NAMESPACE - the project or the namespace to create or use for deployment. REPLICAS - the number of initial instances for the deployment. REPOSITORY - your local repository, the repository used by OpenShift/ or supported Kubernetes platforms, Amazon EKS or MS Azure AKs IMAGENAME - the name of the dxCore image, as added to the repository above. IMAGETAG - the tag for the target image as added to the above repository. VOLUMENAME - the volume to be used by the deployment for persistence, this must use AccessMode ReadWriteMany . STORAGECLASS - the storage class name used to create the persistent volume. DBTYPE - the database type. By default, and initially, this is Derby. INGRESSIMAGE - The image name to use for ambassador. INGRESSTAG - The image tag to use for ambassador. After the updateDx.sh script has completed, DX administrators are encouraged to check the following log files to ensure the CF update completed successfully. Check the ConfigTrace.log located under the /opt/HCL/wp_profile/ConfigEngine/log/ directory to ensure the update task was successful, as shown in this example: [2020-08-26 19:53:28.658] Target finished: action-apply-cf BUILD SUCCESSFUL Review for any exceptions in the SystemOut.log located under the /opt/HCL/wp_profile/logs/WebSphere_Portal directory. Additional steps required for HCL DX 9.5 deployments to supported Kubernetes platforms: Amazon EKS or Microsoft Azure AKS. Administrators must perform delete/deploy or redeploy the ambassador definitions. This can be done by performing these commands, operating on an MS Azure AKS environment in these examples: kubectl delete crd tracingservices.getambassador.io -n az-demo kubectl delete crd tlscontexts.getambassador.io -n az-demo kubectl delete crd tcpmappings.getambassador.io -n az-demo kubectl delete crd ratelimitservices.getambassador.io -n az-demo kubectl delete crd ratelimits.getambassador.io -n az-demo kubectl delete crd projectversions.getambassador.io -n az-demo kubectl delete crd projects.getambassador.io -n az-demo kubectl delete crd projectsrevisions.getambassador.io -n az-demo kubectl delete crd modules.getambassador.io -n az-demo kubectl delete crd mappings.getambassador.io -n az-demo kubectl delete crd logservices.getambassador.io -n az-demo kubectl delete crd authservices.getambassador.io -n az-demo kubectl delete crd consulresolvers.getambassador.io -n az-demo kubectl delete crd hosts.getambassador.io -n az-demo kubectl delete crd kubernetesserviceresolvers.getambassador.io -n az-demo kubectl delete crd kubernetesendpointresolvers.getambassador.io -n az-demo Upon completion, these are automatically redeployed at version 1 and version 2, provided you have an active deployment. If not, they are redeployed once the HCL DX 9.5 DX is deployed. The previous ambassador version, prior to CF183 at level 0.85.0, is deployed and uses the ambassador version 1 APIs. There are additional options to customize the deployment . For example, once the database is transferred to a non-Derby database, the DBTYPE must updated so you can scale the instances higher. Additionally, once the database is transferred, the number of replicas can be increased. During the Update process, the deployment automatically restarts a few times and make appropriate configuration changes during these restarts. Once complete the deployment is upgraded. For instructions to update the Content Composer, Digital Asset Management, and Experience API container deployment images, see the following topics. Documentation resource: Install the Experience API, Content Composer, and DAM Components Documentation resource: Update the Experience API, Content Composer, and DAM Components","title":"Update the Digital Experience 9.5 Core Kubernetes or Red Hat OpenShift container deployment"},{"location":"containerization/Management/update_dx_core_kubernetes_container_deployment/#instructions-to-delete-a-dx-95-container-deployment","text":"Removing the entire deployment requires several steps, this is by design. To remove the deployment in a specific namespace, run the following: ./scripts/removeDx.sh NAMESPACE NAMESPACE - the project or the namespace created or used for deployment. To remove a namespace, use any of the following commands: OpenShift commands: 'oc delete project **<project\\_name\\>**' 'oc delete -f dxNameSpace_**NAMESPACE**.yaml' where **NAMESPACE** is the namespace to be removed Kubernetes command: 'kubectl delete -f dxNameSpace_**NAMESPACE**.yaml' where **NAMESPACE** is the namespace to be removed The persistent volume associated to the deployment needs to be cleaned up by your Administrator. To reuse a persistent volume, see the following steps: Open the persistent volume in a visual editor (vi) using the: OpenShift command: oc edit pv your_namespace Kubernetes command: kubectl edit pv your_namespace Remove the claimRef section: claimRef: apiVersion: v1 kind: PersistentVolumeClaim name: dx-deploy-pvc namespace: your_namespace resourceVersion: \"488931120\" uid: ebd58361-0e2a-11ea-b02e-02f8fe687954 Ensure you get the 'persistentvolume/your_namespace edited' message. You may need to manually remove any data remaining from the previous deployment.","title":"Instructions to Delete a DX 9.5 Container Deployment"},{"location":"containerization/Management/virtualportals/","text":"Managing virtual portals This topic describes the commands that are used in managing the virtual portal activities such as creating, listing, importing, or exporting virtual portals. Virtual Portal commands Command description The manage-virtual-portal command is used to manage virtual portal tasks such as create, list, export, and import in the DX server. ``` dxclient manage-virtual-portal ``` Help command This command shows the help document on the manage-virtual-portal command: Help command for creating virtual portals: dxclient manage-virtual-portal create -h Help command for listing virtual portals: dxclient manage-virtual-portal list -h Help command for importing virtual portals: dxclient manage-virtual-portal import -h Help command for exporting virtual portals: dxclient manage-virtual-portal export -h Commands Create virtual portal task in the DX server: manage-virtual-portal create [OPTIONS] List virtual portal task in the DX server: manage-virtual-portal list [OPTIONS] Import virtual portal task in the DX server: manage-virtual-portal import [OPTIONS] Export virtual portal task in the DX server: manage-virtual-portal export [OPTIONS] Required Commands manage-virtual-portal create command: Use this attribute to specify the username that is required for authenticating with the server -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server -dxPassword <value> Use this attribute to specify the configuration wizard home (route change only in case of Open Shift Kubernetes Environment, otherwise same as hostname) that is required for authenticating to the cw_profile -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile(for Kubernetes Environment dxConnectPort is 443) -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile -dxConnectPassword <value> Use this attribute to specify the profile name of the DX core server -dxProfileName <Profile name of the DX core server> Use this attribute to specify the username of the DX WAS server -dxWASUsername <value> Use this attribute to specify the password of the DX WAS server -dxWASPassword <value> Use this attribute to specify the virtual portal Title -vpTitle <value> Use this attribute to specify the virtual portal Realm -vpRealm <value> Use this attribute to specify the virtual portal AdminGroup -vpAdminGroup <value> Use this attribute to specify the virtual portal HostName -vpHostname <value> Use this attribute to specify the virtual portal Context -vpContext <value> Note: Create virtual portal task creates an empty virtual portal in the DX server. Example usage: dxclient manage-virtual-portal create -dxConnectHostname <dxConnectHostname> -dxConnectPort <dxConnectPort> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -dxProfileName <Profile name of the DX Server> -dxUsername <dxUsername> -dxPassword <dxPassword> -dxWASUsername < Username of the DX WAS server> -dxWASPassword <Password of the DX WAS server> -vpTitle <virtual-portal-Title> -vpRealm <virtual-portal-realm> -vpAdminGroup <virtual-portal-adminGroup> -vpHostname <virtual-portal-hostname> -vpContext<virtual-portal-context> manage-virtual-portal list command Use this attribute to specify the username that is required for authenticating with the server -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server -dxPassword <value> Use this attribute to specify the configuration wizard home (route change only in case of Open Shift Kubernetes Environment, otherwise same as hostname) that is required for authenticating to the cw_profile -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile(for Kubernetes Environment dxConnectPort is 443) -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile -dxConnectPassword <value> Use this attribute to specify the profile name of the DX core server -dxProfileName <Profile name of the DX core server> Use this attribute to specify the username of the DX WAS server -dxWASUsername <value> Use this attribute to specify the password of the DX WAS server -dxWASPassword <value> Example usage: dxclient manage-virtual-portal list -dxConnectHostname <dxConnectHostname> -dxConnectPort <dxConnectPort> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -dxProfileName <profile-name-of-the-DX-server> -dxUsername <dxUsername> -dxPassword <dxPassword> -dxWASUsername <username-of-the-DX-WAS-server> -dxWASPassword <password-of-the-DX-WAS-server> manage-virtual-portal import command: Use this attribute to specify the protocol with which to connect to the server -dxProtocol <value> Use this attribute to specify the username that is required for authenticating with the server -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server -dxPassword <value> Use this attribute to specify the hostname of the target server -hostname <value> Use this attribute to specify the port on which to connect to the server(for Kubernetes Environment dxPort is 443) -dxPort <value> Use this attribute to specify the path to DX configuration endpoint (for example: /wps/config) -xmlConfigPath <value> Use this attribute to specify the XML file name with absolute path of the input file -xmlFile <value> Use this attribute to specify the virtual portal Context -vpContext <value> Limitation: Currently, import virtual portal feature supports only vpContext and does not support vpHostname . Support for Virtual portal with hostname might be added in the future release. Example Usage: dxclient manage-virtual-portal import -dxProtocol <http/https> -hostname <host-name> -dxPort <dxPort> -xmlConfigPath <xmlConfigPath> -dxUsername <dxUsername> -dxPassword <dxPassword> -xmlFile <xml-file-with-path> -vpContext <virtual-portal-context> manage-virtual-portal export command: Use this attribute to specify the protocol with which to connect to the server -dxProtocol <value> Use this attribute to specify the username that is required for authenticating with the server -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server -dxPassword <value> Use this attribute to specify the hostname of the target server -hostname <value> Use this attribute to specify the port on which to connect to the server(for Kubernetes Environment dxPort is 443) -dxPort <value> Use this attribute to specify the path to DX configuration endpoint (for example: /wps/config) -xmlConfigPath <value> Use this attribute to specify the virtual portal Context -vpContext <value> Use this attribute to specify the virtual portal Title -vpTitle <value> Use this attribute to specify the XML file name with absolute path of the input file to export the virtual portal content. -xmlFile <value> Limitation: Currently, exporting virtual portal feature supports only vpContext and does not support vpHostname . Support for Virtual portal with hostname might be added in the future release. Example usage: dxclient manage-virtual-portal export -hostname <hostname> -dxProtocol <dxProtocol> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -vpTitle <vpTitle> -vpContext <vpContext> -xmlFile <xml-file-with-path> All the above command options can be configured in the config.json file of the tool, which would be read by default. Location of the configuration file - <working-directory>/store/config.json . The options passed through command line will override these default values. Log files from command execution can be found in the logs directory of the DXClient installation.","title":"Managing virtual portals"},{"location":"containerization/Management/virtualportals/#managing-virtual-portals","text":"This topic describes the commands that are used in managing the virtual portal activities such as creating, listing, importing, or exporting virtual portals.","title":"Managing virtual portals"},{"location":"containerization/Management/virtualportals/#virtual-portal-commands","text":"Command description The manage-virtual-portal command is used to manage virtual portal tasks such as create, list, export, and import in the DX server. ``` dxclient manage-virtual-portal ``` Help command This command shows the help document on the manage-virtual-portal command: Help command for creating virtual portals: dxclient manage-virtual-portal create -h Help command for listing virtual portals: dxclient manage-virtual-portal list -h Help command for importing virtual portals: dxclient manage-virtual-portal import -h Help command for exporting virtual portals: dxclient manage-virtual-portal export -h Commands Create virtual portal task in the DX server: manage-virtual-portal create [OPTIONS] List virtual portal task in the DX server: manage-virtual-portal list [OPTIONS] Import virtual portal task in the DX server: manage-virtual-portal import [OPTIONS] Export virtual portal task in the DX server: manage-virtual-portal export [OPTIONS] Required Commands manage-virtual-portal create command: Use this attribute to specify the username that is required for authenticating with the server -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server -dxPassword <value> Use this attribute to specify the configuration wizard home (route change only in case of Open Shift Kubernetes Environment, otherwise same as hostname) that is required for authenticating to the cw_profile -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile(for Kubernetes Environment dxConnectPort is 443) -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile -dxConnectPassword <value> Use this attribute to specify the profile name of the DX core server -dxProfileName <Profile name of the DX core server> Use this attribute to specify the username of the DX WAS server -dxWASUsername <value> Use this attribute to specify the password of the DX WAS server -dxWASPassword <value> Use this attribute to specify the virtual portal Title -vpTitle <value> Use this attribute to specify the virtual portal Realm -vpRealm <value> Use this attribute to specify the virtual portal AdminGroup -vpAdminGroup <value> Use this attribute to specify the virtual portal HostName -vpHostname <value> Use this attribute to specify the virtual portal Context -vpContext <value> Note: Create virtual portal task creates an empty virtual portal in the DX server. Example usage: dxclient manage-virtual-portal create -dxConnectHostname <dxConnectHostname> -dxConnectPort <dxConnectPort> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -dxProfileName <Profile name of the DX Server> -dxUsername <dxUsername> -dxPassword <dxPassword> -dxWASUsername < Username of the DX WAS server> -dxWASPassword <Password of the DX WAS server> -vpTitle <virtual-portal-Title> -vpRealm <virtual-portal-realm> -vpAdminGroup <virtual-portal-adminGroup> -vpHostname <virtual-portal-hostname> -vpContext<virtual-portal-context> manage-virtual-portal list command Use this attribute to specify the username that is required for authenticating with the server -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server -dxPassword <value> Use this attribute to specify the configuration wizard home (route change only in case of Open Shift Kubernetes Environment, otherwise same as hostname) that is required for authenticating to the cw_profile -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile(for Kubernetes Environment dxConnectPort is 443) -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile -dxConnectPassword <value> Use this attribute to specify the profile name of the DX core server -dxProfileName <Profile name of the DX core server> Use this attribute to specify the username of the DX WAS server -dxWASUsername <value> Use this attribute to specify the password of the DX WAS server -dxWASPassword <value> Example usage: dxclient manage-virtual-portal list -dxConnectHostname <dxConnectHostname> -dxConnectPort <dxConnectPort> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -dxProfileName <profile-name-of-the-DX-server> -dxUsername <dxUsername> -dxPassword <dxPassword> -dxWASUsername <username-of-the-DX-WAS-server> -dxWASPassword <password-of-the-DX-WAS-server> manage-virtual-portal import command: Use this attribute to specify the protocol with which to connect to the server -dxProtocol <value> Use this attribute to specify the username that is required for authenticating with the server -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server -dxPassword <value> Use this attribute to specify the hostname of the target server -hostname <value> Use this attribute to specify the port on which to connect to the server(for Kubernetes Environment dxPort is 443) -dxPort <value> Use this attribute to specify the path to DX configuration endpoint (for example: /wps/config) -xmlConfigPath <value> Use this attribute to specify the XML file name with absolute path of the input file -xmlFile <value> Use this attribute to specify the virtual portal Context -vpContext <value> Limitation: Currently, import virtual portal feature supports only vpContext and does not support vpHostname . Support for Virtual portal with hostname might be added in the future release. Example Usage: dxclient manage-virtual-portal import -dxProtocol <http/https> -hostname <host-name> -dxPort <dxPort> -xmlConfigPath <xmlConfigPath> -dxUsername <dxUsername> -dxPassword <dxPassword> -xmlFile <xml-file-with-path> -vpContext <virtual-portal-context> manage-virtual-portal export command: Use this attribute to specify the protocol with which to connect to the server -dxProtocol <value> Use this attribute to specify the username that is required for authenticating with the server -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server -dxPassword <value> Use this attribute to specify the hostname of the target server -hostname <value> Use this attribute to specify the port on which to connect to the server(for Kubernetes Environment dxPort is 443) -dxPort <value> Use this attribute to specify the path to DX configuration endpoint (for example: /wps/config) -xmlConfigPath <value> Use this attribute to specify the virtual portal Context -vpContext <value> Use this attribute to specify the virtual portal Title -vpTitle <value> Use this attribute to specify the XML file name with absolute path of the input file to export the virtual portal content. -xmlFile <value> Limitation: Currently, exporting virtual portal feature supports only vpContext and does not support vpHostname . Support for Virtual portal with hostname might be added in the future release. Example usage: dxclient manage-virtual-portal export -hostname <hostname> -dxProtocol <dxProtocol> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -vpTitle <vpTitle> -vpContext <vpContext> -xmlFile <xml-file-with-path> All the above command options can be configured in the config.json file of the tool, which would be read by default. Location of the configuration file - <working-directory>/store/config.json . The options passed through command line will override these default values. Log files from command execution can be found in the logs directory of the DXClient installation.","title":"Virtual Portal commands"},{"location":"containerization/Management/xmlaccess/","text":"XML Access This topic provides information about the xmlaccess command that is used to export or import portlet configurations. XML Access The xmlaccess command is used to export or import pages or portlet configurations from a target HCL DX 9.5 CF19 or later server using the input XMLAccess file. Required file XMLAccess file : This XML file must contain the configuration update or export operation for the web application. Command dxclient xmlaccess -xmlFile <path> Help command This command shows the help information for xmlaccess command usage: dxclient xmlaccess -h Command options Use this attribute to specify the protocol with which to connect to the DX server (wp_profile): -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server ( wp_profile ): -dxPort <value> Use this attribute to specify the path to DX configuration endpoint (e.g. /wps/config): -xmlConfigPath <value> Use this attribute to specify the username to authenticate with the DX server ( wp_profile ): -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the local path to the XMLAccess file: -xmlFile <Absolute or relative path to xmlaccess input file> All of the above command options can also be configured in the config.json configuration file of the DXClient tool, available in the <working-directory>/store directory of the DXClient installation. Note: If you have installed DXClient using the node package file, then you can find the config.json file in the following path: dist/src/configuration. Command options passed through the command line overrides values set in the config.json file. Log files from command execution can be found in the logs directory of the DXClient installation. Example usage: dxclient xmlaccess -xmlFile <xml-file-with-path>","title":"XML Access"},{"location":"containerization/Management/xmlaccess/#xml-access","text":"This topic provides information about the xmlaccess command that is used to export or import portlet configurations.","title":"XML Access"},{"location":"containerization/Management/xmlaccess/#xml-access_1","text":"The xmlaccess command is used to export or import pages or portlet configurations from a target HCL DX 9.5 CF19 or later server using the input XMLAccess file. Required file XMLAccess file : This XML file must contain the configuration update or export operation for the web application. Command dxclient xmlaccess -xmlFile <path> Help command This command shows the help information for xmlaccess command usage: dxclient xmlaccess -h Command options Use this attribute to specify the protocol with which to connect to the DX server (wp_profile): -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server ( wp_profile ): -dxPort <value> Use this attribute to specify the path to DX configuration endpoint (e.g. /wps/config): -xmlConfigPath <value> Use this attribute to specify the username to authenticate with the DX server ( wp_profile ): -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the local path to the XMLAccess file: -xmlFile <Absolute or relative path to xmlaccess input file> All of the above command options can also be configured in the config.json configuration file of the DXClient tool, available in the <working-directory>/store directory of the DXClient installation. Note: If you have installed DXClient using the node package file, then you can find the config.json file in the following path: dist/src/configuration. Command options passed through the command line overrides values set in the config.json file. Log files from command execution can be found in the logs directory of the DXClient installation. Example usage: dxclient xmlaccess -xmlFile <xml-file-with-path>","title":"XML Access"},{"location":"containerization/helm/helm_deployment/","text":"Deploying container platforms using Helm Learn to deploy HCL Digital Experience 9.5 CF196 and later release containers to Kubernetes using Helm as verified in Google Kubernetes Engine (GKE) . Beginning with HCL Digital Experience CF197 and later releases, the Helm deployment pattern is supported for new deployments to Red Hat Open Shift , Amazon Elastic Kubernetes Service (EKS) , and Microsoft Azure Kubernetes Service (AKS) . Video: Deploy HCL DX 9.5 Container Update using Helm . About this task This section provides administrators with all Helm-based deployment tasks to deploy HCL Digital Experience CF196 and later releases to supported Kubernetes platforms. This includes preparation, installation, and uninstallation of the deployments using Helm. Note: Beginning with HCL Digital Experience 9.5 Container Update CF199, migration from an Operator (dxctl) based deployment to a Helm deployment of Container Update CF199 or higher is supported. Reference the Help Center topic Migration from Operator (dxctl) to Helm deployment. for more information. Migration from earlier HCL Digital Experience 9.5 Container Update CF196 - CF198 Operator based deployments to Helm deployments is not supported. Follow these steps to prepare for and deploy HCL Digital Experience 9.5 CF196 and later release to Kubernetes using Helm, as verified in Google Kubernetes Engine (GKE) , and with HCL Digital Experience CF197 and later releases, the Helm deployment pattern is supported for new deployments to Red Hat Open Shift , Amazon Elastic Kubernetes Service (EKS) , and Microsoft Azure Kubernetes Service (AKS) . Before you begin: Refer to the latest HCL DX 9.5 Container Update image files list given in the Docker image list topic.","title":"Deploying container platforms using Helm"},{"location":"containerization/helm/helm_deployment/#deploying-container-platforms-using-helm","text":"Learn to deploy HCL Digital Experience 9.5 CF196 and later release containers to Kubernetes using Helm as verified in Google Kubernetes Engine (GKE) . Beginning with HCL Digital Experience CF197 and later releases, the Helm deployment pattern is supported for new deployments to Red Hat Open Shift , Amazon Elastic Kubernetes Service (EKS) , and Microsoft Azure Kubernetes Service (AKS) . Video: Deploy HCL DX 9.5 Container Update using Helm .","title":"Deploying container platforms using Helm"},{"location":"containerization/helm/helm_deployment/#about-this-task","text":"This section provides administrators with all Helm-based deployment tasks to deploy HCL Digital Experience CF196 and later releases to supported Kubernetes platforms. This includes preparation, installation, and uninstallation of the deployments using Helm. Note: Beginning with HCL Digital Experience 9.5 Container Update CF199, migration from an Operator (dxctl) based deployment to a Helm deployment of Container Update CF199 or higher is supported. Reference the Help Center topic Migration from Operator (dxctl) to Helm deployment. for more information. Migration from earlier HCL Digital Experience 9.5 Container Update CF196 - CF198 Operator based deployments to Helm deployments is not supported. Follow these steps to prepare for and deploy HCL Digital Experience 9.5 CF196 and later release to Kubernetes using Helm, as verified in Google Kubernetes Engine (GKE) , and with HCL Digital Experience CF197 and later releases, the Helm deployment pattern is supported for new deployments to Red Hat Open Shift , Amazon Elastic Kubernetes Service (EKS) , and Microsoft Azure Kubernetes Service (AKS) . Before you begin: Refer to the latest HCL DX 9.5 Container Update image files list given in the Docker image list topic.","title":"About this task"},{"location":"containerization/helm/helm_update_deployment/","text":"Update deployment to a later version This section shows how to update your HCL DX 9.5 Container Update CF197 and later deployment to a newer DX 9.5 Container Update release version. To proceed, administrators should have prepared the container platform cluster, together with the HCL DX 9.5 container deployment custom-values.yaml using the following guidance, Planning your container deployment using Helm , and then install your deployment using the instructions in Install and uninstall commands for HCL DX 9.5 CF196 and later container deployments to Kubernetes and Red Hat OpenShift platforms using Helm . Important: As of HCL DX 9.5 Container Update CF197, you can use this process to update a DX 9.5 deployment from Container Update CF196 on the Google Kubernetes Engine (GKE) platform. Support to update DX 9.5 197 container deployments using Helm to CF198 and later DX 9.5 container versions is provided for Red Hat OpenShift, Amazon EKS, Azure AKS, as well as Google GKE platforms beginning with Container Update CF198. Follow the guidance in this section to update the HCL DX 9.5 container release version CF197 and later deployment, to Kubernetes or Red Hat OpenShift that was installed using Helm. These instructions assume that you have made all configuration changes using the recommended Helm upgrade route described in Updating the DX 9.5 Deployment Configuration . This ensures that your custom-values.yaml file is an updated description of the configuration of your environment. If that is not the case, you must update your custom-values.yaml file first with all configuration changes. Populate your repository with the new images Download the new HCL DX 9.5 container update images you need to upgrade and ensure that they are available in the image repository specified in your custom-values.yaml file. See the Docker image list for the latest HCL DX 9.5 container update images available. Download the Helm charts for the version to be installed Download the Helm charts corresponding to the HCL DX 9.5 container versions you want to install. You must always use the Helm charts that correspond to the container versions you are installing or to which you are upgrading. Update the image tags Update the image tags in your custom-values.yaml file to match those for the new images in your repository. See Planning your container deployment using Helm for more information. Run the upgrade command After making the changes to the custom-values.yaml file, use the following command to upgrade your HCL DX 9.5 deployment to CF197 and later release version: ``` Helm upgrade command helm upgrade -n your-namespace -f path/to/your/custom-values.yaml your-release-name path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz ``` In this example: your-namespace is the namespace in which your HCL Digital Experience 9.5 Container Update deployment is installed and your-release-name is the Helm release name you used when installing. The -f path/to/your/custom-values.yaml parameter must point to the custom-values.yaml you updated. path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz is the HCL Digital Experience 9.5 Container Update Helm Chart that you extracted in the preparation steps.","title":"Update deployment to a later version"},{"location":"containerization/helm/helm_update_deployment/#update-deployment-to-a-later-version","text":"This section shows how to update your HCL DX 9.5 Container Update CF197 and later deployment to a newer DX 9.5 Container Update release version. To proceed, administrators should have prepared the container platform cluster, together with the HCL DX 9.5 container deployment custom-values.yaml using the following guidance, Planning your container deployment using Helm , and then install your deployment using the instructions in Install and uninstall commands for HCL DX 9.5 CF196 and later container deployments to Kubernetes and Red Hat OpenShift platforms using Helm . Important: As of HCL DX 9.5 Container Update CF197, you can use this process to update a DX 9.5 deployment from Container Update CF196 on the Google Kubernetes Engine (GKE) platform. Support to update DX 9.5 197 container deployments using Helm to CF198 and later DX 9.5 container versions is provided for Red Hat OpenShift, Amazon EKS, Azure AKS, as well as Google GKE platforms beginning with Container Update CF198. Follow the guidance in this section to update the HCL DX 9.5 container release version CF197 and later deployment, to Kubernetes or Red Hat OpenShift that was installed using Helm. These instructions assume that you have made all configuration changes using the recommended Helm upgrade route described in Updating the DX 9.5 Deployment Configuration . This ensures that your custom-values.yaml file is an updated description of the configuration of your environment. If that is not the case, you must update your custom-values.yaml file first with all configuration changes. Populate your repository with the new images Download the new HCL DX 9.5 container update images you need to upgrade and ensure that they are available in the image repository specified in your custom-values.yaml file. See the Docker image list for the latest HCL DX 9.5 container update images available. Download the Helm charts for the version to be installed Download the Helm charts corresponding to the HCL DX 9.5 container versions you want to install. You must always use the Helm charts that correspond to the container versions you are installing or to which you are upgrading. Update the image tags Update the image tags in your custom-values.yaml file to match those for the new images in your repository. See Planning your container deployment using Helm for more information. Run the upgrade command After making the changes to the custom-values.yaml file, use the following command to upgrade your HCL DX 9.5 deployment to CF197 and later release version: ```","title":"Update deployment to a later version"},{"location":"containerization/helm/helm_update_deployment/#helm-upgrade-command","text":"helm upgrade -n your-namespace -f path/to/your/custom-values.yaml your-release-name path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz ``` In this example: your-namespace is the namespace in which your HCL Digital Experience 9.5 Container Update deployment is installed and your-release-name is the Helm release name you used when installing. The -f path/to/your/custom-values.yaml parameter must point to the custom-values.yaml you updated. path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz is the HCL Digital Experience 9.5 Container Update Helm Chart that you extracted in the preparation steps.","title":"Helm upgrade command"},{"location":"kube/","text":"Community We are here to help Lorem ipsum","title":"Marx and Spenser"},{"location":"kube/#community","text":"","title":"Community"},{"location":"kube/#we-are-here-to-help","text":"Lorem ipsum","title":"We are here to help"},{"location":"kube/k8s-next-conclusion/","text":"Status Date APPROVED 29th of April 2021 Introduction This document compiles all the outcomes of the different design and PoC documents that have been created for our Kubernetes Next Deployment. Each section might contain one or multiple milestones. All sections of the same milestone should be considered as a new \"release\" of the Kubernetes Next strategy. Design Summary Helm as the primary deployment method We want to leverage Helm as our primary way of deployment. Helm will exactly define what resources are being created during the deployment. Resources in our deployment will ideally only be created or deleted with Helm. Update and Rollback tasks would be issued using Helm as well. The Helm chart will contain suitable default settings for a default deployment and also allow customers to customize as much of the deployment as possible for their needs. All the configuration will be determined by values.yaml - as per the Helm standard. DXCTL would still be an option for customers who don't want to use Helm. The deployment procedure that DXCTL will use will be directly and programmatically derived from our Helm chart, thus enabling the Helm chart to be our single point of truth regarding deployment structure. Milestone 1 Summary Providing a simple, fresh deployment via Helm. Limitations - no official support for other ambassadors, OS-routes,... to reduce test effort - dxctl as a deployment option will follow in a later milestone Runtime Controller with focus on operation/controller tasks Based on What is a Kube Operator we are not building an operator but a runtime controller. The wording needs to be precise here since the term operator implies certain functionalities which we are explicitly avoiding with a runtime controller. Instead of the focus on deploying and reconciling resources with a CRD, the runtime controller will focus on operational topics. Part of this is that it should not have to take care of application logic, that ideally should be handled by our applications themselves. The runtime controller should not act as a mitigation strategy for lack of application capabilities, but rather help with application-specific tasks and behaviors. This would include e.g. automatic rollout of configuration changes from ConfigMaps, verification of Kubernetes resource changes, overall health metrics, support during backup and restore procedures. Milestone 1 Summary - Create a new runtime controller with - DAM persistence RW/RO failover - Autorollout config changes via Pod-Recycle Configuration changes via Helm Enable (disable) services like DAM Service configurations (DAM file size limits, CORS,..) Scaling Settings Horizontal Pod Autoscaler SSL certificate management for Ambassador and services DX Admin credential management (Secrets) Limitations - Deployment cannot run old operator and new runtime controller at the same time - SSL certificate management will only cover shipping the current certificate Milestone 3 Summary - auto renewal of interservice SSL certificates ConfigMaps as configuration backbone Since CRDs are cluster-wide resources that require special care (e.g. versioning), we will go with ConfigMaps as our single way of passing configuration values. CRDs also cannot be updated using Helm, which already poses issues for our SoFy deployments. Validation of ConfigMaps could be achieved by using VerificationHooks that call the runtime controller for sign-off on changes. Milestone 1 Summary Changes in ConfigMaps will be rolled out automatically by the runtime controller since our Containers are consuming ConfigMap values only on startup. Limitations - No verification Hooks Containers need to be more self-sustaining Our applications, especially the stateful ones, need to be improved to behave more naturally in a Kubernetes / Containerized Environment. This is especially true for DX Core. Currently, the operator mainly defines how DX Core starts, behaves and updates. Some of this will have to go into the DX Core container and away from the newly planned runtime controller. Milestone 1 Summary DX-Core will become more self-aware. By definition of kube, containers have the naming scheme <pod-name>-<incrementing integer> like dx-core-0 to dx-core-(n-1) where n is the number of Pods. dx-core-0 will be the first core pod created and therefore: - run initialization of the profile - run db transfer for multi-pod environments All other pods will not start until: - init is done - db transfer has happened - This logic can apply in kube and docker-compose environments if we apply the same name schema to the pods/containers. After initialization, pods will be notified (by Helm or the Runtime Controller) of config changes via a checksum environment variable. When any pod starts it will compare this value to the config checksum value stored on the shared profile persistent volume. If the value has changed, then the pod will apply the configuration changes and afterwards update the value in the profile. Because of the order in which kube recycles pods in a stateful set when the spec changes, the pod making the changes should in practice always be dx-core-(n-1) . Limitations - providing a docker-compose file is a stretch goal Vanilla as possible, Custom as needed To keep the support for multiple Kubernetes platforms as easy as possible and the deployment maintainable for us, we want to use as few Kubernetes flavour-specific resources as possible. Furthermore we want to leverage as much as possible of the OOB functionality that Kubernetes provides for our applications regarding deployments, updates, scaling etc. This would also mean the we try to get away from seemingly different deployment behaviors on different flavours, like Openshift vs GKE. Milestone 1 Summary Dropping the support of OS routes for Kubernetes Next deployments. Still relying on shared volumes where provided, NFS-Containers as a workaround where needed. Limitations - tbd Enable logging using Kubernetes logs We will use the sidecar container concept to expose as many logs from DX and its applications as possible, so that customers can directly use Kubernetes standard tools to retrieve them. In the future we could provide a fully fledged logging stack, that uses this logging concept and provides customers with proper logfile management. Milestone 1 Not considered in this milestone. Establish a metrics/monitoring stack We will expose application-specific metrics in our containers that can easily be consumed by software like Prometheus and allows customers to establish proper monitoring. In the future we could also provide our own monitoring stack allowing customers to easily see the current status of their deployment. Milestone 1 Not considered in this milestone. Rolling update We will provide a rolling upgrade capability so customers can easily update their deployment. A Rolling update does not imply a deployment with zero downtime for all features. However zero downtime is possible for rendering. Milestone 1 Not considered in this milestone since there is nothing to update from. Milestone 2 Summary By design kube always updates Statefulsets by picking the pod with the highest number first. In all other scenarios kube picks a random pod. Pods are always upgraded one at a time. Once this pod is ready, the next pod will be upgraded. DAM is self-aware of its own updates. If a database change is needed, DAM will take care of it based of the release-to-release migration. 1. Kube recycles pod with new image - if no DB change is necessary proceed with step 4. 2. The new pod is initializing release-to-release migration. 3. The new pod is performing the release-to-release migration. 4. Once the migration is done it marks itself ready via ready-probe. 5. Kube updates the next pod with the new container image. 6. Kube updates the next pod until there are none on the old image version. Core image will be modified to support rolling updates. This will not change the core build itself but only the wrapping container. 1. Kube recycles pod with new image 2. Pod will create a new, cloned wp_profile directory on the shared volume 3. Pod will switch its own symlink from wp_profile to the cloned, versioned wp_profile 4. Pod will perform the actual upgrade and marks itself ready via ready-probe 5. Kube updates the next pod. 6. Pod checks if the new wp_profile directory exists and adjusts own symlink prior to startup 7. repeat steps 5 and 6 until there are no further pods on the old image The following flowcharts give more detail: Limitations - helm only upgrade - no rollback possible - stick with shared volume - writes for DAM will have a downtime - wp_profile changes from old core pods might be lost during upgrade Undeploy Milestone 1 Summary Use helm uninstall to remove the deployment. PVs will not be removed. Limitations - we do not clean up parts from the current Kubernetes strategy Operator/Runtime Controller technology stack As mentioned earlier within this document we are not building a fully fledged operator. Therefore we do not need to rely on an operator framework to manage CRDs etc. Based on the skillset within the DX development team we decided that the official kuberenetes client for Java is capable of everything we need and are therefore building a Java based runtime controller. Milestone 1 Summary Creating a first version of the runtime controller including proper build, lint and test CI/CD. We also need a proper development setup based on maven and make files.","title":"Conclusion"},{"location":"kube/k8s-next-conclusion/#introduction","text":"This document compiles all the outcomes of the different design and PoC documents that have been created for our Kubernetes Next Deployment. Each section might contain one or multiple milestones. All sections of the same milestone should be considered as a new \"release\" of the Kubernetes Next strategy.","title":"Introduction"},{"location":"kube/k8s-next-conclusion/#design-summary","text":"","title":"Design Summary"},{"location":"kube/k8s-next-conclusion/#helm-as-the-primary-deployment-method","text":"We want to leverage Helm as our primary way of deployment. Helm will exactly define what resources are being created during the deployment. Resources in our deployment will ideally only be created or deleted with Helm. Update and Rollback tasks would be issued using Helm as well. The Helm chart will contain suitable default settings for a default deployment and also allow customers to customize as much of the deployment as possible for their needs. All the configuration will be determined by values.yaml - as per the Helm standard. DXCTL would still be an option for customers who don't want to use Helm. The deployment procedure that DXCTL will use will be directly and programmatically derived from our Helm chart, thus enabling the Helm chart to be our single point of truth regarding deployment structure.","title":"Helm as the primary deployment method"},{"location":"kube/k8s-next-conclusion/#milestone-1","text":"Summary Providing a simple, fresh deployment via Helm. Limitations - no official support for other ambassadors, OS-routes,... to reduce test effort - dxctl as a deployment option will follow in a later milestone","title":"Milestone 1"},{"location":"kube/k8s-next-conclusion/#runtime-controller-with-focus-on-operationcontroller-tasks","text":"Based on What is a Kube Operator we are not building an operator but a runtime controller. The wording needs to be precise here since the term operator implies certain functionalities which we are explicitly avoiding with a runtime controller. Instead of the focus on deploying and reconciling resources with a CRD, the runtime controller will focus on operational topics. Part of this is that it should not have to take care of application logic, that ideally should be handled by our applications themselves. The runtime controller should not act as a mitigation strategy for lack of application capabilities, but rather help with application-specific tasks and behaviors. This would include e.g. automatic rollout of configuration changes from ConfigMaps, verification of Kubernetes resource changes, overall health metrics, support during backup and restore procedures.","title":"Runtime Controller with focus on operation/controller tasks"},{"location":"kube/k8s-next-conclusion/#milestone-1_1","text":"Summary - Create a new runtime controller with - DAM persistence RW/RO failover - Autorollout config changes via Pod-Recycle Configuration changes via Helm Enable (disable) services like DAM Service configurations (DAM file size limits, CORS,..) Scaling Settings Horizontal Pod Autoscaler SSL certificate management for Ambassador and services DX Admin credential management (Secrets) Limitations - Deployment cannot run old operator and new runtime controller at the same time - SSL certificate management will only cover shipping the current certificate","title":"Milestone 1"},{"location":"kube/k8s-next-conclusion/#milestone-3","text":"Summary - auto renewal of interservice SSL certificates","title":"Milestone 3"},{"location":"kube/k8s-next-conclusion/#configmaps-as-configuration-backbone","text":"Since CRDs are cluster-wide resources that require special care (e.g. versioning), we will go with ConfigMaps as our single way of passing configuration values. CRDs also cannot be updated using Helm, which already poses issues for our SoFy deployments. Validation of ConfigMaps could be achieved by using VerificationHooks that call the runtime controller for sign-off on changes.","title":"ConfigMaps as configuration backbone"},{"location":"kube/k8s-next-conclusion/#milestone-1_2","text":"Summary Changes in ConfigMaps will be rolled out automatically by the runtime controller since our Containers are consuming ConfigMap values only on startup. Limitations - No verification Hooks","title":"Milestone 1"},{"location":"kube/k8s-next-conclusion/#containers-need-to-be-more-self-sustaining","text":"Our applications, especially the stateful ones, need to be improved to behave more naturally in a Kubernetes / Containerized Environment. This is especially true for DX Core. Currently, the operator mainly defines how DX Core starts, behaves and updates. Some of this will have to go into the DX Core container and away from the newly planned runtime controller.","title":"Containers need to be more self-sustaining"},{"location":"kube/k8s-next-conclusion/#milestone-1_3","text":"Summary DX-Core will become more self-aware. By definition of kube, containers have the naming scheme <pod-name>-<incrementing integer> like dx-core-0 to dx-core-(n-1) where n is the number of Pods. dx-core-0 will be the first core pod created and therefore: - run initialization of the profile - run db transfer for multi-pod environments All other pods will not start until: - init is done - db transfer has happened - This logic can apply in kube and docker-compose environments if we apply the same name schema to the pods/containers. After initialization, pods will be notified (by Helm or the Runtime Controller) of config changes via a checksum environment variable. When any pod starts it will compare this value to the config checksum value stored on the shared profile persistent volume. If the value has changed, then the pod will apply the configuration changes and afterwards update the value in the profile. Because of the order in which kube recycles pods in a stateful set when the spec changes, the pod making the changes should in practice always be dx-core-(n-1) . Limitations - providing a docker-compose file is a stretch goal","title":"Milestone 1"},{"location":"kube/k8s-next-conclusion/#vanilla-as-possible-custom-as-needed","text":"To keep the support for multiple Kubernetes platforms as easy as possible and the deployment maintainable for us, we want to use as few Kubernetes flavour-specific resources as possible. Furthermore we want to leverage as much as possible of the OOB functionality that Kubernetes provides for our applications regarding deployments, updates, scaling etc. This would also mean the we try to get away from seemingly different deployment behaviors on different flavours, like Openshift vs GKE.","title":"Vanilla as possible, Custom as needed"},{"location":"kube/k8s-next-conclusion/#milestone-1_4","text":"Summary Dropping the support of OS routes for Kubernetes Next deployments. Still relying on shared volumes where provided, NFS-Containers as a workaround where needed. Limitations - tbd","title":"Milestone 1"},{"location":"kube/k8s-next-conclusion/#enable-logging-using-kubernetes-logs","text":"We will use the sidecar container concept to expose as many logs from DX and its applications as possible, so that customers can directly use Kubernetes standard tools to retrieve them. In the future we could provide a fully fledged logging stack, that uses this logging concept and provides customers with proper logfile management.","title":"Enable logging using Kubernetes logs"},{"location":"kube/k8s-next-conclusion/#milestone-1_5","text":"Not considered in this milestone.","title":"Milestone 1"},{"location":"kube/k8s-next-conclusion/#establish-a-metricsmonitoring-stack","text":"We will expose application-specific metrics in our containers that can easily be consumed by software like Prometheus and allows customers to establish proper monitoring. In the future we could also provide our own monitoring stack allowing customers to easily see the current status of their deployment.","title":"Establish a metrics/monitoring stack"},{"location":"kube/k8s-next-conclusion/#milestone-1_6","text":"Not considered in this milestone.","title":"Milestone 1"},{"location":"kube/k8s-next-conclusion/#rolling-update","text":"We will provide a rolling upgrade capability so customers can easily update their deployment. A Rolling update does not imply a deployment with zero downtime for all features. However zero downtime is possible for rendering.","title":"Rolling update"},{"location":"kube/k8s-next-conclusion/#milestone-1_7","text":"Not considered in this milestone since there is nothing to update from.","title":"Milestone 1"},{"location":"kube/k8s-next-conclusion/#milestone-2","text":"Summary By design kube always updates Statefulsets by picking the pod with the highest number first. In all other scenarios kube picks a random pod. Pods are always upgraded one at a time. Once this pod is ready, the next pod will be upgraded. DAM is self-aware of its own updates. If a database change is needed, DAM will take care of it based of the release-to-release migration. 1. Kube recycles pod with new image - if no DB change is necessary proceed with step 4. 2. The new pod is initializing release-to-release migration. 3. The new pod is performing the release-to-release migration. 4. Once the migration is done it marks itself ready via ready-probe. 5. Kube updates the next pod with the new container image. 6. Kube updates the next pod until there are none on the old image version. Core image will be modified to support rolling updates. This will not change the core build itself but only the wrapping container. 1. Kube recycles pod with new image 2. Pod will create a new, cloned wp_profile directory on the shared volume 3. Pod will switch its own symlink from wp_profile to the cloned, versioned wp_profile 4. Pod will perform the actual upgrade and marks itself ready via ready-probe 5. Kube updates the next pod. 6. Pod checks if the new wp_profile directory exists and adjusts own symlink prior to startup 7. repeat steps 5 and 6 until there are no further pods on the old image The following flowcharts give more detail: Limitations - helm only upgrade - no rollback possible - stick with shared volume - writes for DAM will have a downtime - wp_profile changes from old core pods might be lost during upgrade","title":"Milestone 2"},{"location":"kube/k8s-next-conclusion/#undeploy","text":"","title":"Undeploy"},{"location":"kube/k8s-next-conclusion/#milestone-1_8","text":"Summary Use helm uninstall to remove the deployment. PVs will not be removed. Limitations - we do not clean up parts from the current Kubernetes strategy","title":"Milestone 1"},{"location":"kube/k8s-next-conclusion/#operatorruntime-controller-technology-stack","text":"As mentioned earlier within this document we are not building a fully fledged operator. Therefore we do not need to rely on an operator framework to manage CRDs etc. Based on the skillset within the DX development team we decided that the official kuberenetes client for Java is capable of everything we need and are therefore building a Java based runtime controller.","title":"Operator/Runtime Controller technology stack"},{"location":"kube/k8s-next-conclusion/#milestone-1_9","text":"Summary Creating a first version of the runtime controller including proper build, lint and test CI/CD. We also need a proper development setup based on maven and make files.","title":"Milestone 1"},{"location":"kube/k8s-next-deploy/","text":"Status Date APPROVED 22nd of April 2021 All the stuff which is documented here, it is was only created under a deployment perspective. Operations, undeploy and version updates are not part of this architecture concept. Requirements Transparency of resources and the whole deployment Try to meet the Kube standards in terms of different flavours Ongoing support for new versions of Kube API's Single point of configuration truth Transparency of resources and the whole deployment The complexity of our current deployment is to large. Customers have no real chance to determine how many resources will be consumed before they actually deploy it. The new deployment strategy need a clear summary of: min/max number of pods per service requested and limit of CPU&RAM scaling triggers required disk space resulting minimal footprint resulting maximum footprint Also is a deployment via a operator extremely non-transparent. A customer has no changes to get information about: What is going via the deployment What will be deployed What will be changed automatically ... Try to meet the Kube standards in terms of different flavours. The different kuberenetes flavours offer additional features to distinguish themselves in the market. While this is nice for individual application development, DX supports multiple vendors. Therefore it is very important that we are only following the kubernetes standard and not supporting some special functionality of other kubernetes flavour. Example of a special flavour functionality: Kubernetes is using Ingress to provide load balancing, SSL termination and name-based virtual hosting. OpenShift additionally offers Routes which provides a bit more functionality than Ingress. Since OpenShift is kubernetes compatible Ingress is also available. https://www.openshift.com/blog/kubernetes-ingress-vs-openshift-route To minimize our maintenance and support cost, it is crucial that we are not using or relying on special functionalities outside the kube standard. Ongoing support for new versions of Kube API's. Kubernetes and its internal API are rapidly changing. Therefore relying on the kubectl cmd is not an ideal way. We have no option to ensure that all deployments are working correctly with newer versions of kube. For that it would be ideal to have a framework as an abstraction layer which is our single interface to communicate with kube. There are multiple kube client libraries available . Some of theme are officially-supported and other are maintained by an open source community. We should only go with a officially-supported library. Officially-supported language are: Golang Python Java dotnet Javascript/Typescript Haskel To reduce the number of different languages we need to maintain within our development organization we should limit ourselves to one of these: Java Javascript/Typescript Single point of configuration truth From an easy maintaining and supporting perspective it is really important to have a single point of configuration which will be used for both deployments ways Helm-based and DXCTL-based. Future design Met some proposals for a new deployment approach For the new future design we have met the following proposals to align the future design with the requirements. Using Helm charts as our single point of configuration Supporting DXCTL as a alternative deployment way DXCTL should be implemented new with a other language then golang Don't use an operator Automatically roll deployment - provide for each application/deployment a separated ConfigMap and provide a Global ConfigMap for generic configurations Using a master consensus logic to executing changes on a multi pod deployment (like the execution of a ConfigTask on DX) Deployment designs HELM-based deployment OLD DIAGRAM DXCTL-based deployment OLD DIAGRAM Detail decisions informations Single configuration point for HELM and DXCTL deployment HELM Chart structure - Charts.yaml - values.yaml - templates/service.yaml - templates/deployments.yaml - ... The values.yaml file provided a way to collect all required values on a central place. The properties form values.yaml can be used in each template. Exports YAML files With the following CMD it is possible to export all templates. During the export process all used values.yaml properties will be replaced with the right value. helm template <CHART-NAME> A example output of this CMD is a text based output and should be looks like that one: --- # Source: mychart/templates/serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata: name: RELEASE-NAME-mychart labels: helm.sh/chart: mychart-0.1.0 app.kubernetes.io/name: mychart app.kubernetes.io/instance: RELEASE-NAME app.kubernetes.io/version: \"1.16.0\" app.kubernetes.io/managed-by: Helm --- # Source: mychart/templates/service.yaml apiVersion: v1 kind: Service ... This output can be stored as an YAML file and can be apply to a cluster via the kubectl tool. The DXCTL tool needs a new functionality to apply a bunch of YAML files to a kube cluster. New implementation of the DXCTL In our DX organization we have extremely less experience in GoLang. The most common skills are present in Java and Javascript/Typescript. Therefore it is from an maintaining and supporting perspective important to write the DXCTL tool in one of this language. Besides of that is the actual code quality not so good. Missing linting Missing tests Missing use of global constants A non-transparent proprietary mapping concept Bad code structure Contained unused code Code only copied from the operator All in all it makes more sense to write the DXCTL tool new. Don't use an operator for the deployment What are the key features at the moment of our DX CLOUD OPERATOR ? Scaling Triggering of some task (like a DX ConfigEngine task) Decided to use Routes (OpenShift) and the Ambassador for the rest Installation of our apps (RingAPI, CC, DAM) All this task are also possible to do that without an operator. The full status quo of the operator you can find here . What are the main reason why we should don't use an operator for the deployment? The work of an operator is to non-transparent. Customers don't like it if they don't know what's happened during the deployment. Automatically roll deployment - provide for each application/deployment a separated ConfigMap and provide a Global ConfigMap for all together For a automatically roll deployment it would be good to have the configuration attributes are separated by deployments. Therefore it is possible to check the special ConfigMap if the values have changed and the special pods must be deployed again. Helm can helps us here with a checksum. The checksum of a special ConfigMap must be a part of the deployment metadata annotation. https://helm.sh/docs/howto/charts_tips_and_tricks/#automatically-roll-deployments Using a leader consensus logic to executing changes on a multi pod deployment (like the execution of a ConfigTask on DX) The leader consensus logic we need to find which is the leader from the multi pods. Only of the leader it is possible to run a ConfigEngin task. This task will change the wp-profile which is used by all core pods. Kubernetes self used etcd as a consensus system. etcd is using the Raft consensus algorithm. Here is a really nice article and demo of Raft.","title":"Deployment"},{"location":"kube/k8s-next-deploy/#requirements","text":"Transparency of resources and the whole deployment Try to meet the Kube standards in terms of different flavours Ongoing support for new versions of Kube API's Single point of configuration truth","title":"Requirements"},{"location":"kube/k8s-next-deploy/#transparency-of-resources-and-the-whole-deployment","text":"The complexity of our current deployment is to large. Customers have no real chance to determine how many resources will be consumed before they actually deploy it. The new deployment strategy need a clear summary of: min/max number of pods per service requested and limit of CPU&RAM scaling triggers required disk space resulting minimal footprint resulting maximum footprint Also is a deployment via a operator extremely non-transparent. A customer has no changes to get information about: What is going via the deployment What will be deployed What will be changed automatically ...","title":"Transparency of resources and the whole deployment"},{"location":"kube/k8s-next-deploy/#try-to-meet-the-kube-standards-in-terms-of-different-flavours","text":"The different kuberenetes flavours offer additional features to distinguish themselves in the market. While this is nice for individual application development, DX supports multiple vendors. Therefore it is very important that we are only following the kubernetes standard and not supporting some special functionality of other kubernetes flavour. Example of a special flavour functionality: Kubernetes is using Ingress to provide load balancing, SSL termination and name-based virtual hosting. OpenShift additionally offers Routes which provides a bit more functionality than Ingress. Since OpenShift is kubernetes compatible Ingress is also available. https://www.openshift.com/blog/kubernetes-ingress-vs-openshift-route To minimize our maintenance and support cost, it is crucial that we are not using or relying on special functionalities outside the kube standard.","title":"Try to meet the Kube standards in terms of different flavours."},{"location":"kube/k8s-next-deploy/#ongoing-support-for-new-versions-of-kube-apis","text":"Kubernetes and its internal API are rapidly changing. Therefore relying on the kubectl cmd is not an ideal way. We have no option to ensure that all deployments are working correctly with newer versions of kube. For that it would be ideal to have a framework as an abstraction layer which is our single interface to communicate with kube. There are multiple kube client libraries available . Some of theme are officially-supported and other are maintained by an open source community. We should only go with a officially-supported library. Officially-supported language are: Golang Python Java dotnet Javascript/Typescript Haskel To reduce the number of different languages we need to maintain within our development organization we should limit ourselves to one of these: Java Javascript/Typescript","title":"Ongoing support for new versions of Kube API's."},{"location":"kube/k8s-next-deploy/#single-point-of-configuration-truth","text":"From an easy maintaining and supporting perspective it is really important to have a single point of configuration which will be used for both deployments ways Helm-based and DXCTL-based.","title":"Single point of configuration truth"},{"location":"kube/k8s-next-deploy/#future-design","text":"","title":"Future design"},{"location":"kube/k8s-next-deploy/#met-some-proposals-for-a-new-deployment-approach","text":"For the new future design we have met the following proposals to align the future design with the requirements. Using Helm charts as our single point of configuration Supporting DXCTL as a alternative deployment way DXCTL should be implemented new with a other language then golang Don't use an operator Automatically roll deployment - provide for each application/deployment a separated ConfigMap and provide a Global ConfigMap for generic configurations Using a master consensus logic to executing changes on a multi pod deployment (like the execution of a ConfigTask on DX)","title":"Met some proposals for a new deployment approach"},{"location":"kube/k8s-next-deploy/#deployment-designs","text":"","title":"Deployment designs"},{"location":"kube/k8s-next-deploy/#helm-based-deployment","text":"OLD DIAGRAM","title":"HELM-based deployment"},{"location":"kube/k8s-next-deploy/#dxctl-based-deployment","text":"OLD DIAGRAM","title":"DXCTL-based deployment"},{"location":"kube/k8s-next-deploy/#detail-decisions-informations","text":"","title":"Detail decisions informations"},{"location":"kube/k8s-next-deploy/#single-configuration-point-for-helm-and-dxctl-deployment","text":"","title":"Single configuration point for HELM and DXCTL deployment"},{"location":"kube/k8s-next-deploy/#helm-chart-structure","text":"- Charts.yaml - values.yaml - templates/service.yaml - templates/deployments.yaml - ... The values.yaml file provided a way to collect all required values on a central place. The properties form values.yaml can be used in each template.","title":"HELM Chart structure"},{"location":"kube/k8s-next-deploy/#exports-yaml-files","text":"With the following CMD it is possible to export all templates. During the export process all used values.yaml properties will be replaced with the right value. helm template <CHART-NAME> A example output of this CMD is a text based output and should be looks like that one: --- # Source: mychart/templates/serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata: name: RELEASE-NAME-mychart labels: helm.sh/chart: mychart-0.1.0 app.kubernetes.io/name: mychart app.kubernetes.io/instance: RELEASE-NAME app.kubernetes.io/version: \"1.16.0\" app.kubernetes.io/managed-by: Helm --- # Source: mychart/templates/service.yaml apiVersion: v1 kind: Service ... This output can be stored as an YAML file and can be apply to a cluster via the kubectl tool. The DXCTL tool needs a new functionality to apply a bunch of YAML files to a kube cluster.","title":"Exports YAML files"},{"location":"kube/k8s-next-deploy/#new-implementation-of-the-dxctl","text":"In our DX organization we have extremely less experience in GoLang. The most common skills are present in Java and Javascript/Typescript. Therefore it is from an maintaining and supporting perspective important to write the DXCTL tool in one of this language. Besides of that is the actual code quality not so good. Missing linting Missing tests Missing use of global constants A non-transparent proprietary mapping concept Bad code structure Contained unused code Code only copied from the operator All in all it makes more sense to write the DXCTL tool new.","title":"New implementation of the DXCTL"},{"location":"kube/k8s-next-deploy/#dont-use-an-operator-for-the-deployment","text":"What are the key features at the moment of our DX CLOUD OPERATOR ? Scaling Triggering of some task (like a DX ConfigEngine task) Decided to use Routes (OpenShift) and the Ambassador for the rest Installation of our apps (RingAPI, CC, DAM) All this task are also possible to do that without an operator. The full status quo of the operator you can find here . What are the main reason why we should don't use an operator for the deployment? The work of an operator is to non-transparent. Customers don't like it if they don't know what's happened during the deployment.","title":"Don't use an operator for the deployment"},{"location":"kube/k8s-next-deploy/#automatically-roll-deployment-provide-for-each-applicationdeployment-a-separated-configmap-and-provide-a-global-configmap-for-all-together","text":"For a automatically roll deployment it would be good to have the configuration attributes are separated by deployments. Therefore it is possible to check the special ConfigMap if the values have changed and the special pods must be deployed again. Helm can helps us here with a checksum. The checksum of a special ConfigMap must be a part of the deployment metadata annotation. https://helm.sh/docs/howto/charts_tips_and_tricks/#automatically-roll-deployments","title":"Automatically roll deployment - provide for each application/deployment a separated ConfigMap and provide a Global ConfigMap for all together"},{"location":"kube/k8s-next-deploy/#using-a-leader-consensus-logic-to-executing-changes-on-a-multi-pod-deployment-like-the-execution-of-a-configtask-on-dx","text":"The leader consensus logic we need to find which is the leader from the multi pods. Only of the leader it is possible to run a ConfigEngin task. This task will change the wp-profile which is used by all core pods. Kubernetes self used etcd as a consensus system. etcd is using the Raft consensus algorithm. Here is a really nice article and demo of Raft.","title":"Using a leader consensus logic to executing changes on a multi pod deployment (like the execution of a ConfigTask on DX)"},{"location":"kube/k8s-next-helm-incubator-values/","text":"Introduction It turns out to be a very common case, that we can not get certain features done within just one release. Hence implementation spans multiple releases and some of the new implementation will be available in the release that we are shipping before the new feature was finalized. We need to ensure a way to enable development to use these new features in development deployments and for testing and at the same time minimize efforts during endgame to make these unfinished features \"invisible\" for a release and we need to prevent documentation efforts for features that might be somewhat visible to customers, but which should actually not be used yet. Example case \"central logging configuration\" We have been working on our central logging configuration for quite some time. At the time of writing, we are right before the endgame sprint of CF199 and we are in the situation that we now have to remove logging configuration options just in the release branch, so that customers don't see the future capabilities and we also don't have to document anything we would not even want to be documented yet. Solution proposal To circumvent these kinds of situations, the proposal is to establish a so called \"incubator\" section inside our helm values.yaml file. We'll add appropriate documentation so that it is obvious to our customers that configuration options within this section are solely for internal use yet. As soon as we then finalize a feature, its configuration will move out of the incubator section, which will sort of represent the release of the feature, which would also then include appropriate end user documentation, too. A nice side effect of the approach is that, we finally would have a good place to hold feature flag configuration e.g. for some beta features that we would only want to allow to use for certain customers. This example values.yaml extract provides a proposal on how the configuration extension would look like: # Image related configuration images: ... # Resource allocation settings, definition per pod # Use number + unit, e.g. 1500m for CPU or 1500M for Memory resources: ... # This is the incubator that contains internal and/or not yet published configuration pieces. # Please only touch any of the incubator configurations in case our support teams ask you to. # Feel free to have a look at new configuration items that might be released in future and make sure to ask any question you might have! incubator: # Logging configuration logging: # Notice: log level settings are currently under development # Core specific logging configuration core: ...","title":"Incubator configurations"},{"location":"kube/k8s-next-helm-incubator-values/#introduction","text":"It turns out to be a very common case, that we can not get certain features done within just one release. Hence implementation spans multiple releases and some of the new implementation will be available in the release that we are shipping before the new feature was finalized. We need to ensure a way to enable development to use these new features in development deployments and for testing and at the same time minimize efforts during endgame to make these unfinished features \"invisible\" for a release and we need to prevent documentation efforts for features that might be somewhat visible to customers, but which should actually not be used yet.","title":"Introduction"},{"location":"kube/k8s-next-helm-incubator-values/#example-case-central-logging-configuration","text":"We have been working on our central logging configuration for quite some time. At the time of writing, we are right before the endgame sprint of CF199 and we are in the situation that we now have to remove logging configuration options just in the release branch, so that customers don't see the future capabilities and we also don't have to document anything we would not even want to be documented yet.","title":"Example case \"central logging configuration\""},{"location":"kube/k8s-next-helm-incubator-values/#solution-proposal","text":"To circumvent these kinds of situations, the proposal is to establish a so called \"incubator\" section inside our helm values.yaml file. We'll add appropriate documentation so that it is obvious to our customers that configuration options within this section are solely for internal use yet. As soon as we then finalize a feature, its configuration will move out of the incubator section, which will sort of represent the release of the feature, which would also then include appropriate end user documentation, too. A nice side effect of the approach is that, we finally would have a good place to hold feature flag configuration e.g. for some beta features that we would only want to allow to use for certain customers. This example values.yaml extract provides a proposal on how the configuration extension would look like: # Image related configuration images: ... # Resource allocation settings, definition per pod # Use number + unit, e.g. 1500m for CPU or 1500M for Memory resources: ... # This is the incubator that contains internal and/or not yet published configuration pieces. # Please only touch any of the incubator configurations in case our support teams ask you to. # Feel free to have a look at new configuration items that might be released in future and make sure to ask any question you might have! incubator: # Logging configuration logging: # Notice: log level settings are currently under development # Core specific logging configuration core: ...","title":"Solution proposal"},{"location":"kube/k8s-next-operation/","text":"Introduction We want to provide a easy to maintain, reliable and transparent Kubernetes deployment. From a operation perspective this means that customers should have the ability to know exactly in what state their deployment is, what resources are maintained, how to create backups and do restores etc. This document provides a proposal on how this deployment could look like. Deployment structure Overview Application Overview Operator influence Deployed applications DX Core Kubernetes Type: StatefulSet with two persistent volumes (1x RWM, 1x RWO) The DX Core application is currently our application with the highest complexity in the Kubernetes deployment. This is mainly driven by the fact that DX Core, unlike our new applications, was not designed for a containerized deployment. The original deployment of DX Core in a clustered fashion usually relied on having multiple Machines / Nodes that have WAS ND deployed. WAS ND would then consist of deployment manager (DMGR) and multiple nodes - This is called a Cell. The primary configuration of all applications would be maintained on the DMGR and then synced throughout the Cell to all nodes. The applications, in our case DX, would be running on multiple nodes and be therefore highly available. In a containerized deployment, the goal is usually to get away from such stateful units. Especially the DMGR would be difficult. To get around that issue and still have a highly available deployment (and scalability) we leverage the \"Farming\" approach for deployment. In that case, each node - or in terms of Kubernetes: Pod - will access a shared persistent storage containing the DX profile, there is no DMGR that defines the profile and pushes it to the node. All nodes see the same profile at all time. The farming deployment brings certain issues with it, that should be mentioned: ConfigEngine Tasks can and must only be run on one DX instance at a time. Other instances in the farm are not allowed to run those tasks. All Nodes / Pods require a shared persistent file storage, which will then usually be something like NFS. To ensure that only one DX Pod is performing the initial setup in Kubernetes and runs ConfigEngine tasks, the current implementation leverages an operator which ensures that before tasks are run / init is done, only one DX Pod is running. That has the following implications: Startup behavior of DX Core is completely defined by the operator, the container image itself remains relatively \"dumb\" During configurations DX Core is not highly available and no load balancing between multiple Pods is possibly, since only one Pod is running There is a distinct relationship between DX Core and the operator. Versions of both must be matched or a successful operation is not guaranteed/supported There are multiple ways to go: Keep an operator that ensures that only DX Core Pod is running to prevent multiple execution of ConfigEngine Tasks / Writing conflicts Enhance the DX Core image + Kubernetes deployment to allow DX Core to have a consensus based configuration The preferred way would be having a consensus based deployment of multiple Pods containing DX Core. This means, that there would also be one elected leader Pod, which executed ConfigEngine tasks, while there are one or multiple follower Pods, which just read the profile. Also it could be thought about, having a persistence sync from leader to followers which is not using a RWM Volume. This would ease deployments, since not all cloud providers allow for RWM volumes. Using a consesus based deployment could dramatically reduce the dependency to an operator and could possibly allow for maintaining HA during configuration changes. RingAPI Kubernetes Type: Deployment/ReplicaSet - stateless The RingAPI is a simple part of our deployment. It basically performs API wrapping actions for DX Core. It does not have any persistence, neither via a DB connection nor via a persistent volume. Thus scaling, initialization and running requires no application specific specialties. Content Composer Kubernetes Type: Deployment/ReplicaSet - stateless The Content Composer is a web application that consumes APIs provided by the RingAPI. It mainly consists of a web server that hosts static JS files used for the web application. It does not have any persistence, neither via a DB connection nor via a persistent volume. Thus scaling, initialization and running requires no application specific specialties. To use content composer integrated in DX, it is necessary to run a ConfigEngine task. DAM Kubernetes Type: StatefulSet with one persistent volume (1x RWM) The Digital Asset Management is a web application that consists of a UI and an API server. The API server uses a persistent volume and a database to provide functionality. DAM is designed to have no writing conflicts between multiple Pods. The application itself will ensure that all configuration tasks are only performed by one Pod. There is not special startup or initialization that needs to be performed by an operator or similar. To use DAM integrated inside DX, it is necessary to run a ConfigEngine task. DAM Persistence Kubernetes Type: Two StatefulSet with two persistent volumes (1x RWO, 1x RWO) The DAM Persistence consists of a RW leader and a RO follower, both having their own persistent volume. Replication is maintained by application logic (PostgreSQL). Since our current implementation of PostgreSQL has no real HA or loadbalancing, we currently use an operator to automatically switch over to RO if the RW Pod dies. This dependency on an operator must be removed for multiple reasons: Switchover from RW to RO is not zero-downtime No load balancing between multiple PostgreSQL Pods No scaling for load surges, we are basically limited to vertical scaling (e.g. allow for much CPU and Memory on a single DB instance) Operator switchover adds complexity to the deployment Operator switchover forms a possibly single point of failure The solution to that is to have a properly scalable Persistence Layer, e.g. HA PostgreSQL. With such a setup, the manually build switchover can be removed and the necessity for a DAM operator vanishes. Image Processor Kubernetes Type: Deployment/ReplicaSet - stateless The image processor is a very simple API based computing application that performs image manipulation. The Pod is completely stateless, therefore scaling, initialization and running requires no application specific specialties. Conclusion Our current implementation of operations using operators is mainly based on certain issues with our applications themselves. Instead of building solutions to handle the symptoms, we should focus on fixing them to make such constructs obsolete and reduce complexity. The goal in a containerized deployment should be to make applications ready for it and not force applications not made for it into a specific deployment structure. Side note: Having operators carrying much logic also has implications for customers that may not want to use Kubernetes, but rather just docker because they are just trying out things or want to quickly spin up development environments. Since operators rely on Kubernetes for their actions, all functionality they provide will not be directly available outside Kubernetes. Having our container images taking more responsibility for themselves allows customers and developers to get a faster start with DX. Monitoring / Metrics We need to enable monitoring capabilities for our customers. This topic can be split into two parts: Health/Monitoring data exposure Data Aggregation and Visualization It is worth having a look at the current implementation of the SoFy team, which at least provides monitoring out of the box for Kubernetes seemingly using Prometheus and Grafana. Health/Monitoring data exposure All our application Pods should expose a metrics endpoint that can be scraped by a metrics data aggregation tooling. The endpoint should ideally be available at the same port for all Pods, thus making the configuration easier and more straight forward. The format should orient on what is best practice, e.g. the Prometheus format. Kubernetes Resource Data Kubernetes itself provides metrics using the Kube state metrics . This contains metrics regarding Kubernetes resources like nodes and Pods. The data provided by the metrics are in a Prometheus consumable format. Java Application Data For Java we can leverage the JMX Exporter . It provides metrics about the state of a JVM in a Prometheus consumable format. NodeJS Application Data For NodeJS we can use the Prom-Client . It is a npm package that provides various metrics regarding the NodeJS eventloop, heap usage etc. It also allows to configure custom metrics that can be exposed. The data format is Prometheus consumable. DB / PostgreSQL Data For our current persistence stack we can use PostgreSQL Exporter . It exposes various DB related metrics and stats directly coming from the database itself. The output format is Prometheus consumable. Data Aggregation All metrics data exposure is worthless if we don't gather that data somewhere central. For that purpose applications like Prometheus should be used. Prometheus is a scraping + persistence engine that will automatically scrape configured endpoints for exported metrics in a defined interval. It allows for specific querying to view data. There are already existing prometheus helm charts that could be leveraged as a dependency or inspiration in our helm charts. For visualization Grafana may present a good choice. It is widely adopted and provides out of the box compatibility with Prometheus. It also comes with a default set of helm charts . Logging It is important that our customers are able to easily access logs during runtime. This is necessary for them to configure e.g. notifications based on certain log messages, as well as for debugging issues. Using Sidecar Containers for log exposure The Kubernetes stack already provides a logging mechanism which exposes logs of containers / Pods. The logs maintained here are based of the stdout and stderr streams of the containers and their respective running application. This principle has a disadvantage: Whenever your application uses something different for logging, e.g. typically files, you may only be able to tail the output of one file during runtime, thus only expose one log file in the Pod. Kubernetes provides the concept of sidecar containers for exactly that case. Each sidecar container can expose a file as a log stream inside Kubernetes, allowing Kubernetes to log multiple log files of a single application. Since sidecar containers share the same volumes as the run containers, they can access and tail the log files, allowing for an easy implementation. Log output gathering Since Kubernetes already gathers all log output of Pods and their containers, the usual approach is to gather the logs via Kubernetes itself. There are two often used ways to do logging in Kubernetes: EKF / ELK (Elasticsearch for data aggregation, Kibana for visualization and Fluentd for log data collection) Loki + Grafana (Loki for aggregation, Grafana for visualization) The basic concept behind both is to take the logs of each cluster Node (which include the logs of the Pods running on that cluster), aggregate them and them add a visual interface to access them for monitoring, search and debugging. Changing configurations Adjusting resource allocations Changing resource allocations like CPU/Memory requests or limits would be applied directly on the definition of Deployments/ReplicaSets. When the values have changed, Kubernetes will internally trigger its own reconciliation and roll out the update to the resources. Adjusting enabled/disabling applications If a customer wants to enable/disable particular parts of the deployment, this should be done via the Helm upgrade process. Since we deploy the necessary Resources incl. Routes, StatefulSets etc. directly during deployment and not via an operator, enabling the application in the values.yaml of helm would then create or destroy the resources based on the enablement condition. It would also adjust the configuration of the DX Core Pods, thus leading to the leader of those Pods to configure the application, e.g. DAM. Change application configuration Since we would like to store our configuration in configMaps, we'd like to know when they have changed and rollout the updated configuration. Therefore we need a mechanism to trigger the rollout of that update via StatefulSets/Deployments. An easy way to do so, is to add a checksum annotation to the Pods of our StatefulSets/Deployments for each configMap they care about. As soon as a configMap is changed, the operator will update the checksum of the affected resources and Kubernetes will start to roll out updated Pods or restart them, performing a rolling update. Change validation Our configuration lives within configMaps and Kubernetes resources. Those are created by Helm or with DXCTL using Helm created yaml files. There are multiple ways to validate changes to the resources we are managing: Using Helm Helm supports the usage of schema files for its values.yaml . Since all values we use in our deployment are derived from that file during install or upgrade, we can leverage that check. Using Kubernetes validation webhooks Kubernetes provides the principle of validation webhooks . In combination with an operator, that provides validation endpoints, we would be able to reject wrong/undesired changes to configMaps directly in Kubernetes during runtime. We can also provide fitting error messages that will the customers why the changes they planned to do will not work. This would not only be applicable to configMaps, but also to other Kubernetes Resources e.g. our Deployments or StatefulSets. Application lifecycle Initial startup During the initial startup we need to ensure that DX Core will only run the initialization once and only in one running Pod. Since the wp_profile is shared between all Pods, the initialized profile will be available to all Pods after initialization. The initialization flow could look the following way: The desired set of DX Core Pods is created by the StatefulSet The Pods negotiate a leader Pod, all others will act as followers. The leading Pod will start the initialization process, all other Pods will not start DX Core until the initialization has been completed After the initialization has been performed by the leader, all Pods (incl. followers) will start their DX Core process The application is initialized and ready to serve This requires communication between the DX Core Pods, e.g. on the shared volume. The current state of initialization could be persistent in a lock/timestamp file inside wp_profile. Other applications like DAM for example do already have initialization logic that prevents multiple Pods from colliding. Scaling We can use HorizontalPodAutoscalers that are included in Kubernetes to automatically scale services up and down based on CPU and memory usage. In case we want to leverage custom metrics, e.g. DAM operations pressure, there are also ways to establish custom metrics in Kubernetes and use them in autoScalers. Backup & Restore Backup and Restore tasks could be run as Jobs, which are one-off Pods in Kubernetes that fulfill a specific task and return a completion status. Those jobs could be created by the operator, since they need to be created during the deployments lifecycle. Creation of those backup tasks could either be triggered manually, or run automatically in defined intervals. Backups could include copying specific files to backup volumes, dumping deployment information and state or even trigger functions like EXIM in DAM and put the output to a desired spot. Restoring of before used backups should also be managed by the operator, as it would be able to bring the applications into their desired state before re-importing the data into them. It could also do validate if a restore was successful, e.g. by checking specific health-checks of individual applications. Used Kubernetes structures Operators/Controllers There are certain actions inside our deployment that would require manual intervention to get to the desired goal. The basic principle of watching a type of resource and acting accordingly to changes is considered a controller in the Kubernetes ecosystem. An operator on the other hand can consist of multiple controllers for different resources and implements applications specific knowledge/actions. A basic example would be a reaction on configuration changes. When we change the configMap of one our applications, we want to make that application aware of the introduced changes. This is true for stateful and stateless applications. A simple, but yet effective way to force the rollout of those changes is to have a annotation at the Pods level, which just contains a checksum of the applications config map. If that checksum changes, the application will automatically cycled through, performing a rollout of the new configuration to all Pods. Helm could provide the checksum during execution. If the value changes happen in Helm and the customer uses Helm upgrade to apply those changes, we could easily update the annotations during that automatically and the changes will be applied. Since that builds up a complexity in the Helm charts and e.g. would not work if customers use the plain yaml output with DXCTL, unless we port that functionality to DXCTL. If the customer applies changes directly in Kubernetes on configMap level, we would not get any automated rollout. To cover all different aspects of changes to configuration, having an operator that watches configurations and modifies our Pod definitions accordingly, would ease the process for customers a lot and reduce implementation overhead in both Helm and DXCTL. Other cases where the operator could act with application specific logic: Operator/Controller use cases Leading Pod only Routes There may be the necessity to have routes that should only go to the leading DX Core Pod. Certain actions, like config changes should only happen in one of our DX Core Pods. The operator could use a leader lookup routine to configure certain routes to point to exactly that Pod, instead of pointing to a Service consisting of multiple Pods. This would require the operator to check who is the leading DX Core Pod in a regular fashion and update the Endpoint for the corresponding route. This could be a repeatable pattern, where we could have multiple pre defined Endpoints that share a common label, allowing the operator to apply the change easily to multiple routes at the same time. Automated ConfigMap application As mentioned in the introduction before, configuration changes on ConfigMap level do not induce a automatic rollout on dependent resources in Kubernetes. This logic needs to be established by us. The operator could watch all relevant ConfigMaps and adjust the checksum annotation of all dependent resources, e.g. StatefulSets and Deployments. The built in Kubernetes reconciliation would then automatically roll out those changes. DAM persistence RO fallback As long as we do not have a true loadbalanced HA persistence solution for DAM, we will rely on a ReadWrite Primary and ReadOnly Secondary PostgreSQL instance, which do automatically replicate. In order to have a fallback in the case of an outage of the primary DB, the operator would change the routing to the DB for DAM automatically to the secondary. This would be restored to use the primary as soon as the primary DB is up and ready again. DX Core configuration Proxy If we are running multiple instances of DX Core, we want customers to execute configuration changes only on the leading Pod. The operator could perform a leader lookup and perform as a proxy for such executions. Instead of directly executing kubectl exec commands in one of the DX Core Pods, they would be pointed to the operator Pod and could either be executed through a direct SSH tunnel to the Core Pod or initiating the Kubernetes exec command on the DX Core Pod. Customers would not need to know which of the Cores is the current leading one. Leading Pod identification For configuration and debugging purpose it may be useful to know, which DX Core Pod is currently the leading one. A simple command that can be triggered in the operator Pod that return the Pod Name could already be sufficient. For other use cases the operator would already need to know which Pod is leading. Configuration validation The operator could provide a validation endpoint that can be used to validate the changes to Resources during runtime. This could include type checks and even specific value verification. DX Deployment Specific Metrics We can use the operator to expose metrics endpoints that aggregate DX deployment relevant health and status information. Since the operator can possibly know all members of the deployment in its namespace, it can act as a point of entry for quick health checks. Custom Resources Definition vs. configMaps A classical operator design usually incorporates the use of a Custom Resource Definition or short CRD. A CRD is an extension to the Kubernetes API and basically allows for creation of custom resource definitions that reflect whatever is necessary. In our case, we used a custom resource (CR) called dxDeployment in the past. What content can be inside that CR is defined by the CRD. You could think of the CRD as a schema or object definition, from which you can create Objects inside Kubernetes. This definition can also contain validation of values. The operator would use this CR as its configuration backbone. ConfigMaps on the other hand are vanilla Kubernetes resources that contain a list of key-value-pairs. The values of those pairs could either be a single value or even be a whole file structure. Still in the end the principle is always key-value-pair. This is a simple data structure that allows for easy editing, but not so much flexibility in regards of structure. ConfigMaps, unlike CRs, can be directly consumed in some Kubernetes resources. Pods can either mount configMaps as a volume inside the Containers or extract values from them and set them as environment variables inside Containers. There is no necessary conversion logic that needs to be implemented. If we perform changes to our configuration, e.g. add a field, we would require the CRD to be updated prior to updating the CR. Also our operator would need the be update, even if it would only pass the configuration field through to another Kubernetes resource. Example: We have a CRD that defines certain parts of Pod specification. Unfortunately, we did not incorporate labels in the past. By our implementation logic, we take add the field to the CRD, then enhance the operator to be able to consume that field and pass it on through the Pod definition e.g. inside a StatefulSet. This apparently small change to just to enable a field for a Pod specification requires changes on CRD definition, changes to operator code, rolling out a new CRD and operator. Alternatively the label could just have directly been added to the StatefulSet in the first place, without being passed through an operator. Also, there has been a lot of discussion the the Helm community regarding the lifecycle of CRDs. Therefore Helm does currently not support updating CRDs , which will make the use of CRDs with SoFy harder for us and our customers, since SoFy relies on Helm. Because CRDs define a content/config structure and are cluster wide resources, special attention has to go into version updates of CRDs, updating them and testing use/edge cases like having two versions of a CRD in the cluster because two different versions of DX are deployed. (E.g. CF191 and CF193 on SoFy sandbox clusters.) Since an operator could also watch ConfigMaps as a source of configuration, we would like to use those. Glossary DMGR: Deployment Manager from WAS ND, maintains deployment cells containing multiple WAS nodes. WAS ND: Websphere Application Server Network Deployment, Java application server used by DX Core. Cell: In WAS Terms consists of a DMGR and multiple nodes. RWO: ReadWriteOnce Volume - can only be accessed by one Pod, one to one relationship between volume and Pod. RWM: ReadWriteMany Volume - can be accessed by multiple Pods, one to many relationship between volume and Pods.","title":"Operations"},{"location":"kube/k8s-next-operation/#introduction","text":"We want to provide a easy to maintain, reliable and transparent Kubernetes deployment. From a operation perspective this means that customers should have the ability to know exactly in what state their deployment is, what resources are maintained, how to create backups and do restores etc. This document provides a proposal on how this deployment could look like.","title":"Introduction"},{"location":"kube/k8s-next-operation/#deployment-structure","text":"","title":"Deployment structure"},{"location":"kube/k8s-next-operation/#overview","text":"","title":"Overview"},{"location":"kube/k8s-next-operation/#application-overview","text":"","title":"Application Overview"},{"location":"kube/k8s-next-operation/#operator-influence","text":"","title":"Operator influence"},{"location":"kube/k8s-next-operation/#deployed-applications","text":"","title":"Deployed applications"},{"location":"kube/k8s-next-operation/#dx-core","text":"Kubernetes Type: StatefulSet with two persistent volumes (1x RWM, 1x RWO) The DX Core application is currently our application with the highest complexity in the Kubernetes deployment. This is mainly driven by the fact that DX Core, unlike our new applications, was not designed for a containerized deployment. The original deployment of DX Core in a clustered fashion usually relied on having multiple Machines / Nodes that have WAS ND deployed. WAS ND would then consist of deployment manager (DMGR) and multiple nodes - This is called a Cell. The primary configuration of all applications would be maintained on the DMGR and then synced throughout the Cell to all nodes. The applications, in our case DX, would be running on multiple nodes and be therefore highly available. In a containerized deployment, the goal is usually to get away from such stateful units. Especially the DMGR would be difficult. To get around that issue and still have a highly available deployment (and scalability) we leverage the \"Farming\" approach for deployment. In that case, each node - or in terms of Kubernetes: Pod - will access a shared persistent storage containing the DX profile, there is no DMGR that defines the profile and pushes it to the node. All nodes see the same profile at all time. The farming deployment brings certain issues with it, that should be mentioned: ConfigEngine Tasks can and must only be run on one DX instance at a time. Other instances in the farm are not allowed to run those tasks. All Nodes / Pods require a shared persistent file storage, which will then usually be something like NFS. To ensure that only one DX Pod is performing the initial setup in Kubernetes and runs ConfigEngine tasks, the current implementation leverages an operator which ensures that before tasks are run / init is done, only one DX Pod is running. That has the following implications: Startup behavior of DX Core is completely defined by the operator, the container image itself remains relatively \"dumb\" During configurations DX Core is not highly available and no load balancing between multiple Pods is possibly, since only one Pod is running There is a distinct relationship between DX Core and the operator. Versions of both must be matched or a successful operation is not guaranteed/supported There are multiple ways to go: Keep an operator that ensures that only DX Core Pod is running to prevent multiple execution of ConfigEngine Tasks / Writing conflicts Enhance the DX Core image + Kubernetes deployment to allow DX Core to have a consensus based configuration The preferred way would be having a consensus based deployment of multiple Pods containing DX Core. This means, that there would also be one elected leader Pod, which executed ConfigEngine tasks, while there are one or multiple follower Pods, which just read the profile. Also it could be thought about, having a persistence sync from leader to followers which is not using a RWM Volume. This would ease deployments, since not all cloud providers allow for RWM volumes. Using a consesus based deployment could dramatically reduce the dependency to an operator and could possibly allow for maintaining HA during configuration changes.","title":"DX Core"},{"location":"kube/k8s-next-operation/#ringapi","text":"Kubernetes Type: Deployment/ReplicaSet - stateless The RingAPI is a simple part of our deployment. It basically performs API wrapping actions for DX Core. It does not have any persistence, neither via a DB connection nor via a persistent volume. Thus scaling, initialization and running requires no application specific specialties.","title":"RingAPI"},{"location":"kube/k8s-next-operation/#content-composer","text":"Kubernetes Type: Deployment/ReplicaSet - stateless The Content Composer is a web application that consumes APIs provided by the RingAPI. It mainly consists of a web server that hosts static JS files used for the web application. It does not have any persistence, neither via a DB connection nor via a persistent volume. Thus scaling, initialization and running requires no application specific specialties. To use content composer integrated in DX, it is necessary to run a ConfigEngine task.","title":"Content Composer"},{"location":"kube/k8s-next-operation/#dam","text":"Kubernetes Type: StatefulSet with one persistent volume (1x RWM) The Digital Asset Management is a web application that consists of a UI and an API server. The API server uses a persistent volume and a database to provide functionality. DAM is designed to have no writing conflicts between multiple Pods. The application itself will ensure that all configuration tasks are only performed by one Pod. There is not special startup or initialization that needs to be performed by an operator or similar. To use DAM integrated inside DX, it is necessary to run a ConfigEngine task.","title":"DAM"},{"location":"kube/k8s-next-operation/#dam-persistence","text":"Kubernetes Type: Two StatefulSet with two persistent volumes (1x RWO, 1x RWO) The DAM Persistence consists of a RW leader and a RO follower, both having their own persistent volume. Replication is maintained by application logic (PostgreSQL). Since our current implementation of PostgreSQL has no real HA or loadbalancing, we currently use an operator to automatically switch over to RO if the RW Pod dies. This dependency on an operator must be removed for multiple reasons: Switchover from RW to RO is not zero-downtime No load balancing between multiple PostgreSQL Pods No scaling for load surges, we are basically limited to vertical scaling (e.g. allow for much CPU and Memory on a single DB instance) Operator switchover adds complexity to the deployment Operator switchover forms a possibly single point of failure The solution to that is to have a properly scalable Persistence Layer, e.g. HA PostgreSQL. With such a setup, the manually build switchover can be removed and the necessity for a DAM operator vanishes.","title":"DAM Persistence"},{"location":"kube/k8s-next-operation/#image-processor","text":"Kubernetes Type: Deployment/ReplicaSet - stateless The image processor is a very simple API based computing application that performs image manipulation. The Pod is completely stateless, therefore scaling, initialization and running requires no application specific specialties.","title":"Image Processor"},{"location":"kube/k8s-next-operation/#conclusion","text":"Our current implementation of operations using operators is mainly based on certain issues with our applications themselves. Instead of building solutions to handle the symptoms, we should focus on fixing them to make such constructs obsolete and reduce complexity. The goal in a containerized deployment should be to make applications ready for it and not force applications not made for it into a specific deployment structure. Side note: Having operators carrying much logic also has implications for customers that may not want to use Kubernetes, but rather just docker because they are just trying out things or want to quickly spin up development environments. Since operators rely on Kubernetes for their actions, all functionality they provide will not be directly available outside Kubernetes. Having our container images taking more responsibility for themselves allows customers and developers to get a faster start with DX.","title":"Conclusion"},{"location":"kube/k8s-next-operation/#monitoring-metrics","text":"We need to enable monitoring capabilities for our customers. This topic can be split into two parts: Health/Monitoring data exposure Data Aggregation and Visualization It is worth having a look at the current implementation of the SoFy team, which at least provides monitoring out of the box for Kubernetes seemingly using Prometheus and Grafana.","title":"Monitoring / Metrics"},{"location":"kube/k8s-next-operation/#healthmonitoring-data-exposure","text":"All our application Pods should expose a metrics endpoint that can be scraped by a metrics data aggregation tooling. The endpoint should ideally be available at the same port for all Pods, thus making the configuration easier and more straight forward. The format should orient on what is best practice, e.g. the Prometheus format.","title":"Health/Monitoring data exposure"},{"location":"kube/k8s-next-operation/#kubernetes-resource-data","text":"Kubernetes itself provides metrics using the Kube state metrics . This contains metrics regarding Kubernetes resources like nodes and Pods. The data provided by the metrics are in a Prometheus consumable format.","title":"Kubernetes Resource Data"},{"location":"kube/k8s-next-operation/#java-application-data","text":"For Java we can leverage the JMX Exporter . It provides metrics about the state of a JVM in a Prometheus consumable format.","title":"Java Application Data"},{"location":"kube/k8s-next-operation/#nodejs-application-data","text":"For NodeJS we can use the Prom-Client . It is a npm package that provides various metrics regarding the NodeJS eventloop, heap usage etc. It also allows to configure custom metrics that can be exposed. The data format is Prometheus consumable.","title":"NodeJS Application Data"},{"location":"kube/k8s-next-operation/#db-postgresql-data","text":"For our current persistence stack we can use PostgreSQL Exporter . It exposes various DB related metrics and stats directly coming from the database itself. The output format is Prometheus consumable.","title":"DB / PostgreSQL Data"},{"location":"kube/k8s-next-operation/#data-aggregation","text":"All metrics data exposure is worthless if we don't gather that data somewhere central. For that purpose applications like Prometheus should be used. Prometheus is a scraping + persistence engine that will automatically scrape configured endpoints for exported metrics in a defined interval. It allows for specific querying to view data. There are already existing prometheus helm charts that could be leveraged as a dependency or inspiration in our helm charts. For visualization Grafana may present a good choice. It is widely adopted and provides out of the box compatibility with Prometheus. It also comes with a default set of helm charts .","title":"Data Aggregation"},{"location":"kube/k8s-next-operation/#logging","text":"It is important that our customers are able to easily access logs during runtime. This is necessary for them to configure e.g. notifications based on certain log messages, as well as for debugging issues.","title":"Logging"},{"location":"kube/k8s-next-operation/#using-sidecar-containers-for-log-exposure","text":"The Kubernetes stack already provides a logging mechanism which exposes logs of containers / Pods. The logs maintained here are based of the stdout and stderr streams of the containers and their respective running application. This principle has a disadvantage: Whenever your application uses something different for logging, e.g. typically files, you may only be able to tail the output of one file during runtime, thus only expose one log file in the Pod. Kubernetes provides the concept of sidecar containers for exactly that case. Each sidecar container can expose a file as a log stream inside Kubernetes, allowing Kubernetes to log multiple log files of a single application. Since sidecar containers share the same volumes as the run containers, they can access and tail the log files, allowing for an easy implementation.","title":"Using Sidecar Containers for log exposure"},{"location":"kube/k8s-next-operation/#log-output-gathering","text":"Since Kubernetes already gathers all log output of Pods and their containers, the usual approach is to gather the logs via Kubernetes itself. There are two often used ways to do logging in Kubernetes: EKF / ELK (Elasticsearch for data aggregation, Kibana for visualization and Fluentd for log data collection) Loki + Grafana (Loki for aggregation, Grafana for visualization) The basic concept behind both is to take the logs of each cluster Node (which include the logs of the Pods running on that cluster), aggregate them and them add a visual interface to access them for monitoring, search and debugging.","title":"Log output gathering"},{"location":"kube/k8s-next-operation/#changing-configurations","text":"","title":"Changing configurations"},{"location":"kube/k8s-next-operation/#adjusting-resource-allocations","text":"Changing resource allocations like CPU/Memory requests or limits would be applied directly on the definition of Deployments/ReplicaSets. When the values have changed, Kubernetes will internally trigger its own reconciliation and roll out the update to the resources.","title":"Adjusting resource allocations"},{"location":"kube/k8s-next-operation/#adjusting-enableddisabling-applications","text":"If a customer wants to enable/disable particular parts of the deployment, this should be done via the Helm upgrade process. Since we deploy the necessary Resources incl. Routes, StatefulSets etc. directly during deployment and not via an operator, enabling the application in the values.yaml of helm would then create or destroy the resources based on the enablement condition. It would also adjust the configuration of the DX Core Pods, thus leading to the leader of those Pods to configure the application, e.g. DAM.","title":"Adjusting enabled/disabling applications"},{"location":"kube/k8s-next-operation/#change-application-configuration","text":"Since we would like to store our configuration in configMaps, we'd like to know when they have changed and rollout the updated configuration. Therefore we need a mechanism to trigger the rollout of that update via StatefulSets/Deployments. An easy way to do so, is to add a checksum annotation to the Pods of our StatefulSets/Deployments for each configMap they care about. As soon as a configMap is changed, the operator will update the checksum of the affected resources and Kubernetes will start to roll out updated Pods or restart them, performing a rolling update.","title":"Change application configuration"},{"location":"kube/k8s-next-operation/#change-validation","text":"Our configuration lives within configMaps and Kubernetes resources. Those are created by Helm or with DXCTL using Helm created yaml files. There are multiple ways to validate changes to the resources we are managing:","title":"Change validation"},{"location":"kube/k8s-next-operation/#using-helm","text":"Helm supports the usage of schema files for its values.yaml . Since all values we use in our deployment are derived from that file during install or upgrade, we can leverage that check.","title":"Using Helm"},{"location":"kube/k8s-next-operation/#using-kubernetes-validation-webhooks","text":"Kubernetes provides the principle of validation webhooks . In combination with an operator, that provides validation endpoints, we would be able to reject wrong/undesired changes to configMaps directly in Kubernetes during runtime. We can also provide fitting error messages that will the customers why the changes they planned to do will not work. This would not only be applicable to configMaps, but also to other Kubernetes Resources e.g. our Deployments or StatefulSets.","title":"Using Kubernetes validation webhooks"},{"location":"kube/k8s-next-operation/#application-lifecycle","text":"","title":"Application lifecycle"},{"location":"kube/k8s-next-operation/#initial-startup","text":"During the initial startup we need to ensure that DX Core will only run the initialization once and only in one running Pod. Since the wp_profile is shared between all Pods, the initialized profile will be available to all Pods after initialization. The initialization flow could look the following way: The desired set of DX Core Pods is created by the StatefulSet The Pods negotiate a leader Pod, all others will act as followers. The leading Pod will start the initialization process, all other Pods will not start DX Core until the initialization has been completed After the initialization has been performed by the leader, all Pods (incl. followers) will start their DX Core process The application is initialized and ready to serve This requires communication between the DX Core Pods, e.g. on the shared volume. The current state of initialization could be persistent in a lock/timestamp file inside wp_profile. Other applications like DAM for example do already have initialization logic that prevents multiple Pods from colliding.","title":"Initial startup"},{"location":"kube/k8s-next-operation/#scaling","text":"We can use HorizontalPodAutoscalers that are included in Kubernetes to automatically scale services up and down based on CPU and memory usage. In case we want to leverage custom metrics, e.g. DAM operations pressure, there are also ways to establish custom metrics in Kubernetes and use them in autoScalers.","title":"Scaling"},{"location":"kube/k8s-next-operation/#backup-restore","text":"Backup and Restore tasks could be run as Jobs, which are one-off Pods in Kubernetes that fulfill a specific task and return a completion status. Those jobs could be created by the operator, since they need to be created during the deployments lifecycle. Creation of those backup tasks could either be triggered manually, or run automatically in defined intervals. Backups could include copying specific files to backup volumes, dumping deployment information and state or even trigger functions like EXIM in DAM and put the output to a desired spot. Restoring of before used backups should also be managed by the operator, as it would be able to bring the applications into their desired state before re-importing the data into them. It could also do validate if a restore was successful, e.g. by checking specific health-checks of individual applications.","title":"Backup &amp; Restore"},{"location":"kube/k8s-next-operation/#used-kubernetes-structures","text":"","title":"Used Kubernetes structures"},{"location":"kube/k8s-next-operation/#operatorscontrollers","text":"There are certain actions inside our deployment that would require manual intervention to get to the desired goal. The basic principle of watching a type of resource and acting accordingly to changes is considered a controller in the Kubernetes ecosystem. An operator on the other hand can consist of multiple controllers for different resources and implements applications specific knowledge/actions. A basic example would be a reaction on configuration changes. When we change the configMap of one our applications, we want to make that application aware of the introduced changes. This is true for stateful and stateless applications. A simple, but yet effective way to force the rollout of those changes is to have a annotation at the Pods level, which just contains a checksum of the applications config map. If that checksum changes, the application will automatically cycled through, performing a rollout of the new configuration to all Pods. Helm could provide the checksum during execution. If the value changes happen in Helm and the customer uses Helm upgrade to apply those changes, we could easily update the annotations during that automatically and the changes will be applied. Since that builds up a complexity in the Helm charts and e.g. would not work if customers use the plain yaml output with DXCTL, unless we port that functionality to DXCTL. If the customer applies changes directly in Kubernetes on configMap level, we would not get any automated rollout. To cover all different aspects of changes to configuration, having an operator that watches configurations and modifies our Pod definitions accordingly, would ease the process for customers a lot and reduce implementation overhead in both Helm and DXCTL. Other cases where the operator could act with application specific logic:","title":"Operators/Controllers"},{"location":"kube/k8s-next-operation/#operatorcontroller-use-cases","text":"","title":"Operator/Controller use cases"},{"location":"kube/k8s-next-operation/#leading-pod-only-routes","text":"There may be the necessity to have routes that should only go to the leading DX Core Pod. Certain actions, like config changes should only happen in one of our DX Core Pods. The operator could use a leader lookup routine to configure certain routes to point to exactly that Pod, instead of pointing to a Service consisting of multiple Pods. This would require the operator to check who is the leading DX Core Pod in a regular fashion and update the Endpoint for the corresponding route. This could be a repeatable pattern, where we could have multiple pre defined Endpoints that share a common label, allowing the operator to apply the change easily to multiple routes at the same time.","title":"Leading Pod only Routes"},{"location":"kube/k8s-next-operation/#automated-configmap-application","text":"As mentioned in the introduction before, configuration changes on ConfigMap level do not induce a automatic rollout on dependent resources in Kubernetes. This logic needs to be established by us. The operator could watch all relevant ConfigMaps and adjust the checksum annotation of all dependent resources, e.g. StatefulSets and Deployments. The built in Kubernetes reconciliation would then automatically roll out those changes.","title":"Automated ConfigMap application"},{"location":"kube/k8s-next-operation/#dam-persistence-ro-fallback","text":"As long as we do not have a true loadbalanced HA persistence solution for DAM, we will rely on a ReadWrite Primary and ReadOnly Secondary PostgreSQL instance, which do automatically replicate. In order to have a fallback in the case of an outage of the primary DB, the operator would change the routing to the DB for DAM automatically to the secondary. This would be restored to use the primary as soon as the primary DB is up and ready again.","title":"DAM persistence RO fallback"},{"location":"kube/k8s-next-operation/#dx-core-configuration-proxy","text":"If we are running multiple instances of DX Core, we want customers to execute configuration changes only on the leading Pod. The operator could perform a leader lookup and perform as a proxy for such executions. Instead of directly executing kubectl exec commands in one of the DX Core Pods, they would be pointed to the operator Pod and could either be executed through a direct SSH tunnel to the Core Pod or initiating the Kubernetes exec command on the DX Core Pod. Customers would not need to know which of the Cores is the current leading one.","title":"DX Core configuration Proxy"},{"location":"kube/k8s-next-operation/#leading-pod-identification","text":"For configuration and debugging purpose it may be useful to know, which DX Core Pod is currently the leading one. A simple command that can be triggered in the operator Pod that return the Pod Name could already be sufficient. For other use cases the operator would already need to know which Pod is leading.","title":"Leading Pod identification"},{"location":"kube/k8s-next-operation/#configuration-validation","text":"The operator could provide a validation endpoint that can be used to validate the changes to Resources during runtime. This could include type checks and even specific value verification.","title":"Configuration validation"},{"location":"kube/k8s-next-operation/#dx-deployment-specific-metrics","text":"We can use the operator to expose metrics endpoints that aggregate DX deployment relevant health and status information. Since the operator can possibly know all members of the deployment in its namespace, it can act as a point of entry for quick health checks.","title":"DX Deployment Specific Metrics"},{"location":"kube/k8s-next-operation/#custom-resources-definition-vs-configmaps","text":"A classical operator design usually incorporates the use of a Custom Resource Definition or short CRD. A CRD is an extension to the Kubernetes API and basically allows for creation of custom resource definitions that reflect whatever is necessary. In our case, we used a custom resource (CR) called dxDeployment in the past. What content can be inside that CR is defined by the CRD. You could think of the CRD as a schema or object definition, from which you can create Objects inside Kubernetes. This definition can also contain validation of values. The operator would use this CR as its configuration backbone. ConfigMaps on the other hand are vanilla Kubernetes resources that contain a list of key-value-pairs. The values of those pairs could either be a single value or even be a whole file structure. Still in the end the principle is always key-value-pair. This is a simple data structure that allows for easy editing, but not so much flexibility in regards of structure. ConfigMaps, unlike CRs, can be directly consumed in some Kubernetes resources. Pods can either mount configMaps as a volume inside the Containers or extract values from them and set them as environment variables inside Containers. There is no necessary conversion logic that needs to be implemented. If we perform changes to our configuration, e.g. add a field, we would require the CRD to be updated prior to updating the CR. Also our operator would need the be update, even if it would only pass the configuration field through to another Kubernetes resource. Example: We have a CRD that defines certain parts of Pod specification. Unfortunately, we did not incorporate labels in the past. By our implementation logic, we take add the field to the CRD, then enhance the operator to be able to consume that field and pass it on through the Pod definition e.g. inside a StatefulSet. This apparently small change to just to enable a field for a Pod specification requires changes on CRD definition, changes to operator code, rolling out a new CRD and operator. Alternatively the label could just have directly been added to the StatefulSet in the first place, without being passed through an operator. Also, there has been a lot of discussion the the Helm community regarding the lifecycle of CRDs. Therefore Helm does currently not support updating CRDs , which will make the use of CRDs with SoFy harder for us and our customers, since SoFy relies on Helm. Because CRDs define a content/config structure and are cluster wide resources, special attention has to go into version updates of CRDs, updating them and testing use/edge cases like having two versions of a CRD in the cluster because two different versions of DX are deployed. (E.g. CF191 and CF193 on SoFy sandbox clusters.) Since an operator could also watch ConfigMaps as a source of configuration, we would like to use those.","title":"Custom Resources Definition vs. configMaps"},{"location":"kube/k8s-next-operation/#glossary","text":"DMGR: Deployment Manager from WAS ND, maintains deployment cells containing multiple WAS nodes. WAS ND: Websphere Application Server Network Deployment, Java application server used by DX Core. Cell: In WAS Terms consists of a DMGR and multiple nodes. RWO: ReadWriteOnce Volume - can only be accessed by one Pod, one to one relationship between volume and Pod. RWM: ReadWriteMany Volume - can be accessed by multiple Pods, one to many relationship between volume and Pods.","title":"Glossary"},{"location":"kube/k8s-next-runtime-log-levels-dbha/","text":"Postgres The log levels for Postgres are defined by the log_min_messages parameter . Supported values are: DEBUG5 , DEBUG4 , DEBUG3 , DEBUG2 , DEBUG1 , INFO , NOTICE , WARNING , ERROR , LOG , FATAL , PANIC Log levels for Postgres can be adjusted in different ways. Update postgresql.conf In the current build of the DBHA Postgres image, the postgresql.conf is located in /var/lib/pgsql/11/data/dx/ . It includes the log_min_messages which is commented out by default. After it was changed, the configuration can be reloaded without restarting Postgres with repmgr node service --action=reload Update in database Another way of configuring the log level is to alter the entry in the database directly. For that we can connect to the database inside the container using psql . To change and apply the level change, we use ALTER SYSTEM SET log_min_messages = 'DEBUG5'; SELECT pg_reload_conf(); Repmgr The log levels for the Repmgr are defined by the log_level parameter . Supported values are: DEBUG , INFO , NOTICE , WARNING , ERROR , ALERT , CRIT , EMERG Log levels for Postgres can be adjusted the following way. Update repmgr.conf In the current build of the DBHA Postgres image, the repmgr.conf is located in /etc/repmgr/11 . It includes the log_level which is commented out by default. After it was changed, the configuration can be reloaded without restarting Repmgr with kill -SIGHUP `cat /tmp/repmgrd.pid` Additional changes In order to make those changes applicable, we need to make some slight changes to the start_postgres.sh script. We need to remove the following lines to enable logging to stdout : __repmgr_set_property \"log_file\" \"${PG_CONFDIR}/repmgr/log/repmgr.log\" \"${REPMGR_CONF_DIR}/repmgr.conf\" __repmgr_set_property \"log_level\" \"NOTICE\" \"${REPMGR_CONF_DIR}/repmgr.conf\" Periodically check for global log config and translate to compatible log config We can create a shell script that runs in the image and periodically checks for changes in the global log config that is mounted in the container at /etc/global-config/log.persistenceNode . Example log string: pers-db:::info,pers-repmgr:::info #!/bin/bash # #################################################################### # Licensed Materials - Property of HCL # # # # Copyright HCL Technologies Ltd. 2021. All Rights Reserved. # # # # Note to US Government Users Restricted Rights: # # # # Use, duplication or disclosure restricted by GSA ADP Schedule # #################################################################### # # Load libraries . /set_repmgr_property.sh # Define service name PG_SERVICE=\"pers-db\" REPMGR_SERVICE=\"pers-repmgr\" REPMGR_CONF_DIR='/etc/repmgr/11' # Define log level mapping for Postgres declare -A PG_MAP PG_MAP['debug']='DEBUG5' # logs DEBUG5, DEBUG4, DEBUG3, DEBUG2, DEBUG1 PG_MAP['info']='INFO' # logs INFO, NOTICE, WARNING PG_MAP['error']='ERROR' # logs ERROR, LOG, FATAL, PANIC # Define log level mapping for Repmgr declare -A REPMGR_MAP REPMGR_MAP['debug']='DEBUG' # logs DEBUG REPMGR_MAP['info']='INFO' # logs INFO, NOTICE, WARNING REPMGR_MAP['error']='ERROR' # logs ERROR, ALERT, CRIT, EMERG Map_Level() { # Transform to lowercase INPUT_LEVEL=$(echo \"$1\" | tr '[:upper:]' '[:lower:]') SERVICE=$2 # Returns mapped value if it exists, oterwise empty string if [[ $SERVICE == $PG_SERVICE ]]; then echo ${PG_MAP[$INPUT_LEVEL]} elif [[ $SERVICE == $REPMGR_SERVICE ]]; then echo ${REPMGR_MAP[$INPUT_LEVEL]} else echo \"\" fi } Parse_And_Set_Level() { # Split by comma IFS=',' read -ra LOG_STR <<< $1 for LOG_PART in \"${LOG_STR[@]}\"; do # Split by colon IFS=':' read -ra LOG_STR_PARTS <<< $LOG_PART # Get common log pattern elements SERVICE=${LOG_STR_PARTS[0]} # Transform to lowercase LOWERCASE_SERVICE=$(echo \"$SERVICE\" | tr '[:upper:]' '[:lower:]') # Suffix and component are currently not used in this service # SUFFIX=${LOG_STR_PARTS[1]} # COMPONENT=${LOG_STR_PARTS[2]} LEVEL=${LOG_STR_PARTS[${#LOG_STR_PARTS[@]} - 1]} # Only apply logs for the matching service if [[ $LOWERCASE_SERVICE == $PG_SERVICE ]]; then MAPPED_LEVEL=$(Map_Level \"$LEVEL\" $PG_SERVICE) if [[ $MAPPED_LEVEL != \"\" ]]; then # Setting the log level echo \"Setting Postgres log level to $MAPPED_LEVEL\" && \\ psql -c \"ALTER SYSTEM SET log_min_messages = '$MAPPED_LEVEL';\" && \\ psql -c \"SELECT pg_reload_conf();\" || \\ echo \"Failed to set Postgres log level to $MAPPED_LEVEL\" fi elif [[ $LOWERCASE_SERVICE == $REPMGR_SERVICE ]]; then MAPPED_LEVEL=$(Map_Level \"$LEVEL\" $REPMGR_SERVICE) if [[ $MAPPED_LEVEL != \"\" ]]; then # Setting the log level echo \"Setting Repmgr log level to $MAPPED_LEVEL\" && \\ __repmgr_set_property \"log_level\" \"${MAPPED_LEVEL}\" \"${REPMGR_CONF_DIR}/repmgr.conf\" && \\ kill -HUP `cat /tmp/repmgrd.pid` || \\ echo \"Failed to set Repmgr log level to $MAPPED_LEVEL\" fi fi done } Periodically_Read_Ronfigfile() { FILE_PATH=$1 # Sleep time defaults to 10 seconds if not passed as argument SLEEP_TIME_SECONDS=${2:-10} LAST_CONFIG_STRING='' while [ true ]; do if [[ -f ${FILE_PATH} || -L ${FILE_PATH} ]]; then # Deal with the fact that the file referenced could actually be a symlink to a file FULLY_RESOLVED_FILE_PATH=$(readlink -f ${FILE_PATH}) # read exactly the first line from that file read -r first_line < ${FULLY_RESOLVED_FILE_PATH} if [ \"${LAST_CONFIG_STRING}\" != \"${first_line}\" ]; then LAST_CONFIG_STRING=${first_line} Parse_And_Set_Level \"$first_line\" else echo \"Nothing done because log config didn't change in ${FULLY_RESOLVED_FILE_PATH} in last\" ${SLEEP_TIME_SECONDS} \"seconds\" fi else echo \"Config map ${FILE_PATH} doesn't exist. No action taken\" fi sleep ${SLEEP_TIME_SECONDS} done } # Start periodic file check Periodically_Read_Ronfigfile \"/etc/global-config/log.persistenceNode\" 10 Pgpool The log levels for Pgpool are defined by the log_min_messages parameter . Supported values are: DEBUG5 , DEBUG4 , DEBUG3 , DEBUG2 , DEBUG1 , INFO , NOTICE , WARNING , ERROR , LOG , FATAL , PANIC Log levels for Pgpool can be adjusted in different ways. Update pgpool.conf In the current build of the DBHA Pgpool image, the pgpool.conf is located in /conf . It includes the log_min_messages which is commented out by default. After it was changed, the configuration can be reloaded without restarting Postgres with pgpool --config-file=/conf/pgpool.conf --hba-file=/conf/pool_hba.conf reload We are currently using Pgpool 4.1. With 4.2, a new command pcp_reload_config was introduced . If we upgrade to 4.2 in the future, this can probably replace the command above. Update in database (Do not use for our case) Another way of configuring the log level is to alter the entry in the database directly. For that we can connect to the database inside the container using Important Note: Do not use this for our use case! This can in some cases work unreliably if it is executed during failover or the setting is not present in all nodes. The config file is better to store the setting in a persistent way. psql postgres://<PGPOOL_SR_CHECK_USER>:<PGPOOL_SR_CHECK_PASSWORD>@localhost/postgres Example: psql postgres://repdxuser:d1gitalRepExperience@localhost/postgres To change and apply the level change, we use PGPOOL SET log_min_messages = 'DEBUG5'; Periodically check for global log config and translate to compatible log config We can create a shell script that runs in the image and periodically checks for changes in the global log config that is mounted in the container at /etc/global-config/log.persistenceConnectionPool . Example log string: pers-pool:::info #!/bin/bash # #################################################################### # Licensed Materials - Property of HCL # # # # Copyright HCL Technologies Ltd. 2021. All Rights Reserved. # # # # Note to US Government Users Restricted Rights: # # # # Use, duplication or disclosure restricted by GSA ADP Schedule # #################################################################### # # Load libraries . /scripts/libpgpool.sh # Load Pgpool env. variables eval \"$(pgpool_env)\" # Define service name CURRENT_SERVICE=\"pers-pool\" # Define log level mapping declare -A map map['debug']='DEBUG5' # logs DEBUG5, DEBUG4, DEBUG3, DEBUG2, DEBUG1 map['info']='INFO' # logs INFO, NOTICE, WARNING map['error']='ERROR' # logs ERROR, LOG, FATAL, PANIC Map_Level() { # Transform to lowercase INPUT_LEVEL=$(echo \"$1\" | tr '[:upper:]' '[:lower:]') # Returns mapped value if it exists, oterwise empty string echo ${map[$INPUT_LEVEL]} } Parse_And_Set_Level() { # Ex: \"pool:::info\" # Split by comma IFS=',' read -ra LOG_STR <<< $1 for LOG_PART in \"${LOG_STR[@]}\"; do # Split by colon IFS=':' read -ra LOG_STR_PARTS <<< $LOG_PART # Get common log pattern elements SERVICE=${LOG_STR_PARTS[0]} # Transform to lowercase LOWERCASE_SERVICE=$(echo \"$SERVICE\" | tr '[:upper:]' '[:lower:]') # Suffix and component are currently not used in this service # SUFFIX=${LOG_STR_PARTS[1]} # COMPONENT=${LOG_STR_PARTS[2]} LEVEL=${LOG_STR_PARTS[${#LOG_STR_PARTS[@]} - 1]} # Only apply logs for the matching service if [[ $LOWERCASE_SERVICE == $CURRENT_SERVICE ]]; then MAPPED_LEVEL=$(Map_Level \"$LEVEL\") if [[ $MAPPED_LEVEL != \"\" ]]; then # Setting the log level echo \"Setting log level to $MAPPED_LEVEL\" && \\ pgpool_set_property \"log_min_messages\" $MAPPED_LEVEL && \\ pgpool --config-file=$PGPOOL_CONF_FILE --hba-file=$PGPOOL_PGHBA_FILE reload || \\ echo \"Failed to set log level to $MAPPED_LEVEL\" fi fi done } Periodically_Read_Ronfigfile() { FILE_PATH=$1 # Sleep time defaults to 10 seconds if not passed as argument SLEEP_TIME_SECONDS=${2:-10} LAST_CONFIG_STRING='' while [ true ]; do if [[ -f ${FILE_PATH} || -L ${FILE_PATH} ]]; then # Deal with the fact that the file referenced could actually be a symlink to a file FULLY_RESOLVED_FILE_PATH=$(readlink -f ${FILE_PATH}) # read exactly the first line from that file read -r first_line < ${FULLY_RESOLVED_FILE_PATH} if [ \"${LAST_CONFIG_STRING}\" != \"${first_line}\" ]; then LAST_CONFIG_STRING=${first_line} Parse_And_Set_Level \"$first_line\" else echo \"Nothing done because log config didn't change in ${FULLY_RESOLVED_FILE_PATH} in last\" ${SLEEP_TIME_SECONDS} \"seconds\" fi else echo \"Config map ${FILE_PATH} doesn't exist. No action taken\" fi sleep ${SLEEP_TIME_SECONDS} done } # Start periodic file check Periodically_Read_Ronfigfile \"/etc/global-config/log.persistenceConnectionPool\" 10 It needs to be called from the entrypoint of the image and run permanently in the background.","title":"Runtime log levels - DBHA"},{"location":"kube/k8s-next-runtime-log-levels-dbha/#postgres","text":"The log levels for Postgres are defined by the log_min_messages parameter . Supported values are: DEBUG5 , DEBUG4 , DEBUG3 , DEBUG2 , DEBUG1 , INFO , NOTICE , WARNING , ERROR , LOG , FATAL , PANIC Log levels for Postgres can be adjusted in different ways.","title":"Postgres"},{"location":"kube/k8s-next-runtime-log-levels-dbha/#update-postgresqlconf","text":"In the current build of the DBHA Postgres image, the postgresql.conf is located in /var/lib/pgsql/11/data/dx/ . It includes the log_min_messages which is commented out by default. After it was changed, the configuration can be reloaded without restarting Postgres with repmgr node service --action=reload","title":"Update postgresql.conf"},{"location":"kube/k8s-next-runtime-log-levels-dbha/#update-in-database","text":"Another way of configuring the log level is to alter the entry in the database directly. For that we can connect to the database inside the container using psql . To change and apply the level change, we use ALTER SYSTEM SET log_min_messages = 'DEBUG5'; SELECT pg_reload_conf();","title":"Update in database"},{"location":"kube/k8s-next-runtime-log-levels-dbha/#repmgr","text":"The log levels for the Repmgr are defined by the log_level parameter . Supported values are: DEBUG , INFO , NOTICE , WARNING , ERROR , ALERT , CRIT , EMERG Log levels for Postgres can be adjusted the following way.","title":"Repmgr"},{"location":"kube/k8s-next-runtime-log-levels-dbha/#update-repmgrconf","text":"In the current build of the DBHA Postgres image, the repmgr.conf is located in /etc/repmgr/11 . It includes the log_level which is commented out by default. After it was changed, the configuration can be reloaded without restarting Repmgr with kill -SIGHUP `cat /tmp/repmgrd.pid`","title":"Update repmgr.conf"},{"location":"kube/k8s-next-runtime-log-levels-dbha/#additional-changes","text":"In order to make those changes applicable, we need to make some slight changes to the start_postgres.sh script. We need to remove the following lines to enable logging to stdout : __repmgr_set_property \"log_file\" \"${PG_CONFDIR}/repmgr/log/repmgr.log\" \"${REPMGR_CONF_DIR}/repmgr.conf\" __repmgr_set_property \"log_level\" \"NOTICE\" \"${REPMGR_CONF_DIR}/repmgr.conf\"","title":"Additional changes"},{"location":"kube/k8s-next-runtime-log-levels-dbha/#periodically-check-for-global-log-config-and-translate-to-compatible-log-config","text":"We can create a shell script that runs in the image and periodically checks for changes in the global log config that is mounted in the container at /etc/global-config/log.persistenceNode . Example log string: pers-db:::info,pers-repmgr:::info #!/bin/bash # #################################################################### # Licensed Materials - Property of HCL # # # # Copyright HCL Technologies Ltd. 2021. All Rights Reserved. # # # # Note to US Government Users Restricted Rights: # # # # Use, duplication or disclosure restricted by GSA ADP Schedule # #################################################################### # # Load libraries . /set_repmgr_property.sh # Define service name PG_SERVICE=\"pers-db\" REPMGR_SERVICE=\"pers-repmgr\" REPMGR_CONF_DIR='/etc/repmgr/11' # Define log level mapping for Postgres declare -A PG_MAP PG_MAP['debug']='DEBUG5' # logs DEBUG5, DEBUG4, DEBUG3, DEBUG2, DEBUG1 PG_MAP['info']='INFO' # logs INFO, NOTICE, WARNING PG_MAP['error']='ERROR' # logs ERROR, LOG, FATAL, PANIC # Define log level mapping for Repmgr declare -A REPMGR_MAP REPMGR_MAP['debug']='DEBUG' # logs DEBUG REPMGR_MAP['info']='INFO' # logs INFO, NOTICE, WARNING REPMGR_MAP['error']='ERROR' # logs ERROR, ALERT, CRIT, EMERG Map_Level() { # Transform to lowercase INPUT_LEVEL=$(echo \"$1\" | tr '[:upper:]' '[:lower:]') SERVICE=$2 # Returns mapped value if it exists, oterwise empty string if [[ $SERVICE == $PG_SERVICE ]]; then echo ${PG_MAP[$INPUT_LEVEL]} elif [[ $SERVICE == $REPMGR_SERVICE ]]; then echo ${REPMGR_MAP[$INPUT_LEVEL]} else echo \"\" fi } Parse_And_Set_Level() { # Split by comma IFS=',' read -ra LOG_STR <<< $1 for LOG_PART in \"${LOG_STR[@]}\"; do # Split by colon IFS=':' read -ra LOG_STR_PARTS <<< $LOG_PART # Get common log pattern elements SERVICE=${LOG_STR_PARTS[0]} # Transform to lowercase LOWERCASE_SERVICE=$(echo \"$SERVICE\" | tr '[:upper:]' '[:lower:]') # Suffix and component are currently not used in this service # SUFFIX=${LOG_STR_PARTS[1]} # COMPONENT=${LOG_STR_PARTS[2]} LEVEL=${LOG_STR_PARTS[${#LOG_STR_PARTS[@]} - 1]} # Only apply logs for the matching service if [[ $LOWERCASE_SERVICE == $PG_SERVICE ]]; then MAPPED_LEVEL=$(Map_Level \"$LEVEL\" $PG_SERVICE) if [[ $MAPPED_LEVEL != \"\" ]]; then # Setting the log level echo \"Setting Postgres log level to $MAPPED_LEVEL\" && \\ psql -c \"ALTER SYSTEM SET log_min_messages = '$MAPPED_LEVEL';\" && \\ psql -c \"SELECT pg_reload_conf();\" || \\ echo \"Failed to set Postgres log level to $MAPPED_LEVEL\" fi elif [[ $LOWERCASE_SERVICE == $REPMGR_SERVICE ]]; then MAPPED_LEVEL=$(Map_Level \"$LEVEL\" $REPMGR_SERVICE) if [[ $MAPPED_LEVEL != \"\" ]]; then # Setting the log level echo \"Setting Repmgr log level to $MAPPED_LEVEL\" && \\ __repmgr_set_property \"log_level\" \"${MAPPED_LEVEL}\" \"${REPMGR_CONF_DIR}/repmgr.conf\" && \\ kill -HUP `cat /tmp/repmgrd.pid` || \\ echo \"Failed to set Repmgr log level to $MAPPED_LEVEL\" fi fi done } Periodically_Read_Ronfigfile() { FILE_PATH=$1 # Sleep time defaults to 10 seconds if not passed as argument SLEEP_TIME_SECONDS=${2:-10} LAST_CONFIG_STRING='' while [ true ]; do if [[ -f ${FILE_PATH} || -L ${FILE_PATH} ]]; then # Deal with the fact that the file referenced could actually be a symlink to a file FULLY_RESOLVED_FILE_PATH=$(readlink -f ${FILE_PATH}) # read exactly the first line from that file read -r first_line < ${FULLY_RESOLVED_FILE_PATH} if [ \"${LAST_CONFIG_STRING}\" != \"${first_line}\" ]; then LAST_CONFIG_STRING=${first_line} Parse_And_Set_Level \"$first_line\" else echo \"Nothing done because log config didn't change in ${FULLY_RESOLVED_FILE_PATH} in last\" ${SLEEP_TIME_SECONDS} \"seconds\" fi else echo \"Config map ${FILE_PATH} doesn't exist. No action taken\" fi sleep ${SLEEP_TIME_SECONDS} done } # Start periodic file check Periodically_Read_Ronfigfile \"/etc/global-config/log.persistenceNode\" 10","title":"Periodically check for global log config and translate to compatible log config"},{"location":"kube/k8s-next-runtime-log-levels-dbha/#pgpool","text":"The log levels for Pgpool are defined by the log_min_messages parameter . Supported values are: DEBUG5 , DEBUG4 , DEBUG3 , DEBUG2 , DEBUG1 , INFO , NOTICE , WARNING , ERROR , LOG , FATAL , PANIC Log levels for Pgpool can be adjusted in different ways.","title":"Pgpool"},{"location":"kube/k8s-next-runtime-log-levels-dbha/#update-pgpoolconf","text":"In the current build of the DBHA Pgpool image, the pgpool.conf is located in /conf . It includes the log_min_messages which is commented out by default. After it was changed, the configuration can be reloaded without restarting Postgres with pgpool --config-file=/conf/pgpool.conf --hba-file=/conf/pool_hba.conf reload We are currently using Pgpool 4.1. With 4.2, a new command pcp_reload_config was introduced . If we upgrade to 4.2 in the future, this can probably replace the command above.","title":"Update pgpool.conf"},{"location":"kube/k8s-next-runtime-log-levels-dbha/#update-in-database-do-not-use-for-our-case","text":"Another way of configuring the log level is to alter the entry in the database directly. For that we can connect to the database inside the container using Important Note: Do not use this for our use case! This can in some cases work unreliably if it is executed during failover or the setting is not present in all nodes. The config file is better to store the setting in a persistent way. psql postgres://<PGPOOL_SR_CHECK_USER>:<PGPOOL_SR_CHECK_PASSWORD>@localhost/postgres Example: psql postgres://repdxuser:d1gitalRepExperience@localhost/postgres To change and apply the level change, we use PGPOOL SET log_min_messages = 'DEBUG5';","title":"Update in database (Do not use for our case)"},{"location":"kube/k8s-next-runtime-log-levels-dbha/#periodically-check-for-global-log-config-and-translate-to-compatible-log-config_1","text":"We can create a shell script that runs in the image and periodically checks for changes in the global log config that is mounted in the container at /etc/global-config/log.persistenceConnectionPool . Example log string: pers-pool:::info #!/bin/bash # #################################################################### # Licensed Materials - Property of HCL # # # # Copyright HCL Technologies Ltd. 2021. All Rights Reserved. # # # # Note to US Government Users Restricted Rights: # # # # Use, duplication or disclosure restricted by GSA ADP Schedule # #################################################################### # # Load libraries . /scripts/libpgpool.sh # Load Pgpool env. variables eval \"$(pgpool_env)\" # Define service name CURRENT_SERVICE=\"pers-pool\" # Define log level mapping declare -A map map['debug']='DEBUG5' # logs DEBUG5, DEBUG4, DEBUG3, DEBUG2, DEBUG1 map['info']='INFO' # logs INFO, NOTICE, WARNING map['error']='ERROR' # logs ERROR, LOG, FATAL, PANIC Map_Level() { # Transform to lowercase INPUT_LEVEL=$(echo \"$1\" | tr '[:upper:]' '[:lower:]') # Returns mapped value if it exists, oterwise empty string echo ${map[$INPUT_LEVEL]} } Parse_And_Set_Level() { # Ex: \"pool:::info\" # Split by comma IFS=',' read -ra LOG_STR <<< $1 for LOG_PART in \"${LOG_STR[@]}\"; do # Split by colon IFS=':' read -ra LOG_STR_PARTS <<< $LOG_PART # Get common log pattern elements SERVICE=${LOG_STR_PARTS[0]} # Transform to lowercase LOWERCASE_SERVICE=$(echo \"$SERVICE\" | tr '[:upper:]' '[:lower:]') # Suffix and component are currently not used in this service # SUFFIX=${LOG_STR_PARTS[1]} # COMPONENT=${LOG_STR_PARTS[2]} LEVEL=${LOG_STR_PARTS[${#LOG_STR_PARTS[@]} - 1]} # Only apply logs for the matching service if [[ $LOWERCASE_SERVICE == $CURRENT_SERVICE ]]; then MAPPED_LEVEL=$(Map_Level \"$LEVEL\") if [[ $MAPPED_LEVEL != \"\" ]]; then # Setting the log level echo \"Setting log level to $MAPPED_LEVEL\" && \\ pgpool_set_property \"log_min_messages\" $MAPPED_LEVEL && \\ pgpool --config-file=$PGPOOL_CONF_FILE --hba-file=$PGPOOL_PGHBA_FILE reload || \\ echo \"Failed to set log level to $MAPPED_LEVEL\" fi fi done } Periodically_Read_Ronfigfile() { FILE_PATH=$1 # Sleep time defaults to 10 seconds if not passed as argument SLEEP_TIME_SECONDS=${2:-10} LAST_CONFIG_STRING='' while [ true ]; do if [[ -f ${FILE_PATH} || -L ${FILE_PATH} ]]; then # Deal with the fact that the file referenced could actually be a symlink to a file FULLY_RESOLVED_FILE_PATH=$(readlink -f ${FILE_PATH}) # read exactly the first line from that file read -r first_line < ${FULLY_RESOLVED_FILE_PATH} if [ \"${LAST_CONFIG_STRING}\" != \"${first_line}\" ]; then LAST_CONFIG_STRING=${first_line} Parse_And_Set_Level \"$first_line\" else echo \"Nothing done because log config didn't change in ${FULLY_RESOLVED_FILE_PATH} in last\" ${SLEEP_TIME_SECONDS} \"seconds\" fi else echo \"Config map ${FILE_PATH} doesn't exist. No action taken\" fi sleep ${SLEEP_TIME_SECONDS} done } # Start periodic file check Periodically_Read_Ronfigfile \"/etc/global-config/log.persistenceConnectionPool\" 10 It needs to be called from the entrypoint of the image and run permanently in the background.","title":"Periodically check for global log config and translate to compatible log config"},{"location":"kube/k8s-next-runtime-log-levels/","text":"Introduction For each application of our DX deployment, we should have a consistent way to set log settings during runtime. Also, the log settings should be stored in a central place. Due to various technologies, different log technologies are used, like: DEBUG - NodeJS applications LOG4J, JUL - Java applications The log configuration formats of the different log technologies differ and therefore we need a way to unify/align them. With this proposal we do not intend to adjust the log output format. Assumptions / Restriction It will only work for applications on a Kubernetes deployment. (DX Core in a hybrid deployment will be excluded) Supported applications: DAM, CC, Core, RingAPI, Design Studio, Image Processor, Runtime Controller, Remote Search, DAM Persistence Open LDAP logging can not be configured this way Only the server log settings will be covered, the UI part is excluded. (Short side note: The logging for the React applications embedded in core will be configured through the same trace settings as for the server.) Only available for a HELM-based deployments Solution proposal General approach A global Config Map can be used as a central place for all log settings. The application Config Maps is not a good approach. (It is not a central place and each change will recycle the pods) We should also introduce a common log configuration format to align the different log technology formats (like DEBUG, JUL, LOG4J). Each application should monitor its own log settings from the global Config Map. Each application should also provide a functionality to change log settings at runtime. Global Config Map and monitoring own log settings Here is an example of how the global Config Map can look. apiVersion: \"v1\" kind: \"ConfigMap\" metadata: name: \"{{ .Release.Name }}-global\" labels: app: \"{{ .Release.Name }}-global\" # Using general labels, see _deployment_helpers.tpl {{- include \"hcl-dx-deployment.labels\" . | nindent 4 }} data: \"log.core\": \"<core log information>\" \"log.digitalAssetManagement\": \"<dam log information>\" All log settings are available inside the pods as mounted config maps under /etc/global-config . The application-specific log settings in the mounted config map need to be monitored by the application to adjust its log levels during runtime. This will obviously also require a change to the stateful set / replica set Helm charts to mount the additional config map. Short example to arrange that. volumeMounts: - name: \"global-config\" mountPath: \"/etc/global-config\" volumes: - name: \"global-config\" configMap: name: \"{{ .Release.Name }}-global\" Change log settings during runtime Each application should be able to consume and execute the new log settings without a restart of the application itself. A change of a log setting should be directly applied to the application log components (Logger). And a transformation of the common log format to the application-specific log would be needed. That means that we will have to do some adjustments to the application itself. Common log settings format The different logging frameworks offer a variety of levels. These levels will not be unified. LOG4J (Java): all, trace, debug, info, warn, error, fatal, off JUL: (Java) finest, finer, fine, config, info, warning, severe DEBUG (NodeJS): debug, info, error The usage of wildcards should also be possible. Each section of a single application component must be separated by a unique delimiter, the colon should be a good delimiter. For a definition of multiple application components, we need also a delimiter, the comma should be a good option for that. Here is an example of the proposed common log pattern: <component>:<pattern>=<level>,<component>:<pattern>=<level> - component > api, worker, wp_profile, cw_profile, prs_profile, repmgr, pgpool, psql (No Wildcards) - pattern > com.hcl.App, com.hcl.util.Data - level > error, info, debug (Always includes higher levels) LOG4J example: logging framework specific log settings: com.hcl.App=info com.hcl.util.Data=finest - component > wp_profile - pattern > com.hcl.App, com.hcl.util.Data - level > info, finest Common log settings: wp_profile::com.hcl.App=info,wp_profile::com.hcl.util.Data=finest DEBUG example: logging framework specific log settings: INFO:api:server-v1:dist* INFO:worker:server-v1:dist* DEBUG:api:server-v1:dist:server* - component > api, worker - pattern > dist, dist:server - level > info, debug Common log message: api:server-v1:dist=info,worker:server-v1:dist=info,api:server-v1:dist:server=debug, Supported component names Each application consists of one or multiple components. Supported component names are: Application Supported component names Content Composer api Core wp_profile , cw_profile DAM persistence psql , repmgr , pgpool Design Studio api Digital Asset Management api , worker Image Processor api Remote Search prs_profile Ring API api Runtime Controller controller Necessary code changes to the currently existing implementation (end of October 2021) Core and Remote Search Adjust the component identifier to use the new component names. Check if the already implemented logic supports log level settings for cw_profile . Otherwise create a ticket to ensure the implementation. enchanted logger Rework the log output format to move the log level to the beginning of the string in caps Rework the log settings transformation to match the proposed format Adjust the applications to match the components Rename the applications in their package.json files to adjust the package name Logging 3rd party DEBUG packages Many other packages like express or loopback utilize the same logging framework called DEBUG . Unfortunately there is no way to change the log level for these packages during runtime. Therefore we need to provide instructions for our support team on how to set tracings for these packages by setting the DEBUG environment variable. Adjusting the DEBUG environment variable will change the pod spec which will lead to a restart of the pod. This implicitly resolves the inability to change these log settings during runtime. Support log levels We have decided that we are supporting only three log levels to align the log levels for all our applications. Only DX Core and Remote Search are excluded as they require a more flexible configuration. Which log levels are we supporting? error, info, debug All NodeJs applications are following this restriction and using only this three log level. For other application (like JAVA) a mapping is needed. Mapping the levels that can be set in the Helm values to the log levels of the services Service debug info error Postgres DEBUG5 , DEBUG4 , DEBUG3 , DEBUG2 , DEBUG1 INFO , NOTICE , WARNING ERROR , LOG , FATAL , PANIC Repmgr DEBUG INFO , NOTICE , WARNING ERROR , ALERT , CRIT , EMERG Pgpool DEBUG5 , DEBUG4 , DEBUG3 , DEBUG2 , DEBUG1 INFO , NOTICE , WARNING ERROR , LOG , FATAL , PANIC Runtime Controller FINEST , FINER , FINE , CONFIG INFO , WARNING SEVERE Content Composer debug info error Digital Asset Management debug info error Ring API debug info error Design Studio debug info error Image Processor debug info error Log output for NodeJS apps We move the log level to the front of a log message, thus making the output more similar to other applications e.g. the ones that are based on WAS. Sample log: INFO:api:server-v1:dist:server.js[12] hello world ERROR:api:server-v1:dist:server.js[12] hello world DEBUG:api:server-v1:dist:server.js[12] hello world","title":"Runtime log levels"},{"location":"kube/k8s-next-runtime-log-levels/#introduction","text":"For each application of our DX deployment, we should have a consistent way to set log settings during runtime. Also, the log settings should be stored in a central place. Due to various technologies, different log technologies are used, like: DEBUG - NodeJS applications LOG4J, JUL - Java applications The log configuration formats of the different log technologies differ and therefore we need a way to unify/align them. With this proposal we do not intend to adjust the log output format.","title":"Introduction"},{"location":"kube/k8s-next-runtime-log-levels/#assumptions-restriction","text":"It will only work for applications on a Kubernetes deployment. (DX Core in a hybrid deployment will be excluded) Supported applications: DAM, CC, Core, RingAPI, Design Studio, Image Processor, Runtime Controller, Remote Search, DAM Persistence Open LDAP logging can not be configured this way Only the server log settings will be covered, the UI part is excluded. (Short side note: The logging for the React applications embedded in core will be configured through the same trace settings as for the server.) Only available for a HELM-based deployments","title":"Assumptions / Restriction"},{"location":"kube/k8s-next-runtime-log-levels/#solution-proposal","text":"","title":"Solution proposal"},{"location":"kube/k8s-next-runtime-log-levels/#general-approach","text":"A global Config Map can be used as a central place for all log settings. The application Config Maps is not a good approach. (It is not a central place and each change will recycle the pods) We should also introduce a common log configuration format to align the different log technology formats (like DEBUG, JUL, LOG4J). Each application should monitor its own log settings from the global Config Map. Each application should also provide a functionality to change log settings at runtime.","title":"General approach"},{"location":"kube/k8s-next-runtime-log-levels/#global-config-map-and-monitoring-own-log-settings","text":"Here is an example of how the global Config Map can look. apiVersion: \"v1\" kind: \"ConfigMap\" metadata: name: \"{{ .Release.Name }}-global\" labels: app: \"{{ .Release.Name }}-global\" # Using general labels, see _deployment_helpers.tpl {{- include \"hcl-dx-deployment.labels\" . | nindent 4 }} data: \"log.core\": \"<core log information>\" \"log.digitalAssetManagement\": \"<dam log information>\" All log settings are available inside the pods as mounted config maps under /etc/global-config . The application-specific log settings in the mounted config map need to be monitored by the application to adjust its log levels during runtime. This will obviously also require a change to the stateful set / replica set Helm charts to mount the additional config map. Short example to arrange that. volumeMounts: - name: \"global-config\" mountPath: \"/etc/global-config\" volumes: - name: \"global-config\" configMap: name: \"{{ .Release.Name }}-global\"","title":"Global Config Map and monitoring own log settings"},{"location":"kube/k8s-next-runtime-log-levels/#change-log-settings-during-runtime","text":"Each application should be able to consume and execute the new log settings without a restart of the application itself. A change of a log setting should be directly applied to the application log components (Logger). And a transformation of the common log format to the application-specific log would be needed. That means that we will have to do some adjustments to the application itself.","title":"Change log settings during runtime"},{"location":"kube/k8s-next-runtime-log-levels/#common-log-settings-format","text":"The different logging frameworks offer a variety of levels. These levels will not be unified. LOG4J (Java): all, trace, debug, info, warn, error, fatal, off JUL: (Java) finest, finer, fine, config, info, warning, severe DEBUG (NodeJS): debug, info, error The usage of wildcards should also be possible. Each section of a single application component must be separated by a unique delimiter, the colon should be a good delimiter. For a definition of multiple application components, we need also a delimiter, the comma should be a good option for that. Here is an example of the proposed common log pattern: <component>:<pattern>=<level>,<component>:<pattern>=<level> - component > api, worker, wp_profile, cw_profile, prs_profile, repmgr, pgpool, psql (No Wildcards) - pattern > com.hcl.App, com.hcl.util.Data - level > error, info, debug (Always includes higher levels) LOG4J example: logging framework specific log settings: com.hcl.App=info com.hcl.util.Data=finest - component > wp_profile - pattern > com.hcl.App, com.hcl.util.Data - level > info, finest Common log settings: wp_profile::com.hcl.App=info,wp_profile::com.hcl.util.Data=finest DEBUG example: logging framework specific log settings: INFO:api:server-v1:dist* INFO:worker:server-v1:dist* DEBUG:api:server-v1:dist:server* - component > api, worker - pattern > dist, dist:server - level > info, debug Common log message: api:server-v1:dist=info,worker:server-v1:dist=info,api:server-v1:dist:server=debug,","title":"Common log settings format"},{"location":"kube/k8s-next-runtime-log-levels/#supported-component-names","text":"Each application consists of one or multiple components. Supported component names are: Application Supported component names Content Composer api Core wp_profile , cw_profile DAM persistence psql , repmgr , pgpool Design Studio api Digital Asset Management api , worker Image Processor api Remote Search prs_profile Ring API api Runtime Controller controller","title":"Supported component names"},{"location":"kube/k8s-next-runtime-log-levels/#necessary-code-changes-to-the-currently-existing-implementation-end-of-october-2021","text":"","title":"Necessary code changes to the currently existing implementation (end of October 2021)"},{"location":"kube/k8s-next-runtime-log-levels/#core-and-remote-search","text":"Adjust the component identifier to use the new component names. Check if the already implemented logic supports log level settings for cw_profile . Otherwise create a ticket to ensure the implementation.","title":"Core and Remote Search"},{"location":"kube/k8s-next-runtime-log-levels/#enchanted-logger","text":"Rework the log output format to move the log level to the beginning of the string in caps Rework the log settings transformation to match the proposed format Adjust the applications to match the components Rename the applications in their package.json files to adjust the package name","title":"enchanted logger"},{"location":"kube/k8s-next-runtime-log-levels/#logging-3rd-party-debug-packages","text":"Many other packages like express or loopback utilize the same logging framework called DEBUG . Unfortunately there is no way to change the log level for these packages during runtime. Therefore we need to provide instructions for our support team on how to set tracings for these packages by setting the DEBUG environment variable. Adjusting the DEBUG environment variable will change the pod spec which will lead to a restart of the pod. This implicitly resolves the inability to change these log settings during runtime.","title":"Logging 3rd party DEBUG packages"},{"location":"kube/k8s-next-runtime-log-levels/#support-log-levels","text":"We have decided that we are supporting only three log levels to align the log levels for all our applications. Only DX Core and Remote Search are excluded as they require a more flexible configuration. Which log levels are we supporting? error, info, debug All NodeJs applications are following this restriction and using only this three log level. For other application (like JAVA) a mapping is needed. Mapping the levels that can be set in the Helm values to the log levels of the services Service debug info error Postgres DEBUG5 , DEBUG4 , DEBUG3 , DEBUG2 , DEBUG1 INFO , NOTICE , WARNING ERROR , LOG , FATAL , PANIC Repmgr DEBUG INFO , NOTICE , WARNING ERROR , ALERT , CRIT , EMERG Pgpool DEBUG5 , DEBUG4 , DEBUG3 , DEBUG2 , DEBUG1 INFO , NOTICE , WARNING ERROR , LOG , FATAL , PANIC Runtime Controller FINEST , FINER , FINE , CONFIG INFO , WARNING SEVERE Content Composer debug info error Digital Asset Management debug info error Ring API debug info error Design Studio debug info error Image Processor debug info error","title":"Support log levels"},{"location":"kube/k8s-next-runtime-log-levels/#log-output-for-nodejs-apps","text":"We move the log level to the front of a log message, thus making the output more similar to other applications e.g. the ones that are based on WAS. Sample log: INFO:api:server-v1:dist:server.js[12] hello world ERROR:api:server-v1:dist:server.js[12] hello world DEBUG:api:server-v1:dist:server.js[12] hello world","title":"Log output for NodeJS apps"},{"location":"kube/k8s-next-undeploy/","text":"Status Date APPROVED 29th of April 2021 Introduction Undeploy unlike delete will not just kill and destroy an existing deployment no matter of the result. We want to provide an easy to handle, reliable and transparent Kubernetes undeployment. From the handling perspective this means that customers should have the ability to select what they want to keep (i.e. namespace information, app configurations, data) from the existing deployment. This document provides a proposal on how this undeployment could look like. Initial considerations To not interfere with running actions within the system (i.e. active client upload, active backup process) an undeployment should run in a fully controlled system. That means no external client interactions should be possible and no ongoing data nor configuration changes should be active. To drop external interactions manually we need to provide a documentation to the customer on how to do this or we can provide this as an automation before starting the bare undeployment. For backup and config job we need to provide an automated checkup before the undeployment. Minimum undeploy actions The minimal functionallity for a Kube undeployment is what we currently provide in our automation: Delete the namespace (or OpenShift project) which in turn deletes all scoped resoucres under it Delete actions not yet provided but maybe considered In addition to the delete actions we already provide we should add optional actions the customer can choose from. Delete CRDs Check for possible Kube V1 artifacts Save data and configurations In addition to just deleting the environment we need to provide the capabilty to save data, configurations, and/or deployment informations from the existing environment for import in a new deployment. This includes the following actions: Export current configuration Save persistent data Save exisitng backups Save existing logs Undeployment CLI and UI Besides the pure delete actions the undeployment needs to provide a UI and CLI. An UI would better visualize the user input and actions rather than just running single commands from command line but is not mandatory in the first place. The CLI/UI takes the appropriate input from the customer: List of parts always undeployed (no choice) List of undeployable parts the customer can select List of data, configurations, deployment information the customer can select to save them Dry-Run feature Final question if undeployment really should run Undeploy process After the customer has confirmed the undeploy process will start. The undeploy process needs to run seamless for the various Kube flavors. Therefor the process needs a configuration file or runtime parameter to provide default information (i.e. Kube flavor, userid/password, tools to run certain actions, etc.). Following is an illustration about the straight forward undeploy process. Customer addons In addition the final undeploy process should provide pre- and post-hooks (customer entry points) for certain actions so the customer can add his own tools and/or flavors.","title":"Undeploy"},{"location":"kube/k8s-next-undeploy/#introduction","text":"Undeploy unlike delete will not just kill and destroy an existing deployment no matter of the result. We want to provide an easy to handle, reliable and transparent Kubernetes undeployment. From the handling perspective this means that customers should have the ability to select what they want to keep (i.e. namespace information, app configurations, data) from the existing deployment. This document provides a proposal on how this undeployment could look like.","title":"Introduction"},{"location":"kube/k8s-next-undeploy/#initial-considerations","text":"To not interfere with running actions within the system (i.e. active client upload, active backup process) an undeployment should run in a fully controlled system. That means no external client interactions should be possible and no ongoing data nor configuration changes should be active. To drop external interactions manually we need to provide a documentation to the customer on how to do this or we can provide this as an automation before starting the bare undeployment. For backup and config job we need to provide an automated checkup before the undeployment.","title":"Initial considerations"},{"location":"kube/k8s-next-undeploy/#minimum-undeploy-actions","text":"The minimal functionallity for a Kube undeployment is what we currently provide in our automation: Delete the namespace (or OpenShift project) which in turn deletes all scoped resoucres under it","title":"Minimum undeploy actions"},{"location":"kube/k8s-next-undeploy/#delete-actions-not-yet-provided-but-maybe-considered","text":"In addition to the delete actions we already provide we should add optional actions the customer can choose from. Delete CRDs Check for possible Kube V1 artifacts","title":"Delete actions not yet provided but maybe considered"},{"location":"kube/k8s-next-undeploy/#save-data-and-configurations","text":"In addition to just deleting the environment we need to provide the capabilty to save data, configurations, and/or deployment informations from the existing environment for import in a new deployment. This includes the following actions: Export current configuration Save persistent data Save exisitng backups Save existing logs","title":"Save data and configurations"},{"location":"kube/k8s-next-undeploy/#undeployment-cli-and-ui","text":"Besides the pure delete actions the undeployment needs to provide a UI and CLI. An UI would better visualize the user input and actions rather than just running single commands from command line but is not mandatory in the first place. The CLI/UI takes the appropriate input from the customer: List of parts always undeployed (no choice) List of undeployable parts the customer can select List of data, configurations, deployment information the customer can select to save them Dry-Run feature Final question if undeployment really should run","title":"Undeployment CLI and UI"},{"location":"kube/k8s-next-undeploy/#undeploy-process","text":"After the customer has confirmed the undeploy process will start. The undeploy process needs to run seamless for the various Kube flavors. Therefor the process needs a configuration file or runtime parameter to provide default information (i.e. Kube flavor, userid/password, tools to run certain actions, etc.). Following is an illustration about the straight forward undeploy process.","title":"Undeploy process"},{"location":"kube/k8s-next-undeploy/#customer-addons","text":"In addition the final undeploy process should provide pre- and post-hooks (customer entry points) for certain actions so the customer can add his own tools and/or flavors.","title":"Customer addons"},{"location":"kube/k8s-next-update/","text":"Status Date APPROVED 22nd of April 2021 Introduction For our Kubernetes deployments we also need to provide the possibility for customers to do updates. Ideally for our customers there would be no downtime when they perform an update. The update process should also cover possibilities to perform rollbacks if necessary. Affected Resources This chapter lists all important resources in our deployments and their requirements regarding updates. Stateful Applications We currently have three stateful applications with distinct requirements: DX Core DX Core uses two persistent storages for its operation. One of them is a (multi-pod shared) persistent volume which contains the wp_profile. The second persistence is realized using a relational database. That database is not part of the current DX Kube deployment. Therefore, all interactions with that DB need to be done by the operation/administration team of the customer OR by DX Core directly. DAM DAM uses two persistent storages for its operation. One of them is a (multi-pod shared) persistent volume which contains the DAM upload data and acts as a storage for binaries. The second persistence is realized using a relational database in form of PostgreSQL. That database is deployed inside our DX Kube deployments and can therefore be accessed by us during updated directly. It would be possibly for either the operation/administration team of the customer, the DX operator or DAM itself to perform necessary actions on that DB. DAM Persistence (PostgreSQL) The DAM persistence layer is one of the two persistence parts of DAM. It currently consists of two distinct Pods, where one is a RW primary DB node and the other one is a RO secondary DB node. Both have their own persistent volumes that are not shared. The persistence layer container images contain no schemas or application data, since they will be initialized by DAM. Changes to schemas would also be done by DAM. From an update perspective the most important aspect is to keep the existing data storage of the Pods compatible with the new DB version that comes in a newer container. This basically relies on how PostgreSQL behaves between version updates and needs to be verified. Stateless Applications RingAPI RingAPI is a stateless application that only uses configuration parameters and acts as a wrapping API server around DX Core. From an update perspective there are no special requirements that need to be taken into account. Updating RingAPI basically boils down to deploying a new version of the container image. Content Composer Content Composer is a stateless application that only uses configuration parameters and acts as a webserver hosting the resources for the Content Composer UI. From an update perspective there are no special requirements that need to be taken into account. Updating Content Composer basically boils down to deploying a new version of the container image. Image Processor Content Composer is a stateless application that only uses configuration parameters and acts as a media transformation service. It only uses HTTP as communication protocol and requires no persistence. From an update perspective there are no special requirements that need to be taken into account. Updating the Image Processor basically boils down to deploying a new version of the container image. Configurations ConfigMaps For general configuration we use two types von ConfigMaps. The first one is an application specific ConfigMap which contains all configuration necessary for individual applications to run. Those ConfigMaps are only consumed by the related application. The second type of ConfigMap is a global deployment wide ConfigMap, that contains configuration that affects all applications. The values of these ConfigMap entries will be directly updated during the update process and applications will be informed about the changes. Secrets For storing secure data like certificates and access credentials, we use Kubernetes Secrets. These secrets will be consumed by our applications. Content of those secrets will be directly updated during the update process and applications will be informed about the changes. Utility Resources Utility Resources e.g. HorizontalPodAutoscalers will be updated directly through the update process. Ambassador deployment We need to replace Ambassador Resources if they have changes. This will happen directly through the update process. Updating Backup Before updating, we need to provide customers with a way to create a backup of the important parts of the deployment. This includes persistent volumes and the data in the DBs of DX Core and DAM. This could be done by: A Kubernetes Job running, triggered by Helm or DXCTL The applications themselves before updating the schemas Update Categories Updates we perform can have multiple implications. Based on the way they change existing deployments, we can categorize them in different update categories. Based on in which category an update fits in, different actions or handling might be necessary. A - Application changes As we implement features and bugfixes to our applications, the application runtime code will change. This can have implications to configuration and persistence schemas, but that is not always the case. Often enough, application logic will be altered, without affecting existing environmental resources. C - Configuration changes As we perform development on our applications, it might be necessary to either introduce new configuration parameters or alter existing ones to reflect the current state of implementation. It can also happen that default behaviors/values might be adjusted between releases, e.g. if performance assessments have shown, that a Pod needs fewer resources and we lower the Kubernetes resource requests. P - Persistence schema changes For our stateful applications, such as DAM or DX Core, it may become necessary to perform changes to our persistence schemas. This would usually affect the schema of the DAM DB or the DX Core DB. This can happen if we need to introduce new fields into existing data structures. N - New application introduction As we move on with our developments for DX, new applications will arise that will be deployed within a DX deployment. Those applications will require new configurations, Kubernetes resources and perhaps preparation. As an example the roll-out of site-manager would fall into this category. R - Removal/replacement of an application Similar to when we add applications, there may also be parts that will be removed because they perhaps got obsolete or are being replaced with other applications. In that case it is important to ensure that the old resources get cleaned up, the configuration, if necessary, gets removed and the DX deployment is made aware that the application is gone and is not having any dependency anymore. Scenario: Update only containing configuration and application changes During this update the customer can either use helm or dxctl to deploy the new version of our DX deployment. With that, all resources affected will receive their respective changes. Statefulsets Since we directly define the Statefulsets in our Helm chart, the update will cause them to receive the new specification to the kubernetes resource. Kubernetes itself will then start its reconciliation and will ensure that the Pods of the statefulset will be restarted with the new configuration. See Statefulset Rolling Update for reference. This logic is implemented in kubernetes itself and will roll out the changes to the application and will only continue updating the next Pod, if the first one has been restarted with the new configuration and got ready. Deployments/Replicasets Applications that are stateless and run as a Deployment will be receiving the updated specification as well directly via the update process. As soon as the Deployment Resource gets altered, Kubernetes will perform a rollover, spawning new Pods with the new configuration. As soon as the new Pods are running successfully and are ready, the old Pods will be terminated. See Deployment Rollover Update for reference. ConfigMaps/Secrets ConfigMaps and Secrets will directly be updated via the update process. Utility Resource Utility Resources like HorizontalPodAutoscalers etc. will directly be updated via the update process. Scenario: Update also including persistence schema changes During this update the customer can either use helm or dxctl to deploy the new version of our DX deployment. With that, all resources affected will receive their respective changes. StatefulSets Since we directly define the StatefulSets in our Helm chart, the update will cause them to receive the new specification to the kubernetes resource. Kubernetes itself will then start its reconciliation and will ensure that the Pods of the StatefulSet will be restarted with the new configuration. See StatefulSet Rolling Update for reference. This logic is implemented in kubernetes itself and will roll out the changes to the application and will only continue updating the next Pod, if the first one has been restarted with the new configuration and got ready. As soon as schema changes are involved, it may become necessary that a preflight task needs to be run to e.g. to migrate a database schema before deploying new Deployments etc. We could leverage one of the following approaches to run necessary pre-flight tasks: Container integrated logic that checks if schema migration tasks are necessary to run Kubernetes Init Containers attached to the Pods, running specific tasks Kubernetes Jobs that run once and report back completion (could also run as a Helm Hook) While DAM will have automated logic to handle version to version migration of database schemas, for DX Core it might be necessary to perform schema updates that are triggered by the operators/administrators of the customer. Since we do not directly manage the DX Database ourselves, Database operations would preferably not done by our deployment directly, unless we are sure the logic we implemented in DX Core is failsafe. Deployments/Replicasets Applications that are stateless and run as a Deployment will be receiving the updated specification as well directly via the update process. As soon as the Deployment Resource gets altered, Kubernetes will perform a rollover, spawning new Pods with the new configuration. As soon as the new Pods are running successfully and are ready, the old Pods will be terminated. See Deployment Rollover Update for reference. ConfigMaps/Secrets ConfigMaps and Secrets will directly be updated via the update process. Utility Resource Utility Resources like HorizontalPodAutoscalers etc. will directly be updated via the update process. Rollback Rolling back our application is mostly relying on the fact if our applications can roll back to older persistence schemas. For disaster recovery we would use the backups done before. Using those backups of persistent volumes and DBs would allow a customer to go back to a previous version of our deployment, assuming the performed a rollback to the old versions of the deployment e.g. using helm rollback . Tooling Helm Customers should use helm upgrade to perform updates on their existing deployments. Using helm for upgrades provides both us and our customers with the benefit of hierarchical configurations. Per default, a helm chart uses the values.yaml to determine what values should be used in the deployment. A helm chart is always delivered with that file, since it includes all the necessary default values for the deployment. For customers there might be customer specific configuration, e.g. scaling settings. We do not want customers to have the need to manually copy over their existing settings into our default values.yaml OR to copy the new values of the default values.yaml into their custom settings file. This can create merge conflicts, wrong configuration and makes the updating process more complicated for customers. With the hierarchical nature of helm configurations, we have the following pattern at hand: Especially important in our case is the use of our default values.yaml and a user defined custom_values.yaml . With every version of DX we ship, we will deliver a default values.yaml that contains all necessary information to get a DX Kube deployment up and running. This also includes image names, version tags etc. If a customer wants to customize the deployment, they could either edit our default values.yaml and apply the changes, or create their own file and only change the values that are of importance to them. The benefit is quiet obvious: Customers can keep customized deployment configuration in one file, while we can always ship a fitting set of defaults that does not overwrite the customers values. The lifecycle of that configuration can be seen in the following diagram: Customers can retain their own custom values file, while we always ship a new one with each release. With that, we don't override customer settings, but always deliver working default deployment values. Note: The custom values file approach can also be used to provide different deployment templates to our customers, without overwriting our default values.yaml DXCTL DXTCL should ideally be able to consume outputs of the Helm templating engine and use that output to update existing deployments in a similar fashion as it does today already.","title":"Updating"},{"location":"kube/k8s-next-update/#introduction","text":"For our Kubernetes deployments we also need to provide the possibility for customers to do updates. Ideally for our customers there would be no downtime when they perform an update. The update process should also cover possibilities to perform rollbacks if necessary.","title":"Introduction"},{"location":"kube/k8s-next-update/#affected-resources","text":"This chapter lists all important resources in our deployments and their requirements regarding updates.","title":"Affected Resources"},{"location":"kube/k8s-next-update/#stateful-applications","text":"We currently have three stateful applications with distinct requirements:","title":"Stateful Applications"},{"location":"kube/k8s-next-update/#dx-core","text":"DX Core uses two persistent storages for its operation. One of them is a (multi-pod shared) persistent volume which contains the wp_profile. The second persistence is realized using a relational database. That database is not part of the current DX Kube deployment. Therefore, all interactions with that DB need to be done by the operation/administration team of the customer OR by DX Core directly.","title":"DX Core"},{"location":"kube/k8s-next-update/#dam","text":"DAM uses two persistent storages for its operation. One of them is a (multi-pod shared) persistent volume which contains the DAM upload data and acts as a storage for binaries. The second persistence is realized using a relational database in form of PostgreSQL. That database is deployed inside our DX Kube deployments and can therefore be accessed by us during updated directly. It would be possibly for either the operation/administration team of the customer, the DX operator or DAM itself to perform necessary actions on that DB.","title":"DAM"},{"location":"kube/k8s-next-update/#dam-persistence-postgresql","text":"The DAM persistence layer is one of the two persistence parts of DAM. It currently consists of two distinct Pods, where one is a RW primary DB node and the other one is a RO secondary DB node. Both have their own persistent volumes that are not shared. The persistence layer container images contain no schemas or application data, since they will be initialized by DAM. Changes to schemas would also be done by DAM. From an update perspective the most important aspect is to keep the existing data storage of the Pods compatible with the new DB version that comes in a newer container. This basically relies on how PostgreSQL behaves between version updates and needs to be verified.","title":"DAM Persistence (PostgreSQL)"},{"location":"kube/k8s-next-update/#stateless-applications","text":"","title":"Stateless Applications"},{"location":"kube/k8s-next-update/#ringapi","text":"RingAPI is a stateless application that only uses configuration parameters and acts as a wrapping API server around DX Core. From an update perspective there are no special requirements that need to be taken into account. Updating RingAPI basically boils down to deploying a new version of the container image.","title":"RingAPI"},{"location":"kube/k8s-next-update/#content-composer","text":"Content Composer is a stateless application that only uses configuration parameters and acts as a webserver hosting the resources for the Content Composer UI. From an update perspective there are no special requirements that need to be taken into account. Updating Content Composer basically boils down to deploying a new version of the container image.","title":"Content Composer"},{"location":"kube/k8s-next-update/#image-processor","text":"Content Composer is a stateless application that only uses configuration parameters and acts as a media transformation service. It only uses HTTP as communication protocol and requires no persistence. From an update perspective there are no special requirements that need to be taken into account. Updating the Image Processor basically boils down to deploying a new version of the container image.","title":"Image Processor"},{"location":"kube/k8s-next-update/#configurations","text":"","title":"Configurations"},{"location":"kube/k8s-next-update/#configmaps","text":"For general configuration we use two types von ConfigMaps. The first one is an application specific ConfigMap which contains all configuration necessary for individual applications to run. Those ConfigMaps are only consumed by the related application. The second type of ConfigMap is a global deployment wide ConfigMap, that contains configuration that affects all applications. The values of these ConfigMap entries will be directly updated during the update process and applications will be informed about the changes.","title":"ConfigMaps"},{"location":"kube/k8s-next-update/#secrets","text":"For storing secure data like certificates and access credentials, we use Kubernetes Secrets. These secrets will be consumed by our applications. Content of those secrets will be directly updated during the update process and applications will be informed about the changes.","title":"Secrets"},{"location":"kube/k8s-next-update/#utility-resources","text":"Utility Resources e.g. HorizontalPodAutoscalers will be updated directly through the update process.","title":"Utility Resources"},{"location":"kube/k8s-next-update/#ambassador-deployment","text":"We need to replace Ambassador Resources if they have changes. This will happen directly through the update process.","title":"Ambassador deployment"},{"location":"kube/k8s-next-update/#updating","text":"","title":"Updating"},{"location":"kube/k8s-next-update/#backup","text":"Before updating, we need to provide customers with a way to create a backup of the important parts of the deployment. This includes persistent volumes and the data in the DBs of DX Core and DAM. This could be done by: A Kubernetes Job running, triggered by Helm or DXCTL The applications themselves before updating the schemas","title":"Backup"},{"location":"kube/k8s-next-update/#update-categories","text":"Updates we perform can have multiple implications. Based on the way they change existing deployments, we can categorize them in different update categories. Based on in which category an update fits in, different actions or handling might be necessary.","title":"Update Categories"},{"location":"kube/k8s-next-update/#a-application-changes","text":"As we implement features and bugfixes to our applications, the application runtime code will change. This can have implications to configuration and persistence schemas, but that is not always the case. Often enough, application logic will be altered, without affecting existing environmental resources.","title":"A - Application changes"},{"location":"kube/k8s-next-update/#c-configuration-changes","text":"As we perform development on our applications, it might be necessary to either introduce new configuration parameters or alter existing ones to reflect the current state of implementation. It can also happen that default behaviors/values might be adjusted between releases, e.g. if performance assessments have shown, that a Pod needs fewer resources and we lower the Kubernetes resource requests.","title":"C - Configuration changes"},{"location":"kube/k8s-next-update/#p-persistence-schema-changes","text":"For our stateful applications, such as DAM or DX Core, it may become necessary to perform changes to our persistence schemas. This would usually affect the schema of the DAM DB or the DX Core DB. This can happen if we need to introduce new fields into existing data structures.","title":"P - Persistence schema changes"},{"location":"kube/k8s-next-update/#n-new-application-introduction","text":"As we move on with our developments for DX, new applications will arise that will be deployed within a DX deployment. Those applications will require new configurations, Kubernetes resources and perhaps preparation. As an example the roll-out of site-manager would fall into this category.","title":"N - New application introduction"},{"location":"kube/k8s-next-update/#r-removalreplacement-of-an-application","text":"Similar to when we add applications, there may also be parts that will be removed because they perhaps got obsolete or are being replaced with other applications. In that case it is important to ensure that the old resources get cleaned up, the configuration, if necessary, gets removed and the DX deployment is made aware that the application is gone and is not having any dependency anymore.","title":"R - Removal/replacement of an application"},{"location":"kube/k8s-next-update/#scenario-update-only-containing-configuration-and-application-changes","text":"During this update the customer can either use helm or dxctl to deploy the new version of our DX deployment. With that, all resources affected will receive their respective changes.","title":"Scenario: Update only containing configuration and application changes"},{"location":"kube/k8s-next-update/#statefulsets","text":"Since we directly define the Statefulsets in our Helm chart, the update will cause them to receive the new specification to the kubernetes resource. Kubernetes itself will then start its reconciliation and will ensure that the Pods of the statefulset will be restarted with the new configuration. See Statefulset Rolling Update for reference. This logic is implemented in kubernetes itself and will roll out the changes to the application and will only continue updating the next Pod, if the first one has been restarted with the new configuration and got ready.","title":"Statefulsets"},{"location":"kube/k8s-next-update/#deploymentsreplicasets","text":"Applications that are stateless and run as a Deployment will be receiving the updated specification as well directly via the update process. As soon as the Deployment Resource gets altered, Kubernetes will perform a rollover, spawning new Pods with the new configuration. As soon as the new Pods are running successfully and are ready, the old Pods will be terminated. See Deployment Rollover Update for reference.","title":"Deployments/Replicasets"},{"location":"kube/k8s-next-update/#configmapssecrets","text":"ConfigMaps and Secrets will directly be updated via the update process.","title":"ConfigMaps/Secrets"},{"location":"kube/k8s-next-update/#utility-resource","text":"Utility Resources like HorizontalPodAutoscalers etc. will directly be updated via the update process.","title":"Utility Resource"},{"location":"kube/k8s-next-update/#scenario-update-also-including-persistence-schema-changes","text":"During this update the customer can either use helm or dxctl to deploy the new version of our DX deployment. With that, all resources affected will receive their respective changes.","title":"Scenario: Update also including persistence schema changes"},{"location":"kube/k8s-next-update/#statefulsets_1","text":"Since we directly define the StatefulSets in our Helm chart, the update will cause them to receive the new specification to the kubernetes resource. Kubernetes itself will then start its reconciliation and will ensure that the Pods of the StatefulSet will be restarted with the new configuration. See StatefulSet Rolling Update for reference. This logic is implemented in kubernetes itself and will roll out the changes to the application and will only continue updating the next Pod, if the first one has been restarted with the new configuration and got ready. As soon as schema changes are involved, it may become necessary that a preflight task needs to be run to e.g. to migrate a database schema before deploying new Deployments etc. We could leverage one of the following approaches to run necessary pre-flight tasks: Container integrated logic that checks if schema migration tasks are necessary to run Kubernetes Init Containers attached to the Pods, running specific tasks Kubernetes Jobs that run once and report back completion (could also run as a Helm Hook) While DAM will have automated logic to handle version to version migration of database schemas, for DX Core it might be necessary to perform schema updates that are triggered by the operators/administrators of the customer. Since we do not directly manage the DX Database ourselves, Database operations would preferably not done by our deployment directly, unless we are sure the logic we implemented in DX Core is failsafe.","title":"StatefulSets"},{"location":"kube/k8s-next-update/#deploymentsreplicasets_1","text":"Applications that are stateless and run as a Deployment will be receiving the updated specification as well directly via the update process. As soon as the Deployment Resource gets altered, Kubernetes will perform a rollover, spawning new Pods with the new configuration. As soon as the new Pods are running successfully and are ready, the old Pods will be terminated. See Deployment Rollover Update for reference.","title":"Deployments/Replicasets"},{"location":"kube/k8s-next-update/#configmapssecrets_1","text":"ConfigMaps and Secrets will directly be updated via the update process.","title":"ConfigMaps/Secrets"},{"location":"kube/k8s-next-update/#utility-resource_1","text":"Utility Resources like HorizontalPodAutoscalers etc. will directly be updated via the update process.","title":"Utility Resource"},{"location":"kube/k8s-next-update/#rollback","text":"Rolling back our application is mostly relying on the fact if our applications can roll back to older persistence schemas. For disaster recovery we would use the backups done before. Using those backups of persistent volumes and DBs would allow a customer to go back to a previous version of our deployment, assuming the performed a rollback to the old versions of the deployment e.g. using helm rollback .","title":"Rollback"},{"location":"kube/k8s-next-update/#tooling","text":"","title":"Tooling"},{"location":"kube/k8s-next-update/#helm","text":"Customers should use helm upgrade to perform updates on their existing deployments. Using helm for upgrades provides both us and our customers with the benefit of hierarchical configurations. Per default, a helm chart uses the values.yaml to determine what values should be used in the deployment. A helm chart is always delivered with that file, since it includes all the necessary default values for the deployment. For customers there might be customer specific configuration, e.g. scaling settings. We do not want customers to have the need to manually copy over their existing settings into our default values.yaml OR to copy the new values of the default values.yaml into their custom settings file. This can create merge conflicts, wrong configuration and makes the updating process more complicated for customers. With the hierarchical nature of helm configurations, we have the following pattern at hand: Especially important in our case is the use of our default values.yaml and a user defined custom_values.yaml . With every version of DX we ship, we will deliver a default values.yaml that contains all necessary information to get a DX Kube deployment up and running. This also includes image names, version tags etc. If a customer wants to customize the deployment, they could either edit our default values.yaml and apply the changes, or create their own file and only change the values that are of importance to them. The benefit is quiet obvious: Customers can keep customized deployment configuration in one file, while we can always ship a fitting set of defaults that does not overwrite the customers values. The lifecycle of that configuration can be seen in the following diagram: Customers can retain their own custom values file, while we always ship a new one with each release. With that, we don't override customer settings, but always deliver working default deployment values. Note: The custom values file approach can also be used to provide different deployment templates to our customers, without overwriting our default values.yaml","title":"Helm"},{"location":"kube/k8s-next-update/#dxctl","text":"DXTCL should ideally be able to consume outputs of the Helm templating engine and use that output to update existing deployments in a similar fashion as it does today already.","title":"DXCTL"},{"location":"kube/Migration/k8s-next-migrate-detail-core/","text":"This section will documented a overview how it is possible to migrate a core profile from a operator based deployment to a HELM based deployment. Also a output of this design is a first draft for the raw documentation. Migration-Mode for the HELM deployment Before we can start to document the backup and restore/migration of the core profile we should implemented a migration mode. The goal of this mode is to start the core pod but without starting DX. The Pod should stay alive without DX runninng, so it is possible to connect to the pod and perform actions on the file system. For the migration mode need to should adjust the following areas: Adding a migration parameter to the HELM chart Liveness, readiness, startUp - checks Core start scripts Here are some example implementations on how to do that, as I have tested this in feature branches. https://git.cwp.pnp-hcl.com/websphere-portal-incubator/dx-helm-charts/tree/feature/DXQ-19119 https://git.cwp.pnp-hcl.com/Team-Q/Portal-Docker-Images/compare/feature/DXQ-19119 The migration mode should be also interested for DAM. Here are the needed changes. HELM charts add config map entry \"dam.config.dam.migration\": \"{{ .Values.migration.enabled }}\" # Liveness probe {{- if .Values.migration.enabled }} livenessProbe: exec: command: - /bin/sh - -c - exit 0 failureThreshold: {{ .failureThreshold }} initialDelaySeconds: {{ .initialDelaySeconds }} periodSeconds: {{ .periodSeconds }} successThreshold: {{ .successThreshold }} timeoutSeconds: {{ .timeoutSeconds }} {{ else }} livenessProbe: {{- with .Values.probes.digitalAssetManagement.livenessProbe }} {{- toYaml . | nindent 12 }} {{- end }} {{- end }} # Readiness probe {{- if .Values.migration.enabled }} readinessProbe: exec: command: - /bin/sh - -c - exit 0 failureThreshold: {{ .failureThreshold }} initialDelaySeconds: {{ .initialDelaySeconds }} periodSeconds: {{ .periodSeconds }} successThreshold: {{ .successThreshold }} timeoutSeconds: {{ .timeoutSeconds }} {{ else }} readinessProbe: {{- with .Values.probes.digitalAssetManagement.readinessProbe }} {{- toYaml . | nindent 12 }} {{- end }} {{- end }} Start scripts: #!/bin/bash # Uses environment variables from a config map if mounted for file in /etc/config/dam.config.dam.*; do # Loop through all the files in directory and check if file exists. if test -f \"$file\"; then # Split the filename into array with dot(.) delimiter. IFS='.' read -r -a array <<< \"$(basename \"$file\")\" # Get the last element of array for eg. postgres_db. envVariableTemp=${array[@]: -1:1} # Capitilise the string. envVariable=$(tr '[a-z]' '[A-Z]' <<< $envVariableTemp) fileName=\"${file##*/}\" # Assign the variables with contents of file. eg. POSTGRES_DB='testdb'. printf -v $envVariable $(cat /etc/config/$fileName) # export the variable as environment variable. export $envVariable fi done clean_up () { echo \"Sending SIGTERM to all node instances...\" cd /opt/app/server-v1 && npm run docker:stop-daemon } stop_container () { echo \"Caught SIGTERM, stopping DX Core container\" exit 0 } echo \"Migration is: $MIGRATION\" if [ \"$MIGRATION\" == \"true\" ]; then trap stop_container SIGTERM echo \"Listening for SIGTERM\" echo \"Migration is enabled, stay alive and do nothing ...\" # ensure the docker container stays alive and can listen for signals every second while true; do sleep 1; done else trap clean_up SIGHUP SIGINT SIGTERM # run server version 1 cd /opt/app/server-v1 && npm run docker:start fi Backup and restore process (raw doc) This section documents the migration process of the core profile. Backup form the operator based deployment 1. Ensure that only one core pod is running Check how many pods are running. Use the following command for that. kubectl -n <namespace> get all If more then one core pod running, scaling down the core pods to only one. On the operator deployment adjust the DXCTL property file and apply this changes via the DXCTL tool. 2. Connecting to the core pod With the following command it is possible to jump directly into the running container. kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-0 -n dxns -- /bin/bash 3. Stop the server Before a backup from the wp-profile can be create, the core application should be stopped. Navigate to the profile bin folder and run the stopServer command. cd /opt/HCL/wp_profile/bin/ ./stopServer.sh WebSphere_Portal -username <username> -password <password> 4. Compress the profile For the operator-based deployment is the whole core profile located under /opt/HCL/wp_profile . This folder must be compressed. cd /opt/HCL tar -cvpzf core_prof_95_CF197.tar.gz --exclude=/core_prof_95_CF197.tar.gz --one-file-system wp_profile 5. Download the backup profile From a local system it is now possible to download the backup core profile from the core pod container. kubectl cp <namespace>/<pod-name>:<source-file> <target-file> Example: kubectl cp dxns/dx-deployment-0:opt/HCL/core_prof_95_CF197.tar.gz /tmp/core_prof_95_CF197.tar.gz Restore to the HELM based deployment 1. Start the HELM deployment Before we can start with the restore we should ensure that the HELM based deployment is in correct state. Some adjustments are necessary. The extraction of kubernetes DX configuration from the operator based deployment to a valid values.yaml file is done. Enable the migration mode. For the first start the runtimeController and the core application should be enabled. Now the HELM deployment can be started. helm install -n <namespace> --create-namespace -f <values.yaml> <prefix> <chart> Example: helm install -n dxns --create-namespace -f hcl-dx-deployment/value-samples/internal-repos.yaml hha hcl-dx-deployment What we expected now is: The core pod is running and keep alive. The core application is NOT running. No default profile should be created automatically. The folder /opt/HCL/wp_profile is empty. 2. Upload the backup profile Now it is possible to transfer the backup profile to the remote core pod container. kubectl cp <source-file> <namespace>/<pod-name>:<target-file> Example: kubectl cp /tmp/core_prof_95_CF197.tar.gz dxns/hha-core-0:/tmp/core_prof_95_CF197.tar.gz 3. Connect to the core pod With the following command it is possible to connect directly into the running container. kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/hha-core-0 -n dxns -- /bin/bash 4. Extracting the profile At first the backup file core_prof_95_CF197.tar.gz should moved from /tmp folder to the profile folder /opt/HCL/profiles . And the extraction can be started. tar -xf /tmp/core_prof_95_CF197.tar.gz --directory /opt/HCL/profiles mv /opt/HCL/profiles/wp_profile /opt/HCL/profiles/prof_95_CF197 rm /tmp/core_prof_95_CF197.tar.gz The next step is to create a symlink. rm -r /opt/HCL/wp_profile ln -s /opt/HCL/profiles/prof_95_CF197 /opt/HCL/wp_profile 5. Disable the migration mode and the deployment Before we can start with the final upgrad of the HELM deployment some adjustments are necessary. Disable the migration mode. Enable all relevant applications. Now the HELM deployment can be upgraded. helm upgrade -n <namespace> --create-namespace -f <values.yaml> <prefix> <chart> Example: helm upgrade -n dxns --create-namespace -f hcl-dx-deployment/value-samples/internal-repos.yaml hha hcl-dx-deployment","title":"Operator migration - core"},{"location":"kube/Migration/k8s-next-migrate-detail-core/#migration-mode-for-the-helm-deployment","text":"Before we can start to document the backup and restore/migration of the core profile we should implemented a migration mode. The goal of this mode is to start the core pod but without starting DX. The Pod should stay alive without DX runninng, so it is possible to connect to the pod and perform actions on the file system. For the migration mode need to should adjust the following areas: Adding a migration parameter to the HELM chart Liveness, readiness, startUp - checks Core start scripts Here are some example implementations on how to do that, as I have tested this in feature branches. https://git.cwp.pnp-hcl.com/websphere-portal-incubator/dx-helm-charts/tree/feature/DXQ-19119 https://git.cwp.pnp-hcl.com/Team-Q/Portal-Docker-Images/compare/feature/DXQ-19119 The migration mode should be also interested for DAM. Here are the needed changes. HELM charts add config map entry \"dam.config.dam.migration\": \"{{ .Values.migration.enabled }}\" # Liveness probe {{- if .Values.migration.enabled }} livenessProbe: exec: command: - /bin/sh - -c - exit 0 failureThreshold: {{ .failureThreshold }} initialDelaySeconds: {{ .initialDelaySeconds }} periodSeconds: {{ .periodSeconds }} successThreshold: {{ .successThreshold }} timeoutSeconds: {{ .timeoutSeconds }} {{ else }} livenessProbe: {{- with .Values.probes.digitalAssetManagement.livenessProbe }} {{- toYaml . | nindent 12 }} {{- end }} {{- end }} # Readiness probe {{- if .Values.migration.enabled }} readinessProbe: exec: command: - /bin/sh - -c - exit 0 failureThreshold: {{ .failureThreshold }} initialDelaySeconds: {{ .initialDelaySeconds }} periodSeconds: {{ .periodSeconds }} successThreshold: {{ .successThreshold }} timeoutSeconds: {{ .timeoutSeconds }} {{ else }} readinessProbe: {{- with .Values.probes.digitalAssetManagement.readinessProbe }} {{- toYaml . | nindent 12 }} {{- end }} {{- end }} Start scripts: #!/bin/bash # Uses environment variables from a config map if mounted for file in /etc/config/dam.config.dam.*; do # Loop through all the files in directory and check if file exists. if test -f \"$file\"; then # Split the filename into array with dot(.) delimiter. IFS='.' read -r -a array <<< \"$(basename \"$file\")\" # Get the last element of array for eg. postgres_db. envVariableTemp=${array[@]: -1:1} # Capitilise the string. envVariable=$(tr '[a-z]' '[A-Z]' <<< $envVariableTemp) fileName=\"${file##*/}\" # Assign the variables with contents of file. eg. POSTGRES_DB='testdb'. printf -v $envVariable $(cat /etc/config/$fileName) # export the variable as environment variable. export $envVariable fi done clean_up () { echo \"Sending SIGTERM to all node instances...\" cd /opt/app/server-v1 && npm run docker:stop-daemon } stop_container () { echo \"Caught SIGTERM, stopping DX Core container\" exit 0 } echo \"Migration is: $MIGRATION\" if [ \"$MIGRATION\" == \"true\" ]; then trap stop_container SIGTERM echo \"Listening for SIGTERM\" echo \"Migration is enabled, stay alive and do nothing ...\" # ensure the docker container stays alive and can listen for signals every second while true; do sleep 1; done else trap clean_up SIGHUP SIGINT SIGTERM # run server version 1 cd /opt/app/server-v1 && npm run docker:start fi","title":"Migration-Mode for the HELM deployment"},{"location":"kube/Migration/k8s-next-migrate-detail-core/#backup-and-restore-process-raw-doc","text":"This section documents the migration process of the core profile.","title":"Backup and restore process (raw doc)"},{"location":"kube/Migration/k8s-next-migrate-detail-core/#backup-form-the-operator-based-deployment","text":"","title":"Backup form the operator based deployment"},{"location":"kube/Migration/k8s-next-migrate-detail-core/#1-ensure-that-only-one-core-pod-is-running","text":"Check how many pods are running. Use the following command for that. kubectl -n <namespace> get all If more then one core pod running, scaling down the core pods to only one. On the operator deployment adjust the DXCTL property file and apply this changes via the DXCTL tool.","title":"1. Ensure that only one core pod is running"},{"location":"kube/Migration/k8s-next-migrate-detail-core/#2-connecting-to-the-core-pod","text":"With the following command it is possible to jump directly into the running container. kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-0 -n dxns -- /bin/bash","title":"2. Connecting to the core pod"},{"location":"kube/Migration/k8s-next-migrate-detail-core/#3-stop-the-server","text":"Before a backup from the wp-profile can be create, the core application should be stopped. Navigate to the profile bin folder and run the stopServer command. cd /opt/HCL/wp_profile/bin/ ./stopServer.sh WebSphere_Portal -username <username> -password <password>","title":"3. Stop the server"},{"location":"kube/Migration/k8s-next-migrate-detail-core/#4-compress-the-profile","text":"For the operator-based deployment is the whole core profile located under /opt/HCL/wp_profile . This folder must be compressed. cd /opt/HCL tar -cvpzf core_prof_95_CF197.tar.gz --exclude=/core_prof_95_CF197.tar.gz --one-file-system wp_profile","title":"4. Compress the profile"},{"location":"kube/Migration/k8s-next-migrate-detail-core/#5-download-the-backup-profile","text":"From a local system it is now possible to download the backup core profile from the core pod container. kubectl cp <namespace>/<pod-name>:<source-file> <target-file> Example: kubectl cp dxns/dx-deployment-0:opt/HCL/core_prof_95_CF197.tar.gz /tmp/core_prof_95_CF197.tar.gz","title":"5. Download the backup profile"},{"location":"kube/Migration/k8s-next-migrate-detail-core/#restore-to-the-helm-based-deployment","text":"","title":"Restore to the HELM based deployment"},{"location":"kube/Migration/k8s-next-migrate-detail-core/#1-start-the-helm-deployment","text":"Before we can start with the restore we should ensure that the HELM based deployment is in correct state. Some adjustments are necessary. The extraction of kubernetes DX configuration from the operator based deployment to a valid values.yaml file is done. Enable the migration mode. For the first start the runtimeController and the core application should be enabled. Now the HELM deployment can be started. helm install -n <namespace> --create-namespace -f <values.yaml> <prefix> <chart> Example: helm install -n dxns --create-namespace -f hcl-dx-deployment/value-samples/internal-repos.yaml hha hcl-dx-deployment What we expected now is: The core pod is running and keep alive. The core application is NOT running. No default profile should be created automatically. The folder /opt/HCL/wp_profile is empty.","title":"1. Start the HELM deployment"},{"location":"kube/Migration/k8s-next-migrate-detail-core/#2-upload-the-backup-profile","text":"Now it is possible to transfer the backup profile to the remote core pod container. kubectl cp <source-file> <namespace>/<pod-name>:<target-file> Example: kubectl cp /tmp/core_prof_95_CF197.tar.gz dxns/hha-core-0:/tmp/core_prof_95_CF197.tar.gz","title":"2. Upload the backup profile"},{"location":"kube/Migration/k8s-next-migrate-detail-core/#3-connect-to-the-core-pod","text":"With the following command it is possible to connect directly into the running container. kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/hha-core-0 -n dxns -- /bin/bash","title":"3. Connect to the core pod"},{"location":"kube/Migration/k8s-next-migrate-detail-core/#4-extracting-the-profile","text":"At first the backup file core_prof_95_CF197.tar.gz should moved from /tmp folder to the profile folder /opt/HCL/profiles . And the extraction can be started. tar -xf /tmp/core_prof_95_CF197.tar.gz --directory /opt/HCL/profiles mv /opt/HCL/profiles/wp_profile /opt/HCL/profiles/prof_95_CF197 rm /tmp/core_prof_95_CF197.tar.gz The next step is to create a symlink. rm -r /opt/HCL/wp_profile ln -s /opt/HCL/profiles/prof_95_CF197 /opt/HCL/wp_profile","title":"4. Extracting the profile"},{"location":"kube/Migration/k8s-next-migrate-detail-core/#5-disable-the-migration-mode-and-the-deployment","text":"Before we can start with the final upgrad of the HELM deployment some adjustments are necessary. Disable the migration mode. Enable all relevant applications. Now the HELM deployment can be upgraded. helm upgrade -n <namespace> --create-namespace -f <values.yaml> <prefix> <chart> Example: helm upgrade -n dxns --create-namespace -f hcl-dx-deployment/value-samples/internal-repos.yaml hha hcl-dx-deployment","title":"5. Disable the migration mode and the deployment"},{"location":"kube/Migration/k8s-next-migrate-detail-dam/","text":"This section will documented a overview how it is possible to migrate a dam persistence and binaries from a operator based deployment to a HELM based deployment. Also a output of this design is a first draft for the raw documentation. This document may change after DBHA implementation. Migration-Mode for the HELM deployment Before we can start to document the backup and restore of the dam persistence and binaries we should implemented a migration mode. The details of implementing migration mode will be available here . Backup and restore process This section documents the backup and restore process of the dam persistence and binaries. Backup form the operator based deployment 1. Ensure persistence(read write) and dam pod are running The following command will help to find pods current status. kubectl -n <namespace> get all Ensure pods are up and running. If more than one dam and persistence pods are running, then scale down the pods to one. 2. Database backup from persistence(read write) a. Connect to persistence pod. The following command is used to connect with persistence container. kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-persistence-0 -n dx-ns -- /bin/bash b. Dump the current database using pg_dump. [dx_user@dx-deployment-persistence-0 /]$ pg_dump dxmediadb > /tmp/dxmediadb.dmp c. Download the dumped database to local system by using the following command. kubectl cp <namespace>/<pod-name>:<source-file> <target-file> Example kubectl cp dx-ns/dx-deployment-persistence-0:/tmp/dxmediadb.dmp /backup/dxmediadb.dmp 3. DAM binaries and profile backup a. Connect to DAM pod. The following command is used to connect with container. kubectl exec --stdin --tty pod/dx-deployment-dam-0 -n dx-ns -- /bin/bash b. Compress the DAM binaries which are located under /opt/app/upload directory. tar -cvpzf backupml.tar.gz --exclude=/backupml.tar.gz --one-file-system /opt/app/upload c. Compress the DAM profiles which are located under /etc/config directory. tar -cvpzf backupmlcfg.tar.gz --exclude=/backupmlcfg.tar.gz --one-file-system /etc/config d. Download the compressed binaries and profiles to the local system. kubectl cp dx-ns/dx-deployment-dam-0:/opt/app/server-v1/backupml.tar.gz /backup/backupml.tar.gz kubectl cp dx-ns/dx-deployment-dam-0:/opt/app/server-v1/backupmlcfg.tar.gz /backup/backupmlcfg.tar.gz Restore to the HELM based deployment 1. Start the HELM deployment Before we can start with the restore we must ensure that the HELM based deployment is in correct state. Some adjustments are necessary. The extraction of kubernetes DX configuration from the operator based deployment to a valid values.yaml file is done. Enable the migration mode. Now the HELM deployment can be started. helm install -n <namespace> --create-namespace -f <values.yaml> <prefix> <chart> Example: helm install -n dxns-helm --create-namespace -f hcl-dx-deployment/value-sample.yaml dx-deployment hcl-dx-deployment At the moment, the dam and persistence pods are alive. But application is not ready since migration mode is enabled. 2. Restore the database a. Upload the backup database to persistence pod. kubectl cp /backup/dxmediadb.dmp dxns-helm/pod/dx-deployment-persistence-rw-0:/tmp/dxmediadb.dmp b. Connect to persistence read write pod and perform the restore process. kubectl exec --stdin --tty pod/dx-deployment-persistence-rw-0 -n dxns-helm -- /bin/bash [dx_user@dx-deployment-persistence-rw-0 tmp]$ dropdb dxmediadb [dx_user@dx-deployment-persistence-rw-0 tmp]$ createdb -O dxuser dxmediadb [dx_user@dx-deployment-persistence-rw-0 tmp]$ psql dxmediadb < dxmediadb.dmp 3. Restore the binaries and profiles a. Upload the backup binary and profiles to dam pod. kubectl cp /backup/backupml.tar.gz dxns-helm/pod/dx-deployment-digital-asset-management-0:/opt/app/server-v1/backupml.tar.gz kubectl cp /backup/backupmlcfg.tar.gz dxns-helm/pod/dx-deployment-digital-asset-management-0:/opt/app/server-v1/backupmlcfg.tar.gz b. Connect to dam pod and perform the restore process. kubectl exec --stdin --tty pod/dx-deployment-digital-asset-management-0 -n dxns-helm -- /bin/bash [dx_user@dx-deployment-digital-asset-management-0 server-v1]$ tar -xf /backupml.tar.gz --directory /opt/app/upload [dx_user@dx-deployment-digital-asset-management-0 server-v1]$ tar -xf /backupmlcfg.tar.gz --directory /etc/config [dx_user@dx-deployment-digital-asset-management-0 server-v1]$ rm /backupml.tar.gz [dx_user@dx-deployment-digital-asset-management-0 server-v1]$ rm /backupmlcfg.tar.gz 4. Disable the migration mode and the deployment Before we can start with the final upgrade of the HELM deployment some adjustments are necessary. Disable the migration mode. Enable all relevant applications. Now the HELM deployment can be upgraded. helm upgrade -n <namespace> --create-namespace -f <values.yaml> <prefix> <chart> Example: helm upgrade -n dxns-helm --create-namespace -f hcl-dx-deployment/value-sample.yaml dx-deployment hcl-dx-deployment","title":"Operator migration - DAM"},{"location":"kube/Migration/k8s-next-migrate-detail-dam/#migration-mode-for-the-helm-deployment","text":"Before we can start to document the backup and restore of the dam persistence and binaries we should implemented a migration mode. The details of implementing migration mode will be available here .","title":"Migration-Mode for the HELM deployment"},{"location":"kube/Migration/k8s-next-migrate-detail-dam/#backup-and-restore-process","text":"This section documents the backup and restore process of the dam persistence and binaries.","title":"Backup and restore process"},{"location":"kube/Migration/k8s-next-migrate-detail-dam/#backup-form-the-operator-based-deployment","text":"","title":"Backup form the operator based deployment"},{"location":"kube/Migration/k8s-next-migrate-detail-dam/#1-ensure-persistenceread-write-and-dam-pod-are-running","text":"The following command will help to find pods current status. kubectl -n <namespace> get all Ensure pods are up and running. If more than one dam and persistence pods are running, then scale down the pods to one.","title":"1. Ensure persistence(read write) and dam pod are running"},{"location":"kube/Migration/k8s-next-migrate-detail-dam/#2-database-backup-from-persistenceread-write","text":"a. Connect to persistence pod. The following command is used to connect with persistence container. kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-persistence-0 -n dx-ns -- /bin/bash b. Dump the current database using pg_dump. [dx_user@dx-deployment-persistence-0 /]$ pg_dump dxmediadb > /tmp/dxmediadb.dmp c. Download the dumped database to local system by using the following command. kubectl cp <namespace>/<pod-name>:<source-file> <target-file> Example kubectl cp dx-ns/dx-deployment-persistence-0:/tmp/dxmediadb.dmp /backup/dxmediadb.dmp","title":"2. Database backup from persistence(read write)"},{"location":"kube/Migration/k8s-next-migrate-detail-dam/#3-dam-binaries-and-profile-backup","text":"a. Connect to DAM pod. The following command is used to connect with container. kubectl exec --stdin --tty pod/dx-deployment-dam-0 -n dx-ns -- /bin/bash b. Compress the DAM binaries which are located under /opt/app/upload directory. tar -cvpzf backupml.tar.gz --exclude=/backupml.tar.gz --one-file-system /opt/app/upload c. Compress the DAM profiles which are located under /etc/config directory. tar -cvpzf backupmlcfg.tar.gz --exclude=/backupmlcfg.tar.gz --one-file-system /etc/config d. Download the compressed binaries and profiles to the local system. kubectl cp dx-ns/dx-deployment-dam-0:/opt/app/server-v1/backupml.tar.gz /backup/backupml.tar.gz kubectl cp dx-ns/dx-deployment-dam-0:/opt/app/server-v1/backupmlcfg.tar.gz /backup/backupmlcfg.tar.gz","title":"3. DAM binaries and profile backup"},{"location":"kube/Migration/k8s-next-migrate-detail-dam/#restore-to-the-helm-based-deployment","text":"","title":"Restore to the HELM based deployment"},{"location":"kube/Migration/k8s-next-migrate-detail-dam/#1-start-the-helm-deployment","text":"Before we can start with the restore we must ensure that the HELM based deployment is in correct state. Some adjustments are necessary. The extraction of kubernetes DX configuration from the operator based deployment to a valid values.yaml file is done. Enable the migration mode. Now the HELM deployment can be started. helm install -n <namespace> --create-namespace -f <values.yaml> <prefix> <chart> Example: helm install -n dxns-helm --create-namespace -f hcl-dx-deployment/value-sample.yaml dx-deployment hcl-dx-deployment At the moment, the dam and persistence pods are alive. But application is not ready since migration mode is enabled.","title":"1. Start the HELM deployment"},{"location":"kube/Migration/k8s-next-migrate-detail-dam/#2-restore-the-database","text":"a. Upload the backup database to persistence pod. kubectl cp /backup/dxmediadb.dmp dxns-helm/pod/dx-deployment-persistence-rw-0:/tmp/dxmediadb.dmp b. Connect to persistence read write pod and perform the restore process. kubectl exec --stdin --tty pod/dx-deployment-persistence-rw-0 -n dxns-helm -- /bin/bash [dx_user@dx-deployment-persistence-rw-0 tmp]$ dropdb dxmediadb [dx_user@dx-deployment-persistence-rw-0 tmp]$ createdb -O dxuser dxmediadb [dx_user@dx-deployment-persistence-rw-0 tmp]$ psql dxmediadb < dxmediadb.dmp","title":"2. Restore the database"},{"location":"kube/Migration/k8s-next-migrate-detail-dam/#3-restore-the-binaries-and-profiles","text":"a. Upload the backup binary and profiles to dam pod. kubectl cp /backup/backupml.tar.gz dxns-helm/pod/dx-deployment-digital-asset-management-0:/opt/app/server-v1/backupml.tar.gz kubectl cp /backup/backupmlcfg.tar.gz dxns-helm/pod/dx-deployment-digital-asset-management-0:/opt/app/server-v1/backupmlcfg.tar.gz b. Connect to dam pod and perform the restore process. kubectl exec --stdin --tty pod/dx-deployment-digital-asset-management-0 -n dxns-helm -- /bin/bash [dx_user@dx-deployment-digital-asset-management-0 server-v1]$ tar -xf /backupml.tar.gz --directory /opt/app/upload [dx_user@dx-deployment-digital-asset-management-0 server-v1]$ tar -xf /backupmlcfg.tar.gz --directory /etc/config [dx_user@dx-deployment-digital-asset-management-0 server-v1]$ rm /backupml.tar.gz [dx_user@dx-deployment-digital-asset-management-0 server-v1]$ rm /backupmlcfg.tar.gz","title":"3. Restore the binaries and profiles"},{"location":"kube/Migration/k8s-next-migrate-detail-dam/#4-disable-the-migration-mode-and-the-deployment","text":"Before we can start with the final upgrade of the HELM deployment some adjustments are necessary. Disable the migration mode. Enable all relevant applications. Now the HELM deployment can be upgraded. helm upgrade -n <namespace> --create-namespace -f <values.yaml> <prefix> <chart> Example: helm upgrade -n dxns-helm --create-namespace -f hcl-dx-deployment/value-sample.yaml dx-deployment hcl-dx-deployment","title":"4. Disable the migration mode and the deployment"},{"location":"kube/Migration/k8s-next-migrate-from-operator/","text":"Requirements A proper installed Operator deployment is existing The installed DX release version should be the same for the operator and HELM deployment to minimize conflicts Use the same database for the operator and the HELM deployment to also minimize conflicts Idea of migration path The migration will be separated in three major steps. Extracting kube config's and create a HELM extracted-values.yaml Deployment of a new DX instance via HELM charts and the extracted-values.yaml DX component migration Extracting kube config's and create the helm extracted-values.yaml Extracting configuration data from the Operator deployment Extracting related configuration values from Operator deployment to adjust the values of the helm deployment. The following parameter may be important to look at if you manually adjusted these. default.repository image and tags pullpolicy which applications are enabled cors configuration hybrid configuration request/limit - cpu and memory minreplicas, maxreplicas, targetcpuutilizationpercent, targetmemoryutilizationpercent persistent volume configuration (storageclass, ...) custom resource labels node selector configuration (for DX Core) The extracted values may need to be transfered it into a custom values.yaml helm file. Extracting the values from the dxctl property file Via this approach we should only analyse the DXCTL property file and extract the config values from there. This extracted config values can be use to create a custom values.yaml file. Deployment a new DX instance via HELM charts and the extracted-values.yaml Using the extracted-values.yaml from the operator deployment to get an adjusted HELM deployment. Run the HELM install CMD. For example: helm install -n <namespace> -f ./extracted-values.yaml <suffix> hcl-dx-deployment DX component migration The following components we need to migrate from the operator to the HELM deployment Core: Custom Themes Custom Portlets Pages WCM content/libs DAM assets Personalization rules (PZN) Credential Vault Virtual Portals Customer extensions: Shared libs Resource Environment Providers Some special WAS configs Extensions like preprocessor, filters, ... ... DAM: Collections, Items, Metadata Assets (binaries) We have checked three approaches to migrate the DX Core components and customer extensions. Using the dxclient tool Creating and deploying the initial release - HCL public documentation Profile migration To migrate the DAM data, we have have two options. Using the EXIM tool Using a manual backup and recover path DX Core and customer extensions migration dxclient The dxclient tool should be a good option to do that. For the most DX core components provided the tool a export and import functionality. But some components are missing also it is a problem to cover all the customer stuff, like some extensions, preprocessors, filters and what ever. Besides of that, the export can also be taking really long time, if the customer has create many pages or WCM data. Creating and deploying the initial release The initial release approach is creating a PAA application from some of the most DX core components, but also not for all. Also here the export and import can be taking really long time, if the customer has create many pages or WCM data. At the end this approach has not worked. The export process was successful but the import has some issues. I haven open a bug ( DXQ-18719 ) for that. Profile migration The profile migration was a idea from Thomas, which it looks like is a really good approach to do that. How it works this? Transferring the DB data - it is only relevant if the customer decide to use a new DB instance Stop the core package from HELM deployment Transfer the 'wp-profile' data from the operator deployment to the helm deployment Changed the database via the WAS console - it is only relevant if the customer decide to use a new DB instance Also we should fixed all hardcoded absolute URLs (like the URLs for the DAM assets) DX DAM migration EXIM Using the EXIM tool. The tool helps us to exports and imports all metadata and binaries. But it has some missing functionality and bugs, which are needs to solved it at first. I have open some stories and bugs to solve this. DXQ-18810 - EXIM are using always ports, but the kube deployment are not exporting some ports DXQ-18812 - EXIM has also a problem with the connection agains the Kube deployment. DXQ-18813 - The import from binaries are not working as expected. DXQ-18814 - The import from metadata are not working as expected, the reason is that the DAM collections access rights are not transferred. Workaround to solve it: Before we can import the metadata, we should transfer manually the binaries from the operator deployment to the HELM deployment. The binaries are located in the DAM persistent volume under the dx-dam-media folder. Also we should transfer manually all DAM collections resources permissions. Via XMLAccess we can extract all DAM collections resources permissions. For this export we should use the /opt/HCL/PortalServer/doc/xml-samples/ExportAllDamCollections.xml file to do that. And the generated XML output should be used to import it on the HELM deployment side. If all DAM data's are transferred successful, we need to adjust the stored references to WCM assets in WCM. The references are stored as fully qualified URLs. For this problem exists solution to set a alternate hostname. WCM DAM alternate hostname Backup and recover The following documentation provided a manual backup and recovery way. But it looks that the documentation is not in a good shape. https://help.hcltechsw.com/digital-experience/9.5/containerization/backup_and_recovery_procedures.html https://support.hcltechsw.com/community?id=community_question&sys_id=9e73a02e1b20b850f37655352a4bcbaa Summary Migration from operator-based to helm-based deployments will only be possible with exactly one DX version (presumable CF198). Both deployments have to be at the same version level. Moving Kubernetes configurations : We will only provide documentation on how to extract configurations from an existing operator-based deployment using dxctl and how to interpret these. We will also document how a customer would be able to reflect these in their custom values.yaml for their helm-based deployment. For the DX Core migration the best approach is to copy the wp_profile from the operator-based deployment to the helm-based deployment. We expect other connected infrastructures (e.g. LDAP) to remain unchanged, hence no configuration changes are necessary. DX Core database should just stay the same, hence no further changes would be necessary after copying the WAS profile. The old database needs to be reachable from the new helm-based deployment. If a customer needs to change the connected database, he can use database transfer tooling to move database content between the database instances. For DX DAM the DAM EXIM tool would be the preferred way. Unfortunately the EXIM implementation is not finished yet. Therefore the DAM move between an operator-based and a helm-based deployment will need to be done using Postgres database export/import and a copy of the DAM's binary storage. If EXIM gets finished in time, we can consider changing the documentation appropriately.","title":"Operator migration"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#requirements","text":"A proper installed Operator deployment is existing The installed DX release version should be the same for the operator and HELM deployment to minimize conflicts Use the same database for the operator and the HELM deployment to also minimize conflicts","title":"Requirements"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#idea-of-migration-path","text":"The migration will be separated in three major steps. Extracting kube config's and create a HELM extracted-values.yaml Deployment of a new DX instance via HELM charts and the extracted-values.yaml DX component migration","title":"Idea of migration path"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#extracting-kube-configs-and-create-the-helm-extracted-valuesyaml","text":"","title":"Extracting kube config's and create the helm extracted-values.yaml"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#extracting-configuration-data-from-the-operator-deployment","text":"Extracting related configuration values from Operator deployment to adjust the values of the helm deployment. The following parameter may be important to look at if you manually adjusted these. default.repository image and tags pullpolicy which applications are enabled cors configuration hybrid configuration request/limit - cpu and memory minreplicas, maxreplicas, targetcpuutilizationpercent, targetmemoryutilizationpercent persistent volume configuration (storageclass, ...) custom resource labels node selector configuration (for DX Core) The extracted values may need to be transfered it into a custom values.yaml helm file.","title":"Extracting configuration data from the Operator deployment"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#extracting-the-values-from-the-dxctl-property-file","text":"Via this approach we should only analyse the DXCTL property file and extract the config values from there. This extracted config values can be use to create a custom values.yaml file.","title":"Extracting the values from the dxctl property file"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#deployment-a-new-dx-instance-via-helm-charts-and-the-extracted-valuesyaml","text":"Using the extracted-values.yaml from the operator deployment to get an adjusted HELM deployment. Run the HELM install CMD. For example: helm install -n <namespace> -f ./extracted-values.yaml <suffix> hcl-dx-deployment","title":"Deployment a new DX instance via HELM charts and the extracted-values.yaml"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#dx-component-migration","text":"The following components we need to migrate from the operator to the HELM deployment","title":"DX component migration"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#core","text":"Custom Themes Custom Portlets Pages WCM content/libs DAM assets Personalization rules (PZN) Credential Vault Virtual Portals","title":"Core:"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#customer-extensions","text":"Shared libs Resource Environment Providers Some special WAS configs Extensions like preprocessor, filters, ... ...","title":"Customer extensions:"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#dam","text":"Collections, Items, Metadata Assets (binaries) We have checked three approaches to migrate the DX Core components and customer extensions. Using the dxclient tool Creating and deploying the initial release - HCL public documentation Profile migration To migrate the DAM data, we have have two options. Using the EXIM tool Using a manual backup and recover path","title":"DAM:"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#dx-core-and-customer-extensions-migration","text":"","title":"DX Core and customer extensions migration"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#dxclient","text":"The dxclient tool should be a good option to do that. For the most DX core components provided the tool a export and import functionality. But some components are missing also it is a problem to cover all the customer stuff, like some extensions, preprocessors, filters and what ever. Besides of that, the export can also be taking really long time, if the customer has create many pages or WCM data.","title":"dxclient"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#creating-and-deploying-the-initial-release","text":"The initial release approach is creating a PAA application from some of the most DX core components, but also not for all. Also here the export and import can be taking really long time, if the customer has create many pages or WCM data. At the end this approach has not worked. The export process was successful but the import has some issues. I haven open a bug ( DXQ-18719 ) for that.","title":"Creating and deploying the initial release"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#profile-migration","text":"The profile migration was a idea from Thomas, which it looks like is a really good approach to do that. How it works this? Transferring the DB data - it is only relevant if the customer decide to use a new DB instance Stop the core package from HELM deployment Transfer the 'wp-profile' data from the operator deployment to the helm deployment Changed the database via the WAS console - it is only relevant if the customer decide to use a new DB instance Also we should fixed all hardcoded absolute URLs (like the URLs for the DAM assets)","title":"Profile migration"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#dx-dam-migration","text":"","title":"DX DAM migration"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#exim","text":"Using the EXIM tool. The tool helps us to exports and imports all metadata and binaries. But it has some missing functionality and bugs, which are needs to solved it at first. I have open some stories and bugs to solve this. DXQ-18810 - EXIM are using always ports, but the kube deployment are not exporting some ports DXQ-18812 - EXIM has also a problem with the connection agains the Kube deployment. DXQ-18813 - The import from binaries are not working as expected. DXQ-18814 - The import from metadata are not working as expected, the reason is that the DAM collections access rights are not transferred.","title":"EXIM"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#workaround-to-solve-it","text":"Before we can import the metadata, we should transfer manually the binaries from the operator deployment to the HELM deployment. The binaries are located in the DAM persistent volume under the dx-dam-media folder. Also we should transfer manually all DAM collections resources permissions. Via XMLAccess we can extract all DAM collections resources permissions. For this export we should use the /opt/HCL/PortalServer/doc/xml-samples/ExportAllDamCollections.xml file to do that. And the generated XML output should be used to import it on the HELM deployment side. If all DAM data's are transferred successful, we need to adjust the stored references to WCM assets in WCM. The references are stored as fully qualified URLs. For this problem exists solution to set a alternate hostname. WCM DAM alternate hostname","title":"Workaround to solve it:"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#backup-and-recover","text":"The following documentation provided a manual backup and recovery way. But it looks that the documentation is not in a good shape. https://help.hcltechsw.com/digital-experience/9.5/containerization/backup_and_recovery_procedures.html https://support.hcltechsw.com/community?id=community_question&sys_id=9e73a02e1b20b850f37655352a4bcbaa","title":"Backup and recover"},{"location":"kube/Migration/k8s-next-migrate-from-operator/#summary","text":"Migration from operator-based to helm-based deployments will only be possible with exactly one DX version (presumable CF198). Both deployments have to be at the same version level. Moving Kubernetes configurations : We will only provide documentation on how to extract configurations from an existing operator-based deployment using dxctl and how to interpret these. We will also document how a customer would be able to reflect these in their custom values.yaml for their helm-based deployment. For the DX Core migration the best approach is to copy the wp_profile from the operator-based deployment to the helm-based deployment. We expect other connected infrastructures (e.g. LDAP) to remain unchanged, hence no configuration changes are necessary. DX Core database should just stay the same, hence no further changes would be necessary after copying the WAS profile. The old database needs to be reachable from the new helm-based deployment. If a customer needs to change the connected database, he can use database transfer tooling to move database content between the database instances. For DX DAM the DAM EXIM tool would be the preferred way. Unfortunately the EXIM implementation is not finished yet. Therefore the DAM move between an operator-based and a helm-based deployment will need to be done using Postgres database export/import and a copy of the DAM's binary storage. If EXIM gets finished in time, we can consider changing the documentation appropriately.","title":"Summary"},{"location":"kube/Migration/k8s-next-property-migration/","text":"Introduction In order to migrate from Operator based deployments to Helm, it might be necessary that you migrate the deployment configuration of your old Operator deployment first. This page will provide you with a mapping that allows you to re-use values from your old deployment.properties file in your new custom-values.yaml . Preparation Ensure that you have followed the preparation process for a fresh Helm deployment, including creation of your custom-values.yaml . You will need the properties file that you used with DXCTL to perform your old Operator deployment. If you do not have that property file at hand, you can refer to the DXCTL documentation on how to use the getProperties function of DXCTL to extract a properties file from your existing deployment. Property mappings This chapter will go through relevant properties that can be found in a DXCTL property file and how they may be mapped to the new custom-values.yaml . Please note: You should only transfer settings that you have adjusted for your Operator deployment. It is not recommended to overwrite all Helm defaults with the defaults of the old Operator deployment. Only migrate settings that are relevant for you or have been adjusted by you prior deploying the Operator with DXCTL. General properties DXCTL property values.yaml key Description dx.namespace n/a Namespace used for the deployment, will be handed to helm directly via CLI dx.name n/a Deployment name, will be handed to helm directly via CLI dx.pullpolicy images.pullPolicy Determines the image pull policy for all container images dx.pod.nodeselector nodeSelector.* NodeSelector used for Pods, can now be done per application in Helm dx.config.authoring configuration.core.tuning.authoring Selects if the instance should be tuned for authoring or not composer.enabled applications.contentComposer Selects if Content Composer will be deployed or not dam.enabled applications.digitalAssetManagement Selects if Digital Asset Management will be deployed or not persist.force-read n/a Read-only fallback enablement, always enabled in Helm Storage properties DXCTL property values.yaml key Description dx.volume volumes.core.profile.volumeName Name of the volume to be used for DX Core Profile dx.volume.size volumes.core.profile.requests.storage Size of the volume to be used for DX Core Profile dx.storageclass volumes.core.profile.storageClassName StorageClass of the volume used for DX Core Profile dx.splitlogging: false n/a Determines if log directory uses a separate volume, always enabled in Helm dx.logging.stgclass volumes.core.log.storageClassName StorageClass for DX Core logging volume dx.logging.size volumes.core.log.requests.storage StorageClass for DX Core logging volume dx.tranlogging n/a Determines if the transaction log directory uses a separate volume, always enabled in Helm dx.tranlogging.reclaim n/a Reclaimpolicy for DX Core transaction log volume. Determined by PV instead of Helm dx.tranlogging.stgclass volumes.core.tranlog.storageClassName StorageClass for the DX Core transaction log volume dx.tranlogging.size volumes.core.tranlog.requests.storage Size used for the DX Core transaction log volume remote.search.volume volumes.remoteSearch.prsprofile.volumeName Name of the volume used for the DX Remote Search profile remote.search.stgclass volumes.remoteSearch.prsprofile.storageClassName StorageClass of the volume for the DX Remote Search profile dam.volume volumes.digitalAssetManagement.binaries.volumeName Name of the volume used for DAM dam.stgclass volumes.digitalAssetManagement.binaries.storageClassName StorageClass of the volume used for DAM Networking properties DXCTL property values.yaml key Description dx.path.contextroot networking.core.contextRoot Context root used for DX dx.path.personalized networking.core.personalizedHome Personalized URL path for DX dx.path.home networking.core.home Non personalized URL path for DX dx.deploy.host.override networking.core.host Hostname that should be used instead of the Loadbalancer hostname dx.deploy.host.override.force n/a Force the use of the override host, obsolete in helm dx.config.cors networking.addon.*.corsOrigin CORS configuration for applications, can be configured per application in Helm hybrid.enabled n/a Determines if hybrid is enabled or not, Helm will derive this from other networking and application settings hybrid.url networking.core.host URL of the DX Core instance in a hybrid deployment hybrid.port networking.core.port Port of the DX Core instance in a hybrid deployment Scaling properties Core DXCTL property values.yaml key Description dx.minreplicas scaling.horizontalPodAutoScaler.core.minReplicas Minimum amount of Pods when scaling is enabled dx.maxreplicas scaling.horizontalPodAutoScaler.core.maxReplicas Maximum amount of Pods when scaling is enabled dx.replicas scaling.replicas.core Default amount of Pods when scaling is disabled dx.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.core.targetCPUUtilizationPercentage CPU Target for autoscaling dx.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.core.targetMemoryUtilizationPercentage Memory Target for autoscaling Ring API DXCTL property values.yaml key Description api.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.ringApi.targetCPUUtilizationPercentage CPU Target for autoscaling api.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.ringApi.targetMemoryUtilizationPercentage Memory Target for autoscaling api.minreplicas scaling.horizontalPodAutoScaler.ringApi.minReplicas Minimum amount of Pods when scaling is enabled api.maxreplicas scaling.horizontalPodAutoScaler.ringApi.maxReplicas Maximum amount of Pods when scaling is enabled Content Composer DXCTL property values.yaml key Description composer.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.contentComposer.targetCPUUtilizationPercentage CPU Target for autoscaling composer.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.contentComposer.targetMemoryUtilizationPercentage Memory Target for autoscaling composer.minreplicas scaling.horizontalPodAutoScaler.contentComposer.minReplicas Minimum amount of Pods when scaling is enabled composer.maxreplicas scaling.horizontalPodAutoScaler.contentComposer.maxReplicas Maximum amount of Pods when scaling is enabled Digital Asset Management DXCTL property values.yaml key Description dam.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.digitalAssetManagement.targetCPUUtilizationPercentage CPU Target for autoscaling dam.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.digitalAssetManagement.targetMemoryUtilizationPercentage Memory Target for autoscaling dam.minreplicas scaling.horizontalPodAutoScaler.digitalAssetManagement.minReplicas Minimum amount of Pods when scaling is enabled dam.maxreplicas scaling.horizontalPodAutoScaler.digitalAssetManagement.maxReplicas Maximum amount of Pods when scaling is enabled Image Processor DXCTL property values.yaml key Description imgproc.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.imageProcessor.targetCPUUtilizationPercentage CPU Target for autoscaling imgproc.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.imageProcessor.targetMemoryUtilizationPercentage Memory Target for autoscaling imgproc.minreplicas scaling.horizontalPodAutoScaler.imageProcessor.minReplicas Minimum amount of Pods when scaling is enabled imgproc.maxreplicas scaling.horizontalPodAutoScaler.imageProcessor.maxReplicas Maximum amount of Pods when scaling is enabled Resource allocation properties Core DXCTL property values.yaml key Description dx.request.cpu resources.core.requests.cpu CPU Request dx.request.memory resources.core.requests.memory Memory Request dx.limit.cpu resources.core.limits.cpu CPU Limit dx.limit.memory resources.core.limits.memory Memory Limit Ring API DXCTL property values.yaml key Description api.request.cpu resources.ringApi.requests.cpu CPU Request api.request.memory resources.ringApi.requests.memory Memory Request api.limit.cpu resources.ringApi.limits.cpu CPU Limit api.limit.memory resources.ringApi.limits.memory Memory Limit Content Composer DXCTL property values.yaml key Description composer.request.cpu resources.contentComposer.requests.cpu CPU Request composer.request.memory resources.contentComposer.requests.memory Memory Request composer.limit.cpu resources.contentComposer.limits.cpu CPU Limit composer.limit.memory resources.contentComposer.limits.memory Memory Limit Digital Asset Management DXCTL property values.yaml key Description dam.request.cpu resources.digitalAssetManagement.requests.cpu CPU Request dam.request.memory resources.digitalAssetManagement.requests.memory Memory Request dam.limit.cpu resources.digitalAssetManagement.limits.cpu CPU Limit dam.limit.memory resources.digitalAssetManagement.limits.memory Memory Limit Persistence DXCTL property values.yaml key Description persist.request.cpu resources.persistence.requests.cpu CPU Request persist.request.memory resources.persistence.requests.memory Memory Request persist.limit.cpu resources.persistence.limits.cpu CPU Limit persist.limit.memory resources.persistence.limits.memory Memory Limit Image Processor DXCTL property values.yaml key Description imgproc.request.cpu resources.imageProcessor.requests.cpu CPU Request imgproc.request.memory resources.imageProcessor.requests.memory Memory Request imgproc.limit.cpu resources.imageProcessor.limits.cpu CPU Limit imgproc.limit.memory resources.imageProcessor.limits.memory Memory Limit","title":"Operator property migration"},{"location":"kube/Migration/k8s-next-property-migration/#introduction","text":"In order to migrate from Operator based deployments to Helm, it might be necessary that you migrate the deployment configuration of your old Operator deployment first. This page will provide you with a mapping that allows you to re-use values from your old deployment.properties file in your new custom-values.yaml .","title":"Introduction"},{"location":"kube/Migration/k8s-next-property-migration/#preparation","text":"Ensure that you have followed the preparation process for a fresh Helm deployment, including creation of your custom-values.yaml . You will need the properties file that you used with DXCTL to perform your old Operator deployment. If you do not have that property file at hand, you can refer to the DXCTL documentation on how to use the getProperties function of DXCTL to extract a properties file from your existing deployment.","title":"Preparation"},{"location":"kube/Migration/k8s-next-property-migration/#property-mappings","text":"This chapter will go through relevant properties that can be found in a DXCTL property file and how they may be mapped to the new custom-values.yaml . Please note: You should only transfer settings that you have adjusted for your Operator deployment. It is not recommended to overwrite all Helm defaults with the defaults of the old Operator deployment. Only migrate settings that are relevant for you or have been adjusted by you prior deploying the Operator with DXCTL.","title":"Property mappings"},{"location":"kube/Migration/k8s-next-property-migration/#general-properties","text":"DXCTL property values.yaml key Description dx.namespace n/a Namespace used for the deployment, will be handed to helm directly via CLI dx.name n/a Deployment name, will be handed to helm directly via CLI dx.pullpolicy images.pullPolicy Determines the image pull policy for all container images dx.pod.nodeselector nodeSelector.* NodeSelector used for Pods, can now be done per application in Helm dx.config.authoring configuration.core.tuning.authoring Selects if the instance should be tuned for authoring or not composer.enabled applications.contentComposer Selects if Content Composer will be deployed or not dam.enabled applications.digitalAssetManagement Selects if Digital Asset Management will be deployed or not persist.force-read n/a Read-only fallback enablement, always enabled in Helm","title":"General properties"},{"location":"kube/Migration/k8s-next-property-migration/#storage-properties","text":"DXCTL property values.yaml key Description dx.volume volumes.core.profile.volumeName Name of the volume to be used for DX Core Profile dx.volume.size volumes.core.profile.requests.storage Size of the volume to be used for DX Core Profile dx.storageclass volumes.core.profile.storageClassName StorageClass of the volume used for DX Core Profile dx.splitlogging: false n/a Determines if log directory uses a separate volume, always enabled in Helm dx.logging.stgclass volumes.core.log.storageClassName StorageClass for DX Core logging volume dx.logging.size volumes.core.log.requests.storage StorageClass for DX Core logging volume dx.tranlogging n/a Determines if the transaction log directory uses a separate volume, always enabled in Helm dx.tranlogging.reclaim n/a Reclaimpolicy for DX Core transaction log volume. Determined by PV instead of Helm dx.tranlogging.stgclass volumes.core.tranlog.storageClassName StorageClass for the DX Core transaction log volume dx.tranlogging.size volumes.core.tranlog.requests.storage Size used for the DX Core transaction log volume remote.search.volume volumes.remoteSearch.prsprofile.volumeName Name of the volume used for the DX Remote Search profile remote.search.stgclass volumes.remoteSearch.prsprofile.storageClassName StorageClass of the volume for the DX Remote Search profile dam.volume volumes.digitalAssetManagement.binaries.volumeName Name of the volume used for DAM dam.stgclass volumes.digitalAssetManagement.binaries.storageClassName StorageClass of the volume used for DAM","title":"Storage properties"},{"location":"kube/Migration/k8s-next-property-migration/#networking-properties","text":"DXCTL property values.yaml key Description dx.path.contextroot networking.core.contextRoot Context root used for DX dx.path.personalized networking.core.personalizedHome Personalized URL path for DX dx.path.home networking.core.home Non personalized URL path for DX dx.deploy.host.override networking.core.host Hostname that should be used instead of the Loadbalancer hostname dx.deploy.host.override.force n/a Force the use of the override host, obsolete in helm dx.config.cors networking.addon.*.corsOrigin CORS configuration for applications, can be configured per application in Helm hybrid.enabled n/a Determines if hybrid is enabled or not, Helm will derive this from other networking and application settings hybrid.url networking.core.host URL of the DX Core instance in a hybrid deployment hybrid.port networking.core.port Port of the DX Core instance in a hybrid deployment","title":"Networking properties"},{"location":"kube/Migration/k8s-next-property-migration/#scaling-properties","text":"","title":"Scaling properties"},{"location":"kube/Migration/k8s-next-property-migration/#core","text":"DXCTL property values.yaml key Description dx.minreplicas scaling.horizontalPodAutoScaler.core.minReplicas Minimum amount of Pods when scaling is enabled dx.maxreplicas scaling.horizontalPodAutoScaler.core.maxReplicas Maximum amount of Pods when scaling is enabled dx.replicas scaling.replicas.core Default amount of Pods when scaling is disabled dx.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.core.targetCPUUtilizationPercentage CPU Target for autoscaling dx.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.core.targetMemoryUtilizationPercentage Memory Target for autoscaling","title":"Core"},{"location":"kube/Migration/k8s-next-property-migration/#ring-api","text":"DXCTL property values.yaml key Description api.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.ringApi.targetCPUUtilizationPercentage CPU Target for autoscaling api.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.ringApi.targetMemoryUtilizationPercentage Memory Target for autoscaling api.minreplicas scaling.horizontalPodAutoScaler.ringApi.minReplicas Minimum amount of Pods when scaling is enabled api.maxreplicas scaling.horizontalPodAutoScaler.ringApi.maxReplicas Maximum amount of Pods when scaling is enabled","title":"Ring API"},{"location":"kube/Migration/k8s-next-property-migration/#content-composer","text":"DXCTL property values.yaml key Description composer.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.contentComposer.targetCPUUtilizationPercentage CPU Target for autoscaling composer.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.contentComposer.targetMemoryUtilizationPercentage Memory Target for autoscaling composer.minreplicas scaling.horizontalPodAutoScaler.contentComposer.minReplicas Minimum amount of Pods when scaling is enabled composer.maxreplicas scaling.horizontalPodAutoScaler.contentComposer.maxReplicas Maximum amount of Pods when scaling is enabled","title":"Content Composer"},{"location":"kube/Migration/k8s-next-property-migration/#digital-asset-management","text":"DXCTL property values.yaml key Description dam.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.digitalAssetManagement.targetCPUUtilizationPercentage CPU Target for autoscaling dam.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.digitalAssetManagement.targetMemoryUtilizationPercentage Memory Target for autoscaling dam.minreplicas scaling.horizontalPodAutoScaler.digitalAssetManagement.minReplicas Minimum amount of Pods when scaling is enabled dam.maxreplicas scaling.horizontalPodAutoScaler.digitalAssetManagement.maxReplicas Maximum amount of Pods when scaling is enabled","title":"Digital Asset Management"},{"location":"kube/Migration/k8s-next-property-migration/#image-processor","text":"DXCTL property values.yaml key Description imgproc.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.imageProcessor.targetCPUUtilizationPercentage CPU Target for autoscaling imgproc.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.imageProcessor.targetMemoryUtilizationPercentage Memory Target for autoscaling imgproc.minreplicas scaling.horizontalPodAutoScaler.imageProcessor.minReplicas Minimum amount of Pods when scaling is enabled imgproc.maxreplicas scaling.horizontalPodAutoScaler.imageProcessor.maxReplicas Maximum amount of Pods when scaling is enabled","title":"Image Processor"},{"location":"kube/Migration/k8s-next-property-migration/#resource-allocation-properties","text":"","title":"Resource allocation properties"},{"location":"kube/Migration/k8s-next-property-migration/#core_1","text":"DXCTL property values.yaml key Description dx.request.cpu resources.core.requests.cpu CPU Request dx.request.memory resources.core.requests.memory Memory Request dx.limit.cpu resources.core.limits.cpu CPU Limit dx.limit.memory resources.core.limits.memory Memory Limit","title":"Core"},{"location":"kube/Migration/k8s-next-property-migration/#ring-api_1","text":"DXCTL property values.yaml key Description api.request.cpu resources.ringApi.requests.cpu CPU Request api.request.memory resources.ringApi.requests.memory Memory Request api.limit.cpu resources.ringApi.limits.cpu CPU Limit api.limit.memory resources.ringApi.limits.memory Memory Limit","title":"Ring API"},{"location":"kube/Migration/k8s-next-property-migration/#content-composer_1","text":"DXCTL property values.yaml key Description composer.request.cpu resources.contentComposer.requests.cpu CPU Request composer.request.memory resources.contentComposer.requests.memory Memory Request composer.limit.cpu resources.contentComposer.limits.cpu CPU Limit composer.limit.memory resources.contentComposer.limits.memory Memory Limit","title":"Content Composer"},{"location":"kube/Migration/k8s-next-property-migration/#digital-asset-management_1","text":"DXCTL property values.yaml key Description dam.request.cpu resources.digitalAssetManagement.requests.cpu CPU Request dam.request.memory resources.digitalAssetManagement.requests.memory Memory Request dam.limit.cpu resources.digitalAssetManagement.limits.cpu CPU Limit dam.limit.memory resources.digitalAssetManagement.limits.memory Memory Limit","title":"Digital Asset Management"},{"location":"kube/Migration/k8s-next-property-migration/#persistence","text":"DXCTL property values.yaml key Description persist.request.cpu resources.persistence.requests.cpu CPU Request persist.request.memory resources.persistence.requests.memory Memory Request persist.limit.cpu resources.persistence.limits.cpu CPU Limit persist.limit.memory resources.persistence.limits.memory Memory Limit","title":"Persistence"},{"location":"kube/Migration/k8s-next-property-migration/#image-processor_1","text":"DXCTL property values.yaml key Description imgproc.request.cpu resources.imageProcessor.requests.cpu CPU Request imgproc.request.memory resources.imageProcessor.requests.memory Memory Request imgproc.limit.cpu resources.imageProcessor.limits.cpu CPU Limit imgproc.limit.memory resources.imageProcessor.limits.memory Memory Limit","title":"Image Processor"},{"location":"kube/Prometheus/k8s-next-nodejs-prom/","text":"PoC Goals The goal is to evaluate how we can expose Prometheus compatible metrics in our NodeJS applications and expose metrics via the main NodeJS process of one of our applications. The outcome of that PoC should be documented so it can be reused for a thorough implementation in our NodeJS applications as required. For this PoC, the DAM is used as an example application. Due to the similar structure of all skeleton based NodeJS projects, the steps shown here can be re-used in other applications like RingAPI as well. Prepare application to expose metrics Install prom-client to the server package We use the package prom-client to generate Prometheus compatible metrics. This package exposes NodeJS default metrics and is also able to expose custom metrics if desired. cd packages/server-v1 npm install --save prom-client After the install is successful, the prom-client can be used. Expose metrics endpoint The prom-client itself is only creating a Prometheus compatible data structure, but not exposing those via any HTTP endpoint. Therefore, we create a metrics probe endpoint at the path /probe/metrics . Since application metrics and logging are mostly infrastructure related information, choosing this path appeared appropriate. We create the file /packages/server-v1/src/probes/metrics.probe.ts with the following contents: /* ******************************************************************** * Licensed Materials - Property of HCL * * * * Copyright HCL Technologies Ltd. 2021. All Rights Reserved. * * * * Note to US Government Users Restricted Rights: * * * * Use, duplication or disclosure restricted by GSA ADP Schedule * ******************************************************************** */ import { NextFunction, Request, Response } from 'express'; import * as client from 'prom-client'; import { loggerFactory } from '@enchanted-prod/logger'; const logger = loggerFactory(); client.collectDefaultMetrics({ gcDurationBuckets: [0.001, 0.01, 0.1, 1, 2, 5], // These are the default buckets. }); /* * This probe provides prometheus consumable metrics. */ export class MetricsProbe { public metricsProbeHandler = async (req: Request, res: Response, next: NextFunction) => { logger.debug('Metric Probe is called.'); const reg = client.register; const defaultMetrics = await reg.metrics(); return res.send(defaultMetrics).status(200); } } After that, we adjust the file /packages/server-v1/src/probes/index.js to also include the metrics endpoint. /* ******************************************************************** * Licensed Materials - Property of HCL * * * * Copyright HCL Technologies Ltd. 2021. All Rights Reserved. * * * * Note to US Government Users Restricted Rights: * * * * Use, duplication or disclosure restricted by GSA ADP Schedule * ******************************************************************** */ export * from './ready.probe'; export * from './live.probe'; export * from './metrics.probe'; With that configured, we are now able to enable the endpoint inside the /packages/server-v1/src/server.ts where the other probes are already in place: const readyProbe = new ReadyProbe(this.lbApp, this.operationScheduler); this.app.use('/probe/ready', readyProbe.readyProbeHandler); const liveProbe = new LiveProbe(); this.app.use('/probe/live', liveProbe.liveProbeHandler); // Expose the metrics probe endpoint const metricsProbe = new MetricsProbe(); this.app.use('/probe/metrics', metricsProbe.metricsProbeHandler) const staticUi = new StaticUi(); With that configured, the DAM Pod will now expose metrics data at /probe/metrics . Add custom metrics To expose custom metrics, we can leverage prom-client again and add custom metrics like counters or gauges. The example adds the gauge values dam_active_workers and dam_free_workers as a custom metric. Therefore we add two custom gauges in the file /packages/server-v1/src/operations/scheduler.ts . const activeWorkerGauge = new client.Gauge({ name: 'dam_active_workers', help: 'Active workers handling operations.' }); const freeWorkerGauge = new client.Gauge({ name: 'dam_free_workers', help: 'Free workers ready for operations.' }); It contains the name of the metric, as well as a text description that will be shown as a help for the metrics values. Whenever the count of active or free workers change, we need to update the previously created gauge by setting the new value. activeWorkerGauge.set(this.activeWorkers.workers.length); freeWorkerGauge.set(this.freeWorkers.workers.length); In case of DAM, we enhance the activeWorkers and freeWorkers arrays to trigger the update of the gauges themselves whenever workers are added to or removed from it. Deploying DAM with NodeJS metrics enabled in Kubernetes For building the image and pushing it to artifactory, a build pipeline for DAM has been used on PJT. Deploy DX including the DAM image with metrics The DX deployment used in this PoC is running on k3s locally and with a minimal configuration. The Helm Charts used are hcl-dx-deployment-v2.0.0_20210716-1545_rohan_develop.tgz . Only DAM, Core, Persistence, Image Processor, RingAPI and Ambassador are deployed, with a minimal request set of resources. The following custom values are being used for deployment: #******************************************************************** #* Licensed Materials - Property of HCL * #* * #* Copyright HCL Technologies Ltd. 2021. All Rights Reserved. * #* * #* Note to US Government Users Restricted Rights: * #* * #* Use, duplication or disclosure restricted by GSA ADP Schedule * #******************************************************************** # Prometheus DAM PoC values, smallest deployment # Image related configuration images: # Container repository used to retrieve the images repository: \"quintana-docker.artifactory.cwp.pnp-hcl.com/dx-build-output\" # Image tag for each application tags: core: \"v95_CF197_20210802-055523_rohan_develop_6107af8e\" digitalAssetManagement: \"v1.9.0_20210802-1721_pjd_feature_DXQ-16661-prom-exporter-poc\" imageProcessor: \"v1.10.0_20210721-1401_rohan_release_95_CF197\" persistence: \"v1.10.0_20210727-1300_rohan_release_95_CF197\" ringApi: \"v1.10.0_20210726-1106_rohan_develop\" ambassadorIngress: \"1.5.4\" ambassadorRedis: \"5.0.1\" # Image name for each application names: core: \"core/dxen\" digitalAssetManagement: \"core-addon/media-library\" imageProcessor: \"core-addon/image-processor\" persistence: \"core-addon/persistence/postgres\" ringApi: \"core-addon/api/ringapi\" ambassadorIngress: \"common/ambassador\" ambassadorRedis: \"common/redis\" # Resource allocation settings, definition per pod # Use number + unit, e.g. 1500m for CPU or 1500M for Memory resources: # Content composer resource allocation contentComposer: requests: cpu: \"100m\" memory: \"128Mi\" # Core resource allocation core: requests: cpu: \"1000m\" memory: \"3072Mi\" # Design Studio resource allocation designStudio: requests: cpu: \"100m\" memory: \"128Mi\" # Digital asset management resource allocation digitalAssetManagement: requests: cpu: \"250m\" memory: \"1G\" # Image processor resource allocation imageProcessor: requests: cpu: \"100m\" memory: \"1280Mi\" # Open LDAP resource allocation openLdap: requests: cpu: \"200m\" memory: \"512Mi\" # Persistence resource allocation persistence: requests: cpu: \"250m\" memory: \"512Mi\" # Remote Search resource allocation remoteSearch: requests: cpu: \"500m\" memory: \"768Mi\" # Ring API resource allocation ringApi: requests: cpu: \"100m\" memory: \"128Mi\" # Ambassador ingress resource allocation ambassadorIngress: requests: cpu: \"200m\" memory: \"300Mi\" # Ambassador Redis resource allocation ambassadorRedis: requests: cpu: \"100m\" memory: \"256Mi\" # Runtime Controller resource allocation runtimeController: requests: cpu: \"100m\" memory: \"256Mi\" applications: contentComposer: false core: true designStudio: false digitalAssetManagement: true imageProcessor: true openLdap: false persistence: true remoteSearch: false ringApi: true ambassador: true runtimeController: false Install Prometheus To install prometheus, we use Helm. Add Prometheus Helm Chart Repo to Helm: helm repo add prometheus-community https://prometheus-community.github.io/helm-charts We can extract the default values via Helm: helm show values prometheus-community/prometheus > prom-values.yaml We'll use the following values for a simple PoC deployment of Prometheus, disabling persistence and additional services. serviceAccounts: alertmanager: create: false nodeExporter: create: false alertmanager: enabled: false nodeExporter: enabled: false server: enabled: true persistentVolume: enabled: false service: type: NodePort pushgateway: enabled: false Install the Prometheus Application: helm install prometheus prometheus-community/prometheus -n prom -f prom-values.yaml Find the NodePort that is used and access Prometheus: NODEPORT=$(kubectl get -o jsonpath=\"{.spec.ports[0].nodePort}\" services prometheus-server -n prom) echo $NODEPORT You can now access Prometheus using your Browser: http://<NODE_IP>:<NODE_PORT> Adjust DAM Pods to be scraped by Prometheus Add the following annotations to the DAM Pods in the StatefulSet to have Prometheus automatically scrape the metrics endpoint. kubectl edit StatefulSet dx-digital-asset-management -n prom spec: template: metadata: annotations: prometheus.io/scrape: \"true\" prometheus.io/path: \"/probe/metrics\" prometheus.io/port: \"3000\" The DAM Pod will be restarted shortly and Prometheus should scrape the DAM Pod. Configure a test dashboard for DAM You can use the metrics dam_active_workers and dam_free_workers . Here is a sample JSON which can be used: { \"annotations\": { \"list\": [ { \"builtIn\": 1, \"datasource\": \"-- Grafana --\", \"enable\": true, \"hide\": true, \"iconColor\": \"rgba(0, 211, 255, 1)\", \"name\": \"Annotations & Alerts\", \"type\": \"dashboard\" } ] }, \"editable\": true, \"gnetId\": null, \"graphTooltip\": 0, \"id\": null, \"links\": [], \"panels\": [ { \"datasource\": null, \"fieldConfig\": { \"defaults\": { \"color\": { \"mode\": \"palette-classic\" }, \"custom\": { \"axisLabel\": \"\", \"axisPlacement\": \"auto\", \"barAlignment\": 0, \"drawStyle\": \"line\", \"fillOpacity\": 0, \"gradientMode\": \"none\", \"hideFrom\": { \"legend\": false, \"tooltip\": false, \"viz\": false }, \"lineInterpolation\": \"linear\", \"lineWidth\": 1, \"pointSize\": 5, \"scaleDistribution\": { \"type\": \"linear\" }, \"showPoints\": \"auto\", \"spanNulls\": false, \"stacking\": { \"group\": \"A\", \"mode\": \"none\" }, \"thresholdsStyle\": { \"mode\": \"off\" } }, \"mappings\": [], \"thresholds\": { \"mode\": \"absolute\", \"steps\": [ { \"color\": \"green\", \"value\": null }, { \"color\": \"red\", \"value\": 80 } ] } }, \"overrides\": [ { \"__systemRef\": \"hideSeriesFrom\", \"matcher\": { \"id\": \"byNames\", \"options\": { \"mode\": \"exclude\", \"names\": [ \"dam_free_workers{app=\\\"dx-digital-asset-management\\\", app_kubernetes_io_instance=\\\"dx\\\", app_kubernetes_io_managed_by=\\\"Helm\\\", app_kubernetes_io_name=\\\"hcl-dx-deployment\\\", app_kubernetes_io_version=\\\"95_CF197\\\", controller_revision_hash=\\\"dx-digital-asset-management-554ccc9d7b\\\", helm_sh_chart=\\\"hcl-dx-deployment-2.0.0\\\", instance=\\\"10.42.0.75:3000\\\", job=\\\"kubernetes-pods\\\", kubernetes_namespace=\\\"prom\\\", kubernetes_pod_name=\\\"dx-digital-asset-management-0\\\", release=\\\"dx\\\", statefulset_kubernetes_io_pod_name=\\\"dx-digital-asset-management-0\\\"}\" ], \"prefix\": \"All except:\", \"readOnly\": true } }, \"properties\": [ { \"id\": \"custom.hideFrom\", \"value\": { \"legend\": false, \"tooltip\": false, \"viz\": true } } ] } ] }, \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 0 }, \"id\": 4, \"options\": { \"legend\": { \"calcs\": [], \"displayMode\": \"list\", \"placement\": \"bottom\" }, \"tooltip\": { \"mode\": \"single\" } }, \"targets\": [ { \"exemplar\": true, \"expr\": \"dam_free_workers\", \"interval\": \"\", \"legendFormat\": \"\", \"refId\": \"A\" } ], \"title\": \"DAM Free Workers\", \"type\": \"timeseries\" }, { \"datasource\": null, \"description\": \"\", \"fieldConfig\": { \"defaults\": { \"color\": { \"mode\": \"palette-classic\" }, \"custom\": { \"axisLabel\": \"\", \"axisPlacement\": \"auto\", \"barAlignment\": 0, \"drawStyle\": \"line\", \"fillOpacity\": 0, \"gradientMode\": \"none\", \"hideFrom\": { \"legend\": false, \"tooltip\": false, \"viz\": false }, \"lineInterpolation\": \"linear\", \"lineWidth\": 1, \"pointSize\": 5, \"scaleDistribution\": { \"type\": \"linear\" }, \"showPoints\": \"auto\", \"spanNulls\": false, \"stacking\": { \"group\": \"A\", \"mode\": \"none\" }, \"thresholdsStyle\": { \"mode\": \"off\" } }, \"mappings\": [], \"thresholds\": { \"mode\": \"absolute\", \"steps\": [ { \"color\": \"green\", \"value\": null }, { \"color\": \"red\", \"value\": 80 } ] } }, \"overrides\": [] }, \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 8 }, \"id\": 2, \"options\": { \"legend\": { \"calcs\": [], \"displayMode\": \"list\", \"placement\": \"bottom\" }, \"tooltip\": { \"mode\": \"single\" } }, \"targets\": [ { \"exemplar\": true, \"expr\": \"dam_active_workers\", \"interval\": \"\", \"legendFormat\": \"\", \"refId\": \"A\" } ], \"title\": \"DAM Active Workers\", \"type\": \"timeseries\" } ], \"refresh\": \"10s\", \"schemaVersion\": 30, \"style\": \"dark\", \"tags\": [], \"templating\": { \"list\": [] }, \"time\": { \"from\": \"now-15m\", \"to\": \"now\" }, \"timepicker\": {}, \"timezone\": \"\", \"title\": \"New dashboard\", \"uid\": null, \"version\": 0 } The output looks like this: Using existing Grafana Dashboards There are also existing Grafana dashboards that can be leveraged. One that is related to NodeJS and prom-client can be found here Grafana Dashboard 11159 . Conclusion The use of prom-client enables us to easily expose default NodeJS metrics as well as custom applications metrics that are bound to our application logic. The implementation is straight forward and the main effort lies within defining metrics that should be exposed. We will need to adjust our Helm charts to add the necessary annotations to all application Pods that expose metrics for prometheus. With that, prometheus will automatically scrape those applications and aggregate the metrics data. The NodeJS default metrics also allow for in-depth analysis on performance issues, that can be seen in monitoring of the NodeJS runtime.","title":"NodeJS Metrics"},{"location":"kube/Prometheus/k8s-next-nodejs-prom/#poc-goals","text":"The goal is to evaluate how we can expose Prometheus compatible metrics in our NodeJS applications and expose metrics via the main NodeJS process of one of our applications. The outcome of that PoC should be documented so it can be reused for a thorough implementation in our NodeJS applications as required. For this PoC, the DAM is used as an example application. Due to the similar structure of all skeleton based NodeJS projects, the steps shown here can be re-used in other applications like RingAPI as well.","title":"PoC Goals"},{"location":"kube/Prometheus/k8s-next-nodejs-prom/#prepare-application-to-expose-metrics","text":"","title":"Prepare application to expose metrics"},{"location":"kube/Prometheus/k8s-next-nodejs-prom/#install-prom-client-to-the-server-package","text":"We use the package prom-client to generate Prometheus compatible metrics. This package exposes NodeJS default metrics and is also able to expose custom metrics if desired. cd packages/server-v1 npm install --save prom-client After the install is successful, the prom-client can be used.","title":"Install prom-client to the server package"},{"location":"kube/Prometheus/k8s-next-nodejs-prom/#expose-metrics-endpoint","text":"The prom-client itself is only creating a Prometheus compatible data structure, but not exposing those via any HTTP endpoint. Therefore, we create a metrics probe endpoint at the path /probe/metrics . Since application metrics and logging are mostly infrastructure related information, choosing this path appeared appropriate. We create the file /packages/server-v1/src/probes/metrics.probe.ts with the following contents: /* ******************************************************************** * Licensed Materials - Property of HCL * * * * Copyright HCL Technologies Ltd. 2021. All Rights Reserved. * * * * Note to US Government Users Restricted Rights: * * * * Use, duplication or disclosure restricted by GSA ADP Schedule * ******************************************************************** */ import { NextFunction, Request, Response } from 'express'; import * as client from 'prom-client'; import { loggerFactory } from '@enchanted-prod/logger'; const logger = loggerFactory(); client.collectDefaultMetrics({ gcDurationBuckets: [0.001, 0.01, 0.1, 1, 2, 5], // These are the default buckets. }); /* * This probe provides prometheus consumable metrics. */ export class MetricsProbe { public metricsProbeHandler = async (req: Request, res: Response, next: NextFunction) => { logger.debug('Metric Probe is called.'); const reg = client.register; const defaultMetrics = await reg.metrics(); return res.send(defaultMetrics).status(200); } } After that, we adjust the file /packages/server-v1/src/probes/index.js to also include the metrics endpoint. /* ******************************************************************** * Licensed Materials - Property of HCL * * * * Copyright HCL Technologies Ltd. 2021. All Rights Reserved. * * * * Note to US Government Users Restricted Rights: * * * * Use, duplication or disclosure restricted by GSA ADP Schedule * ******************************************************************** */ export * from './ready.probe'; export * from './live.probe'; export * from './metrics.probe'; With that configured, we are now able to enable the endpoint inside the /packages/server-v1/src/server.ts where the other probes are already in place: const readyProbe = new ReadyProbe(this.lbApp, this.operationScheduler); this.app.use('/probe/ready', readyProbe.readyProbeHandler); const liveProbe = new LiveProbe(); this.app.use('/probe/live', liveProbe.liveProbeHandler); // Expose the metrics probe endpoint const metricsProbe = new MetricsProbe(); this.app.use('/probe/metrics', metricsProbe.metricsProbeHandler) const staticUi = new StaticUi(); With that configured, the DAM Pod will now expose metrics data at /probe/metrics .","title":"Expose metrics endpoint"},{"location":"kube/Prometheus/k8s-next-nodejs-prom/#add-custom-metrics","text":"To expose custom metrics, we can leverage prom-client again and add custom metrics like counters or gauges. The example adds the gauge values dam_active_workers and dam_free_workers as a custom metric. Therefore we add two custom gauges in the file /packages/server-v1/src/operations/scheduler.ts . const activeWorkerGauge = new client.Gauge({ name: 'dam_active_workers', help: 'Active workers handling operations.' }); const freeWorkerGauge = new client.Gauge({ name: 'dam_free_workers', help: 'Free workers ready for operations.' }); It contains the name of the metric, as well as a text description that will be shown as a help for the metrics values. Whenever the count of active or free workers change, we need to update the previously created gauge by setting the new value. activeWorkerGauge.set(this.activeWorkers.workers.length); freeWorkerGauge.set(this.freeWorkers.workers.length); In case of DAM, we enhance the activeWorkers and freeWorkers arrays to trigger the update of the gauges themselves whenever workers are added to or removed from it.","title":"Add custom metrics"},{"location":"kube/Prometheus/k8s-next-nodejs-prom/#deploying-dam-with-nodejs-metrics-enabled-in-kubernetes","text":"For building the image and pushing it to artifactory, a build pipeline for DAM has been used on PJT.","title":"Deploying DAM with NodeJS metrics enabled in Kubernetes"},{"location":"kube/Prometheus/k8s-next-nodejs-prom/#deploy-dx-including-the-dam-image-with-metrics","text":"The DX deployment used in this PoC is running on k3s locally and with a minimal configuration. The Helm Charts used are hcl-dx-deployment-v2.0.0_20210716-1545_rohan_develop.tgz . Only DAM, Core, Persistence, Image Processor, RingAPI and Ambassador are deployed, with a minimal request set of resources. The following custom values are being used for deployment: #******************************************************************** #* Licensed Materials - Property of HCL * #* * #* Copyright HCL Technologies Ltd. 2021. All Rights Reserved. * #* * #* Note to US Government Users Restricted Rights: * #* * #* Use, duplication or disclosure restricted by GSA ADP Schedule * #******************************************************************** # Prometheus DAM PoC values, smallest deployment # Image related configuration images: # Container repository used to retrieve the images repository: \"quintana-docker.artifactory.cwp.pnp-hcl.com/dx-build-output\" # Image tag for each application tags: core: \"v95_CF197_20210802-055523_rohan_develop_6107af8e\" digitalAssetManagement: \"v1.9.0_20210802-1721_pjd_feature_DXQ-16661-prom-exporter-poc\" imageProcessor: \"v1.10.0_20210721-1401_rohan_release_95_CF197\" persistence: \"v1.10.0_20210727-1300_rohan_release_95_CF197\" ringApi: \"v1.10.0_20210726-1106_rohan_develop\" ambassadorIngress: \"1.5.4\" ambassadorRedis: \"5.0.1\" # Image name for each application names: core: \"core/dxen\" digitalAssetManagement: \"core-addon/media-library\" imageProcessor: \"core-addon/image-processor\" persistence: \"core-addon/persistence/postgres\" ringApi: \"core-addon/api/ringapi\" ambassadorIngress: \"common/ambassador\" ambassadorRedis: \"common/redis\" # Resource allocation settings, definition per pod # Use number + unit, e.g. 1500m for CPU or 1500M for Memory resources: # Content composer resource allocation contentComposer: requests: cpu: \"100m\" memory: \"128Mi\" # Core resource allocation core: requests: cpu: \"1000m\" memory: \"3072Mi\" # Design Studio resource allocation designStudio: requests: cpu: \"100m\" memory: \"128Mi\" # Digital asset management resource allocation digitalAssetManagement: requests: cpu: \"250m\" memory: \"1G\" # Image processor resource allocation imageProcessor: requests: cpu: \"100m\" memory: \"1280Mi\" # Open LDAP resource allocation openLdap: requests: cpu: \"200m\" memory: \"512Mi\" # Persistence resource allocation persistence: requests: cpu: \"250m\" memory: \"512Mi\" # Remote Search resource allocation remoteSearch: requests: cpu: \"500m\" memory: \"768Mi\" # Ring API resource allocation ringApi: requests: cpu: \"100m\" memory: \"128Mi\" # Ambassador ingress resource allocation ambassadorIngress: requests: cpu: \"200m\" memory: \"300Mi\" # Ambassador Redis resource allocation ambassadorRedis: requests: cpu: \"100m\" memory: \"256Mi\" # Runtime Controller resource allocation runtimeController: requests: cpu: \"100m\" memory: \"256Mi\" applications: contentComposer: false core: true designStudio: false digitalAssetManagement: true imageProcessor: true openLdap: false persistence: true remoteSearch: false ringApi: true ambassador: true runtimeController: false","title":"Deploy DX including the DAM image with metrics"},{"location":"kube/Prometheus/k8s-next-nodejs-prom/#install-prometheus","text":"To install prometheus, we use Helm. Add Prometheus Helm Chart Repo to Helm: helm repo add prometheus-community https://prometheus-community.github.io/helm-charts We can extract the default values via Helm: helm show values prometheus-community/prometheus > prom-values.yaml We'll use the following values for a simple PoC deployment of Prometheus, disabling persistence and additional services. serviceAccounts: alertmanager: create: false nodeExporter: create: false alertmanager: enabled: false nodeExporter: enabled: false server: enabled: true persistentVolume: enabled: false service: type: NodePort pushgateway: enabled: false Install the Prometheus Application: helm install prometheus prometheus-community/prometheus -n prom -f prom-values.yaml Find the NodePort that is used and access Prometheus: NODEPORT=$(kubectl get -o jsonpath=\"{.spec.ports[0].nodePort}\" services prometheus-server -n prom) echo $NODEPORT You can now access Prometheus using your Browser: http://<NODE_IP>:<NODE_PORT>","title":"Install Prometheus"},{"location":"kube/Prometheus/k8s-next-nodejs-prom/#adjust-dam-pods-to-be-scraped-by-prometheus","text":"Add the following annotations to the DAM Pods in the StatefulSet to have Prometheus automatically scrape the metrics endpoint. kubectl edit StatefulSet dx-digital-asset-management -n prom spec: template: metadata: annotations: prometheus.io/scrape: \"true\" prometheus.io/path: \"/probe/metrics\" prometheus.io/port: \"3000\" The DAM Pod will be restarted shortly and Prometheus should scrape the DAM Pod.","title":"Adjust DAM Pods to be scraped by Prometheus"},{"location":"kube/Prometheus/k8s-next-nodejs-prom/#configure-a-test-dashboard-for-dam","text":"You can use the metrics dam_active_workers and dam_free_workers . Here is a sample JSON which can be used: { \"annotations\": { \"list\": [ { \"builtIn\": 1, \"datasource\": \"-- Grafana --\", \"enable\": true, \"hide\": true, \"iconColor\": \"rgba(0, 211, 255, 1)\", \"name\": \"Annotations & Alerts\", \"type\": \"dashboard\" } ] }, \"editable\": true, \"gnetId\": null, \"graphTooltip\": 0, \"id\": null, \"links\": [], \"panels\": [ { \"datasource\": null, \"fieldConfig\": { \"defaults\": { \"color\": { \"mode\": \"palette-classic\" }, \"custom\": { \"axisLabel\": \"\", \"axisPlacement\": \"auto\", \"barAlignment\": 0, \"drawStyle\": \"line\", \"fillOpacity\": 0, \"gradientMode\": \"none\", \"hideFrom\": { \"legend\": false, \"tooltip\": false, \"viz\": false }, \"lineInterpolation\": \"linear\", \"lineWidth\": 1, \"pointSize\": 5, \"scaleDistribution\": { \"type\": \"linear\" }, \"showPoints\": \"auto\", \"spanNulls\": false, \"stacking\": { \"group\": \"A\", \"mode\": \"none\" }, \"thresholdsStyle\": { \"mode\": \"off\" } }, \"mappings\": [], \"thresholds\": { \"mode\": \"absolute\", \"steps\": [ { \"color\": \"green\", \"value\": null }, { \"color\": \"red\", \"value\": 80 } ] } }, \"overrides\": [ { \"__systemRef\": \"hideSeriesFrom\", \"matcher\": { \"id\": \"byNames\", \"options\": { \"mode\": \"exclude\", \"names\": [ \"dam_free_workers{app=\\\"dx-digital-asset-management\\\", app_kubernetes_io_instance=\\\"dx\\\", app_kubernetes_io_managed_by=\\\"Helm\\\", app_kubernetes_io_name=\\\"hcl-dx-deployment\\\", app_kubernetes_io_version=\\\"95_CF197\\\", controller_revision_hash=\\\"dx-digital-asset-management-554ccc9d7b\\\", helm_sh_chart=\\\"hcl-dx-deployment-2.0.0\\\", instance=\\\"10.42.0.75:3000\\\", job=\\\"kubernetes-pods\\\", kubernetes_namespace=\\\"prom\\\", kubernetes_pod_name=\\\"dx-digital-asset-management-0\\\", release=\\\"dx\\\", statefulset_kubernetes_io_pod_name=\\\"dx-digital-asset-management-0\\\"}\" ], \"prefix\": \"All except:\", \"readOnly\": true } }, \"properties\": [ { \"id\": \"custom.hideFrom\", \"value\": { \"legend\": false, \"tooltip\": false, \"viz\": true } } ] } ] }, \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 0 }, \"id\": 4, \"options\": { \"legend\": { \"calcs\": [], \"displayMode\": \"list\", \"placement\": \"bottom\" }, \"tooltip\": { \"mode\": \"single\" } }, \"targets\": [ { \"exemplar\": true, \"expr\": \"dam_free_workers\", \"interval\": \"\", \"legendFormat\": \"\", \"refId\": \"A\" } ], \"title\": \"DAM Free Workers\", \"type\": \"timeseries\" }, { \"datasource\": null, \"description\": \"\", \"fieldConfig\": { \"defaults\": { \"color\": { \"mode\": \"palette-classic\" }, \"custom\": { \"axisLabel\": \"\", \"axisPlacement\": \"auto\", \"barAlignment\": 0, \"drawStyle\": \"line\", \"fillOpacity\": 0, \"gradientMode\": \"none\", \"hideFrom\": { \"legend\": false, \"tooltip\": false, \"viz\": false }, \"lineInterpolation\": \"linear\", \"lineWidth\": 1, \"pointSize\": 5, \"scaleDistribution\": { \"type\": \"linear\" }, \"showPoints\": \"auto\", \"spanNulls\": false, \"stacking\": { \"group\": \"A\", \"mode\": \"none\" }, \"thresholdsStyle\": { \"mode\": \"off\" } }, \"mappings\": [], \"thresholds\": { \"mode\": \"absolute\", \"steps\": [ { \"color\": \"green\", \"value\": null }, { \"color\": \"red\", \"value\": 80 } ] } }, \"overrides\": [] }, \"gridPos\": { \"h\": 8, \"w\": 12, \"x\": 0, \"y\": 8 }, \"id\": 2, \"options\": { \"legend\": { \"calcs\": [], \"displayMode\": \"list\", \"placement\": \"bottom\" }, \"tooltip\": { \"mode\": \"single\" } }, \"targets\": [ { \"exemplar\": true, \"expr\": \"dam_active_workers\", \"interval\": \"\", \"legendFormat\": \"\", \"refId\": \"A\" } ], \"title\": \"DAM Active Workers\", \"type\": \"timeseries\" } ], \"refresh\": \"10s\", \"schemaVersion\": 30, \"style\": \"dark\", \"tags\": [], \"templating\": { \"list\": [] }, \"time\": { \"from\": \"now-15m\", \"to\": \"now\" }, \"timepicker\": {}, \"timezone\": \"\", \"title\": \"New dashboard\", \"uid\": null, \"version\": 0 } The output looks like this:","title":"Configure a test dashboard for DAM"},{"location":"kube/Prometheus/k8s-next-nodejs-prom/#using-existing-grafana-dashboards","text":"There are also existing Grafana dashboards that can be leveraged. One that is related to NodeJS and prom-client can be found here Grafana Dashboard 11159 .","title":"Using existing Grafana Dashboards"},{"location":"kube/Prometheus/k8s-next-nodejs-prom/#conclusion","text":"The use of prom-client enables us to easily expose default NodeJS metrics as well as custom applications metrics that are bound to our application logic. The implementation is straight forward and the main effort lies within defining metrics that should be exposed. We will need to adjust our Helm charts to add the necessary annotations to all application Pods that expose metrics for prometheus. With that, prometheus will automatically scrape those applications and aggregate the metrics data. The NodeJS default metrics also allow for in-depth analysis on performance issues, that can be seen in monitoring of the NodeJS runtime.","title":"Conclusion"},{"location":"kube/Prometheus/k8s-next-postgres-prom/","text":"PoC Goals The goal is to evaluate how we can expose Prometheus compatible metrics in our Postgres persistence. The outcome of that PoC should be documented so it can be reused for a thorough implementation in our Helm deployment. The PoC will use the new persistence deployment with Pgpool and Repmgr (DX internally called \"dbHA\" ), but the described pattern can be used for the \"old\" persistence as well. Postgres Metrics A Postgres Prometheus exporter exists as a prebuilt Docker image as well as on Github . It connects to the database and exposes the Prometheus compatible metrics as an HTTP endpoint. The default metrics are described in the queries.yaml file. Custom metrics can be defined as a YAML file and attached when the exporter is started by using the extend.query-path flag . Adding a sidecar container to the Postgres Pod The preferred way to run the Exporter is to add it to the Postgres Pod as a sidecar container. The configuration below describes the basic setup using the public postgres-exporter image. spec: template: metadata: annotations: # Tell Prometheus to scrape the metrics and where to find them prometheus.io/path: \"/metrics\" prometheus.io/port: \"9187\" prometheus.io/scrape: \"true\" spec: containers: - name: \"persistence-node-metrics\" image: \"quay.io/prometheuscommunity/postgres-exporter\" env: - name: DATA_SOURCE_URI # If implemented like that, we should check fo a way to not hard-code the \"dxmediadb\" value: \"127.0.0.1:5432/dxmediadb?sslmode=disable\" - name: \"DATA_SOURCE_PASS\" valueFrom: secretKeyRef: key: \"password\" name: \"{{ .Release.Name }}-persistence-user\" - name: \"DATA_SOURCE_USER\" valueFrom: secretKeyRef: key: \"username\" name: \"{{ .Release.Name }}-persistence-user\" ports: - name: metrics containerPort: 9187 protocol: TCP Create Postgres Exporter image based on the UBI image When shipped as part of DX, the Postgres Exporter should be included as a separate image, based on the UBI image. We can use the published releases from Github and add it to the UBI image. # #################################################################### # Licensed Materials - Property of HCL # # # # Copyright HCL Technologies Ltd. 2021. All Rights Reserved. # # # # Note to US Government Users Restricted Rights: # # # # Use, duplication or disclosure restricted by GSA ADP Schedule # #################################################################### # ARG REPOSITORY_URL=\"quintana-docker-prod.artifactory.cwp.pnp-hcl.com\" ARG DOCKER_UBI_BASE_IMAGE=\"dx-build-output/common/dxubi:v1.0.0_8.4-205\" FROM $REPOSITORY_URL/$DOCKER_UBI_BASE_IMAGE ARG BUILD_LABEL ARG VERSION LABEL \"product\"=\"HCL Digital Experience Postgres exporter\" LABEL \"version\"=\"${VERSION}\" LABEL \"description\"=\"DX postgres exporter container\" LABEL \"io.k8s.description\"=\"DX postgres exporter container\" LABEL \"io.k8s.display-name\"=\"DX postgres exporter container\" LABEL \"summary\"=\"DX postgres exporter container\" LABEL \"name\"=\"dx-postgresql-exporter\" LABEL \"release\"=\"${BUILD_LABEL}\" LABEL \"maintainer\"=\"HCL Software\" LABEL \"vendor\"=\"HCL Software\" LABEL \"io.openshift.tags\"=\"hcl dx\" LABEL \"url\"=\"\" LABEL \"authoritative-source-url\"=\"\" MAINTAINER HCL Software RUN curl -LJO https://github.com/prometheus-community/postgres_exporter/releases/download/v0.10.0/postgres_exporter-0.10.0.linux-amd64.tar.gz && \\ tar -xvf postgres_exporter-0.10.0.linux-amd64.tar.gz && \\ rm postgres_exporter-0.10.0.linux-amd64.tar.gz && \\ mv postgres_exporter-0.10.0.linux-amd64/postgres_exporter /usr/bin/postgres_exporter && \\ rm -r postgres_exporter-0.10.0.linux-amd64 USER dx_user:dx_users EXPOSE 9187 ENTRYPOINT [\"/usr/bin/postgres_exporter\"] Pgpool Metrics Similar to the Postgres exporter, a docker image exists to export the metrics of Pgpool. The code and releases can be found on Github . The exposed metrics are described on the same page. As we are currently using Pgpool in version 4.1, some of the metrics are not available for the exporter. To use it to it's full potential, an upgrade of Pgpool to version 4.2 should be considered. Adding a sidecar container to the Pgpool Pod The preferred way to run the Exporter is to add it to the Pgpool Pod as a sidecar container. The configuration below describes the basic setup using the public pgpool-exporter image. spec: template: metadata: annotations: # Tell Prometheus to scrape the metrics and where to find them prometheus.io/path: \"/metrics\" prometheus.io/port: \"9719\" prometheus.io/scrape: \"true\" spec: containers: - name: \"persistence-connection-pool-stats\" image: pgpool/pgpool2_exporter env: - name: \"POSTGRES_USERNAME\" valueFrom: secretKeyRef: key: \"username\" name: \"{{ .Release.Name }}-persistence-user\" - name: \"POSTGRES_PASSWORD\" valueFrom: secretKeyRef: key: \"password\" name: \"{{ .Release.Name }}-persistence-user\" - name: PGPOOL_SERVICE value: \"localhost\" - name: PGPOOL_SERVICE_PORT value: \"5432\" ports: - name: metrics containerPort: 9719 protocol: TCP Create Pgpool Exporter image based on the UBI image When shipped as part of DX, the Pgpool Exporter should be included as a separate image, based on the UBI image. We can use the published releases from Github and add it to the UBI image. # #################################################################### # Licensed Materials - Property of HCL # # # # Copyright HCL Technologies Ltd. 2021. All Rights Reserved. # # # # Note to US Government Users Restricted Rights: # # # # Use, duplication or disclosure restricted by GSA ADP Schedule # #################################################################### # ARG REPOSITORY_URL=\"quintana-docker-prod.artifactory.cwp.pnp-hcl.com\" ARG DOCKER_UBI_BASE_IMAGE=\"dx-build-output/common/dxubi:v1.0.0_8.4-205\" FROM $REPOSITORY_URL/$DOCKER_UBI_BASE_IMAGE ARG BUILD_LABEL ARG VERSION LABEL \"product\"=\"HCL Digital Experience Pgpool exporter\" LABEL \"version\"=\"${VERSION}\" LABEL \"description\"=\"DX pgpool exporter container\" LABEL \"io.k8s.description\"=\"DX pgpool exporter container\" LABEL \"io.k8s.display-name\"=\"DX pgpool exporter container\" LABEL \"summary\"=\"DX pgpool exporter container\" LABEL \"name\"=\"dx-pgpool-exporter\" LABEL \"release\"=\"${BUILD_LABEL}\" LABEL \"maintainer\"=\"HCL Software\" LABEL \"vendor\"=\"HCL Software\" LABEL \"io.openshift.tags\"=\"hcl dx\" LABEL \"url\"=\"\" LABEL \"authoritative-source-url\"=\"\" MAINTAINER HCL Software ENV POSTGRES_USERNAME postgres ENV POSTGRES_PASSWORD postgres ENV PGPOOL_SERVICE localhost ENV PGPOOL_SERVICE_PORT 9999 RUN curl -LJO https://github.com/pgpool/pgpool2_exporter/releases/download/v1.0.0/pgpool2_exporter-1.0.0.linux-amd64.tar.gz && \\ tar -xvf pgpool2_exporter-1.0.0.linux-amd64.tar.gz && \\ rm pgpool2_exporter-1.0.0.linux-amd64.tar.gz && \\ mv pgpool2_exporter-1.0.0.linux-amd64/pgpool2_exporter /usr/bin/pgpool2_exporter && \\ rm -r pgpool2_exporter-1.0.0.linux-amd64 USER dx_user:dx_users EXPOSE 9719 ENTRYPOINT [\"/bin/sh\", \"-c\", \"export DATA_SOURCE_NAME=\\\"postgresql://${POSTGRES_USERNAME}:${POSTGRES_PASSWORD}@${PGPOOL_SERVICE}:${PGPOOL_SERVICE_PORT}/postgres?sslmode=disable\\\"; /usr/bin/pgpool2_exporter\"] Using existing Grafana Dashboards There are also existing Grafana dashboards that can be leveraged. One that is related to Postgres with Prometheuse can be found here Grafana Dashboard 9628 . For the Pgpool exporter, a predefined Dashboard is not available in the official Grafana Dashboard directory.","title":"Postgres Metrics"},{"location":"kube/Prometheus/k8s-next-postgres-prom/#poc-goals","text":"The goal is to evaluate how we can expose Prometheus compatible metrics in our Postgres persistence. The outcome of that PoC should be documented so it can be reused for a thorough implementation in our Helm deployment. The PoC will use the new persistence deployment with Pgpool and Repmgr (DX internally called \"dbHA\" ), but the described pattern can be used for the \"old\" persistence as well.","title":"PoC Goals"},{"location":"kube/Prometheus/k8s-next-postgres-prom/#postgres-metrics","text":"A Postgres Prometheus exporter exists as a prebuilt Docker image as well as on Github . It connects to the database and exposes the Prometheus compatible metrics as an HTTP endpoint. The default metrics are described in the queries.yaml file. Custom metrics can be defined as a YAML file and attached when the exporter is started by using the extend.query-path flag .","title":"Postgres Metrics"},{"location":"kube/Prometheus/k8s-next-postgres-prom/#adding-a-sidecar-container-to-the-postgres-pod","text":"The preferred way to run the Exporter is to add it to the Postgres Pod as a sidecar container. The configuration below describes the basic setup using the public postgres-exporter image. spec: template: metadata: annotations: # Tell Prometheus to scrape the metrics and where to find them prometheus.io/path: \"/metrics\" prometheus.io/port: \"9187\" prometheus.io/scrape: \"true\" spec: containers: - name: \"persistence-node-metrics\" image: \"quay.io/prometheuscommunity/postgres-exporter\" env: - name: DATA_SOURCE_URI # If implemented like that, we should check fo a way to not hard-code the \"dxmediadb\" value: \"127.0.0.1:5432/dxmediadb?sslmode=disable\" - name: \"DATA_SOURCE_PASS\" valueFrom: secretKeyRef: key: \"password\" name: \"{{ .Release.Name }}-persistence-user\" - name: \"DATA_SOURCE_USER\" valueFrom: secretKeyRef: key: \"username\" name: \"{{ .Release.Name }}-persistence-user\" ports: - name: metrics containerPort: 9187 protocol: TCP","title":"Adding a sidecar container to the Postgres Pod"},{"location":"kube/Prometheus/k8s-next-postgres-prom/#create-postgres-exporter-image-based-on-the-ubi-image","text":"When shipped as part of DX, the Postgres Exporter should be included as a separate image, based on the UBI image. We can use the published releases from Github and add it to the UBI image. # #################################################################### # Licensed Materials - Property of HCL # # # # Copyright HCL Technologies Ltd. 2021. All Rights Reserved. # # # # Note to US Government Users Restricted Rights: # # # # Use, duplication or disclosure restricted by GSA ADP Schedule # #################################################################### # ARG REPOSITORY_URL=\"quintana-docker-prod.artifactory.cwp.pnp-hcl.com\" ARG DOCKER_UBI_BASE_IMAGE=\"dx-build-output/common/dxubi:v1.0.0_8.4-205\" FROM $REPOSITORY_URL/$DOCKER_UBI_BASE_IMAGE ARG BUILD_LABEL ARG VERSION LABEL \"product\"=\"HCL Digital Experience Postgres exporter\" LABEL \"version\"=\"${VERSION}\" LABEL \"description\"=\"DX postgres exporter container\" LABEL \"io.k8s.description\"=\"DX postgres exporter container\" LABEL \"io.k8s.display-name\"=\"DX postgres exporter container\" LABEL \"summary\"=\"DX postgres exporter container\" LABEL \"name\"=\"dx-postgresql-exporter\" LABEL \"release\"=\"${BUILD_LABEL}\" LABEL \"maintainer\"=\"HCL Software\" LABEL \"vendor\"=\"HCL Software\" LABEL \"io.openshift.tags\"=\"hcl dx\" LABEL \"url\"=\"\" LABEL \"authoritative-source-url\"=\"\" MAINTAINER HCL Software RUN curl -LJO https://github.com/prometheus-community/postgres_exporter/releases/download/v0.10.0/postgres_exporter-0.10.0.linux-amd64.tar.gz && \\ tar -xvf postgres_exporter-0.10.0.linux-amd64.tar.gz && \\ rm postgres_exporter-0.10.0.linux-amd64.tar.gz && \\ mv postgres_exporter-0.10.0.linux-amd64/postgres_exporter /usr/bin/postgres_exporter && \\ rm -r postgres_exporter-0.10.0.linux-amd64 USER dx_user:dx_users EXPOSE 9187 ENTRYPOINT [\"/usr/bin/postgres_exporter\"]","title":"Create Postgres Exporter image based on the UBI image"},{"location":"kube/Prometheus/k8s-next-postgres-prom/#pgpool-metrics","text":"Similar to the Postgres exporter, a docker image exists to export the metrics of Pgpool. The code and releases can be found on Github . The exposed metrics are described on the same page. As we are currently using Pgpool in version 4.1, some of the metrics are not available for the exporter. To use it to it's full potential, an upgrade of Pgpool to version 4.2 should be considered.","title":"Pgpool Metrics"},{"location":"kube/Prometheus/k8s-next-postgres-prom/#adding-a-sidecar-container-to-the-pgpool-pod","text":"The preferred way to run the Exporter is to add it to the Pgpool Pod as a sidecar container. The configuration below describes the basic setup using the public pgpool-exporter image. spec: template: metadata: annotations: # Tell Prometheus to scrape the metrics and where to find them prometheus.io/path: \"/metrics\" prometheus.io/port: \"9719\" prometheus.io/scrape: \"true\" spec: containers: - name: \"persistence-connection-pool-stats\" image: pgpool/pgpool2_exporter env: - name: \"POSTGRES_USERNAME\" valueFrom: secretKeyRef: key: \"username\" name: \"{{ .Release.Name }}-persistence-user\" - name: \"POSTGRES_PASSWORD\" valueFrom: secretKeyRef: key: \"password\" name: \"{{ .Release.Name }}-persistence-user\" - name: PGPOOL_SERVICE value: \"localhost\" - name: PGPOOL_SERVICE_PORT value: \"5432\" ports: - name: metrics containerPort: 9719 protocol: TCP","title":"Adding a sidecar container to the Pgpool Pod"},{"location":"kube/Prometheus/k8s-next-postgres-prom/#create-pgpool-exporter-image-based-on-the-ubi-image","text":"When shipped as part of DX, the Pgpool Exporter should be included as a separate image, based on the UBI image. We can use the published releases from Github and add it to the UBI image. # #################################################################### # Licensed Materials - Property of HCL # # # # Copyright HCL Technologies Ltd. 2021. All Rights Reserved. # # # # Note to US Government Users Restricted Rights: # # # # Use, duplication or disclosure restricted by GSA ADP Schedule # #################################################################### # ARG REPOSITORY_URL=\"quintana-docker-prod.artifactory.cwp.pnp-hcl.com\" ARG DOCKER_UBI_BASE_IMAGE=\"dx-build-output/common/dxubi:v1.0.0_8.4-205\" FROM $REPOSITORY_URL/$DOCKER_UBI_BASE_IMAGE ARG BUILD_LABEL ARG VERSION LABEL \"product\"=\"HCL Digital Experience Pgpool exporter\" LABEL \"version\"=\"${VERSION}\" LABEL \"description\"=\"DX pgpool exporter container\" LABEL \"io.k8s.description\"=\"DX pgpool exporter container\" LABEL \"io.k8s.display-name\"=\"DX pgpool exporter container\" LABEL \"summary\"=\"DX pgpool exporter container\" LABEL \"name\"=\"dx-pgpool-exporter\" LABEL \"release\"=\"${BUILD_LABEL}\" LABEL \"maintainer\"=\"HCL Software\" LABEL \"vendor\"=\"HCL Software\" LABEL \"io.openshift.tags\"=\"hcl dx\" LABEL \"url\"=\"\" LABEL \"authoritative-source-url\"=\"\" MAINTAINER HCL Software ENV POSTGRES_USERNAME postgres ENV POSTGRES_PASSWORD postgres ENV PGPOOL_SERVICE localhost ENV PGPOOL_SERVICE_PORT 9999 RUN curl -LJO https://github.com/pgpool/pgpool2_exporter/releases/download/v1.0.0/pgpool2_exporter-1.0.0.linux-amd64.tar.gz && \\ tar -xvf pgpool2_exporter-1.0.0.linux-amd64.tar.gz && \\ rm pgpool2_exporter-1.0.0.linux-amd64.tar.gz && \\ mv pgpool2_exporter-1.0.0.linux-amd64/pgpool2_exporter /usr/bin/pgpool2_exporter && \\ rm -r pgpool2_exporter-1.0.0.linux-amd64 USER dx_user:dx_users EXPOSE 9719 ENTRYPOINT [\"/bin/sh\", \"-c\", \"export DATA_SOURCE_NAME=\\\"postgresql://${POSTGRES_USERNAME}:${POSTGRES_PASSWORD}@${PGPOOL_SERVICE}:${PGPOOL_SERVICE_PORT}/postgres?sslmode=disable\\\"; /usr/bin/pgpool2_exporter\"]","title":"Create Pgpool Exporter image based on the UBI image"},{"location":"kube/Prometheus/k8s-next-postgres-prom/#using-existing-grafana-dashboards","text":"There are also existing Grafana dashboards that can be leveraged. One that is related to Postgres with Prometheuse can be found here Grafana Dashboard 9628 . For the Pgpool exporter, a predefined Dashboard is not available in the official Grafana Dashboard directory.","title":"Using existing Grafana Dashboards"},{"location":"kube/Prometheus/k8s-next-prom/","text":"Prometheus is data aggregation tool that is widely used, not only for Kubernetes deployments, but monitoring in general. Prometheus can be configured to pull data from defined data-sources. That pull process is called scraping and happens in configurable intervals. The service/application exposing metrics for Prometheus can often be found under the term exporter , as it exports the values of specific metrics for Prometheus. How is Prometheus scraping data structured? The data format of Prometheus scraping data is simple, it consists of key/value pairs that will be consumed by Prometheus. Those data values can have different types, e.g. Counter or Gauges. There is a good documentation on how exporter data is structured: Prometheus exporter . Install Prometheus and Grafana There are two ways of installing Prometheus and Grafana. Please follow EITHER 1 OR 2: 1. Install Prometheus Operator and Grafana stack The following process describes the installation of the kube-prometheus-stack helm chart that includes: The Prometheus Operator Highly available Prometheus Highly available Alertmanager Prometheus node-exporter Prometheus Adapter for Kubernetes Metrics APIs kube-state-metrics Grafana This helm chart is based on the kube-prometheus repository, which collects: [...] Kubernetes manifests, Grafana dashboards, and Prometheus rules combined with documentation and scripts to provide easy to operate end-to-end Kubernetes cluster monitoring with Prometheus using the Prometheus Operator. This stack comes with a set of tools to monitor the Kubernetes cluster as well as pre-installed Grafana dashboards for visualization. Add the prometheus-community repository to Helm. helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update Then deploy the chart with helm install . helm install -n <namespace> <release name> prometheus-community/kube-prometheus-stack -f <custom-values.yaml> Example: helm install -n dxns prometheus-stack prometheus-community/kube-prometheus-stack -f custom-kube-prometheus-stack.yaml The components of the helm chart can be configured or disabled by adjusting the custom helm values. For a full set of values that can be configured, please follow the configuration section in the repository of the helm chart. grafana: adminPassword: \"prom-operator\" service: type: \"NodePort\" nodePort: 32767 # range 30000-32767 prometheus: service: type: \"NodePort\" nodePort: 32766 # range 30000-32767 prometheusSpec: serviceMonitorSelectorNilUsesHelmValues: false To access and test Prometheus and Grafana, we expose them on two ports and assign an admin password for Grafana. Setting the serviceMonitorSelectorNilUsesHelmValues parameter to false makes sure that the ServiceMonitors are discovered by Prometheus. 2. Install Prometheus (non-operator) To install prometheus, we use Helm. Add Prometheus Helm Chart Repo to Helm: helm repo add prometheus-community https://prometheus-community.github.io/helm-charts We can extract the default values via Helm: helm show values prometheus-community/prometheus > prom-values.yaml We'll use the following values for a simple PoC deployment of Prometheus, disabling persistence and additional services. serviceAccounts: alertmanager: create: false nodeExporter: create: false alertmanager: enabled: false nodeExporter: enabled: false server: enabled: true persistentVolume: enabled: false service: type: NodePort pushgateway: enabled: false Install the Prometheus Application: helm install prometheus prometheus-community/prometheus -n prom -f prom-values.yaml Find the NodePort that is used and access Prometheus: NODEPORT=$(kubectl get -o jsonpath=\"{.spec.ports[0].nodePort}\" services prometheus-server -n prom) echo $NODEPORT You can now access Prometheus using your Browser: http://<NODE_IP>:<NODE_PORT> Deploy Grafana Add Grafana repository to Helm. helm repo add grafana https://grafana.github.io/helm-charts We can extract the default values via Helm: helm show values grafana/grafana > grafana-values.yaml Use the following custom values to configure the Service as NodePort. Persistence is disabled per default. service: enabled: true type: NodePort port: 80 targetPort: 3000 portName: service Install Grafana: helm install grafana -n prom grafana/grafana -f grafana-values.yaml Get the NodePort of Grafana: NODEPORT=$(kubectl get -o jsonpath=\"{.spec.ports[0].nodePort}\" services grafana -n prom) echo $NODEPORT Get the adminn password: kubectl get secret --namespace prom grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echos You can now login with the user admin and the returned password. http://<NODE_IP>:<NODE_PORT> Configure a datasource Navigate to Configuration and then go to DataSources . Click on Add data source and select Prometheus . Add the server url http://prometheus-server . Click on Save & Test . Using existing Grafana Dashboards There are existing Grafana dashboards that can be leveraged. They can be found on the Grafana Website DX metrics in this setup To scrape and visualize the DX metrics in this setup, make sure all applications are configured in the custom-values.yaml with either: For Prometheus Operator yaml scrape: true prometheusDiscoveryType: \"serviceMonitor\" For Prometheus (non-operator) yaml scrape: true prometheusDiscoveryType: \"annotation\"","title":"Prometheus"},{"location":"kube/Prometheus/k8s-next-prom/#how-is-prometheus-scraping-data-structured","text":"The data format of Prometheus scraping data is simple, it consists of key/value pairs that will be consumed by Prometheus. Those data values can have different types, e.g. Counter or Gauges. There is a good documentation on how exporter data is structured: Prometheus exporter .","title":"How is Prometheus scraping data structured?"},{"location":"kube/Prometheus/k8s-next-prom/#install-prometheus-and-grafana","text":"There are two ways of installing Prometheus and Grafana. Please follow EITHER 1 OR 2:","title":"Install Prometheus and Grafana"},{"location":"kube/Prometheus/k8s-next-prom/#1-install-prometheus-operator-and-grafana-stack","text":"The following process describes the installation of the kube-prometheus-stack helm chart that includes: The Prometheus Operator Highly available Prometheus Highly available Alertmanager Prometheus node-exporter Prometheus Adapter for Kubernetes Metrics APIs kube-state-metrics Grafana This helm chart is based on the kube-prometheus repository, which collects: [...] Kubernetes manifests, Grafana dashboards, and Prometheus rules combined with documentation and scripts to provide easy to operate end-to-end Kubernetes cluster monitoring with Prometheus using the Prometheus Operator. This stack comes with a set of tools to monitor the Kubernetes cluster as well as pre-installed Grafana dashboards for visualization. Add the prometheus-community repository to Helm. helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update Then deploy the chart with helm install . helm install -n <namespace> <release name> prometheus-community/kube-prometheus-stack -f <custom-values.yaml> Example: helm install -n dxns prometheus-stack prometheus-community/kube-prometheus-stack -f custom-kube-prometheus-stack.yaml The components of the helm chart can be configured or disabled by adjusting the custom helm values. For a full set of values that can be configured, please follow the configuration section in the repository of the helm chart. grafana: adminPassword: \"prom-operator\" service: type: \"NodePort\" nodePort: 32767 # range 30000-32767 prometheus: service: type: \"NodePort\" nodePort: 32766 # range 30000-32767 prometheusSpec: serviceMonitorSelectorNilUsesHelmValues: false To access and test Prometheus and Grafana, we expose them on two ports and assign an admin password for Grafana. Setting the serviceMonitorSelectorNilUsesHelmValues parameter to false makes sure that the ServiceMonitors are discovered by Prometheus.","title":"1. Install Prometheus Operator and Grafana stack"},{"location":"kube/Prometheus/k8s-next-prom/#2-install-prometheus-non-operator","text":"To install prometheus, we use Helm. Add Prometheus Helm Chart Repo to Helm: helm repo add prometheus-community https://prometheus-community.github.io/helm-charts We can extract the default values via Helm: helm show values prometheus-community/prometheus > prom-values.yaml We'll use the following values for a simple PoC deployment of Prometheus, disabling persistence and additional services. serviceAccounts: alertmanager: create: false nodeExporter: create: false alertmanager: enabled: false nodeExporter: enabled: false server: enabled: true persistentVolume: enabled: false service: type: NodePort pushgateway: enabled: false Install the Prometheus Application: helm install prometheus prometheus-community/prometheus -n prom -f prom-values.yaml Find the NodePort that is used and access Prometheus: NODEPORT=$(kubectl get -o jsonpath=\"{.spec.ports[0].nodePort}\" services prometheus-server -n prom) echo $NODEPORT You can now access Prometheus using your Browser: http://<NODE_IP>:<NODE_PORT>","title":"2. Install Prometheus (non-operator)"},{"location":"kube/Prometheus/k8s-next-prom/#deploy-grafana","text":"Add Grafana repository to Helm. helm repo add grafana https://grafana.github.io/helm-charts We can extract the default values via Helm: helm show values grafana/grafana > grafana-values.yaml Use the following custom values to configure the Service as NodePort. Persistence is disabled per default. service: enabled: true type: NodePort port: 80 targetPort: 3000 portName: service Install Grafana: helm install grafana -n prom grafana/grafana -f grafana-values.yaml Get the NodePort of Grafana: NODEPORT=$(kubectl get -o jsonpath=\"{.spec.ports[0].nodePort}\" services grafana -n prom) echo $NODEPORT Get the adminn password: kubectl get secret --namespace prom grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echos You can now login with the user admin and the returned password. http://<NODE_IP>:<NODE_PORT>","title":"Deploy Grafana"},{"location":"kube/Prometheus/k8s-next-prom/#configure-a-datasource","text":"Navigate to Configuration and then go to DataSources . Click on Add data source and select Prometheus . Add the server url http://prometheus-server . Click on Save & Test .","title":"Configure a datasource"},{"location":"kube/Prometheus/k8s-next-prom/#using-existing-grafana-dashboards","text":"There are existing Grafana dashboards that can be leveraged. They can be found on the Grafana Website","title":"Using existing Grafana Dashboards"},{"location":"kube/Prometheus/k8s-next-prom/#dx-metrics-in-this-setup","text":"To scrape and visualize the DX metrics in this setup, make sure all applications are configured in the custom-values.yaml with either: For Prometheus Operator yaml scrape: true prometheusDiscoveryType: \"serviceMonitor\" For Prometheus (non-operator) yaml scrape: true prometheusDiscoveryType: \"annotation\"","title":"DX metrics in this setup"},{"location":"what%27s-new/new_cf17/","text":"What's new in CF17? Combined Cumulative Fix (CF17) includes new software fixes for the latest version of HCL Digital Experience. Go to the HCL Software Support Site for the list of software fixes for HCL Digital Experience. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. Also, see the following link for Portal maintenance guidance: HCL Digital Experience Roadmap: Applying maintenance Parent topic: Latest Combined CF and Container updates","title":"What's new in CF17?"},{"location":"what%27s-new/new_cf17/#whats-new-in-cf17","text":"Combined Cumulative Fix (CF17) includes new software fixes for the latest version of HCL Digital Experience. Go to the HCL Software Support Site for the list of software fixes for HCL Digital Experience. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. Also, see the following link for Portal maintenance guidance: HCL Digital Experience Roadmap: Applying maintenance Parent topic: Latest Combined CF and Container updates","title":"What's new in CF17?"},{"location":"what%27s-new/new_cf171/","text":"What's new in CF171? Containers The Container Update releases include new features and updates for HCL Digital Experience 9.5 container deployments. Password override in Docker Added option to override password in Docker. See Docker deployment . Password override in OpenShift Added option to override password in OpenShift. See OpenShift deployment . Support for Kubernetes as verified in Amazon Elastic Container Service for Kubernetes (EKS) Added support for Kubernetes on AWS EKS. See Deploy HCL Digital Experience 9.5 Container to Amazon EKS . Support for Auto-scaling and Route configuration Added support for auto-scaling based on available CPU and memory utilization and route configuration. See Customizing the container deployment . Downloading DX products and accessing Customer Support You can go to the HCL Software Licensing Portal to access and download product software. See the Step-by-step guide to downloading DX products and accessing Customer Support for more information. Parent topic: Container Update releases","title":"What's new in CF171? Containers"},{"location":"what%27s-new/new_cf171/#whats-new-in-cf171-containers","text":"The Container Update releases include new features and updates for HCL Digital Experience 9.5 container deployments.","title":"What's new in CF171? Containers"},{"location":"what%27s-new/new_cf171/#password-override-in-docker","text":"Added option to override password in Docker. See Docker deployment .","title":"Password override in Docker"},{"location":"what%27s-new/new_cf171/#password-override-in-openshift","text":"Added option to override password in OpenShift. See OpenShift deployment .","title":"Password override in OpenShift"},{"location":"what%27s-new/new_cf171/#support-for-kubernetes-as-verified-in-amazon-elastic-container-service-for-kubernetes-eks","text":"Added support for Kubernetes on AWS EKS. See Deploy HCL Digital Experience 9.5 Container to Amazon EKS .","title":"Support for Kubernetes as verified in Amazon Elastic Container Service for Kubernetes (EKS)"},{"location":"what%27s-new/new_cf171/#support-for-auto-scaling-and-route-configuration","text":"Added support for auto-scaling based on available CPU and memory utilization and route configuration. See Customizing the container deployment .","title":"Support for Auto-scaling and Route configuration"},{"location":"what%27s-new/new_cf171/#downloading-dx-products-and-accessing-customer-support","text":"You can go to the HCL Software Licensing Portal to access and download product software. See the Step-by-step guide to downloading DX products and accessing Customer Support for more information. Parent topic: Container Update releases","title":"Downloading DX products and accessing Customer Support"},{"location":"what%27s-new/new_cf172/","text":"What's new in CF172? Containers The Container Update release include new features and updates for HCL Digital Experience 9.5 container deployments, including DX tool, Support and API updates. Web Content Manager (WCM) Support Tools The HCL WCM Support Tools portlet is provided to investigate and analyze information related to the WCM Java Content Repository (JCR) nodes. The WCM Support Tools Portlet is provided in the HCL Digital Experience 9.5 CF172 Container Update release, and is accessible from the standard Digital Experience administration panel in the CF172 release. See HCL Web Content Manager Support Tools for details. New Web Content Query Parameter APIs New Web Content Query Parameter APIs are added in HCL Digital Experience 9.5 CF172. See REST Query service for web content for details. New Enhanced Content Template API The Enhanced Web Content Manager Content Template API adds the ability to retrieve the configuration details of content template elements. See How to retrieve the settings of the elements for content templates by using REST for details. Updated HCL Digital Experience 9.5 platform support statements Read the updates to HCL Digital Experience 9.5 detailed system support statements published to the HCL Digital Experience Support site. See This HCL Software Support article for details. Downloading DX products and accessing Customer Support You can go to the HCL Software Licensing Portal to access and download product software. See the Step-by-step guide to downloading DX products and accessing Customer Support for more information. Parent topic: Container Update releases","title":"What's new in CF172? Containers"},{"location":"what%27s-new/new_cf172/#whats-new-in-cf172-containers","text":"The Container Update release include new features and updates for HCL Digital Experience 9.5 container deployments, including DX tool, Support and API updates.","title":"What's new in CF172? Containers"},{"location":"what%27s-new/new_cf172/#web-content-manager-wcm-support-tools","text":"The HCL WCM Support Tools portlet is provided to investigate and analyze information related to the WCM Java Content Repository (JCR) nodes. The WCM Support Tools Portlet is provided in the HCL Digital Experience 9.5 CF172 Container Update release, and is accessible from the standard Digital Experience administration panel in the CF172 release. See HCL Web Content Manager Support Tools for details.","title":"Web Content Manager (WCM) Support Tools"},{"location":"what%27s-new/new_cf172/#new-web-content-query-parameter-apis","text":"New Web Content Query Parameter APIs are added in HCL Digital Experience 9.5 CF172. See REST Query service for web content for details.","title":"New Web Content Query Parameter APIs"},{"location":"what%27s-new/new_cf172/#new-enhanced-content-template-api","text":"The Enhanced Web Content Manager Content Template API adds the ability to retrieve the configuration details of content template elements. See How to retrieve the settings of the elements for content templates by using REST for details.","title":"New Enhanced Content Template API"},{"location":"what%27s-new/new_cf172/#updated-hcl-digital-experience-95-platform-support-statements","text":"Read the updates to HCL Digital Experience 9.5 detailed system support statements published to the HCL Digital Experience Support site. See This HCL Software Support article for details.","title":"Updated HCL Digital Experience 9.5 platform support statements"},{"location":"what%27s-new/new_cf172/#downloading-dx-products-and-accessing-customer-support","text":"You can go to the HCL Software Licensing Portal to access and download product software. See the Step-by-step guide to downloading DX products and accessing Customer Support for more information. Parent topic: Container Update releases","title":"Downloading DX products and accessing Customer Support"},{"location":"what%27s-new/new_cf173/","text":"What's new in CF173? Containers This HCL Digital Experience 9.5 Container Update release include new WCM REST APIs, and Tech Previews for Content Composer and Digital Asset Management, and more. Web Content Manager Mirror syndication - Disable full build option An option to disable the rebuild with the WCM mirror syndication option is now available. This option can be set using the WCM Configuration service on the syndicator. See Manually syndicating items . New WCM Restore Version REST API The Restore version API supports restoring content versions to a previous level. See How to use REST with Versions . New Enhanced WCM Content Template API Element Configuration The Enhanced Content Template API Element Configuration Updates allows the configuration of template elements to be updated. See How to set default content values for content templates by using REST . New WCM Export Digital Asset Management references API The Web Content Manager Export DAM references API REST service can be used to retrieve content or components with references to externally managed resources, using the Digital Asset Manager plugin. See How to use REST with content items . New Experience API samples Two new samples are provided for use with the HCL Digital Experience 9.5 Experience API, supporting Sample login and content update process flow, and Get roles with authentication functions. See the Experience API Sample Calls . New HCL Content Composer \u2013 Tech Preview in HCL Digital Experience 9.5 CF173 Content Composer delivers simplified processes for creating and managing Digital Experience site content. Users can access a Tech Preview of the Content Composer features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 CF173 Content Composer \u2013 Tech Preview for details. New Digital Asset Management \u2013 Tech Preview in HCL Digital Experience 9.5 CF173 Digital Asset Management delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access a Tech Preview of the Digital Asset Management features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 CF173 Digital Asset Management \u2013 Tech Preview for details. Downloading DX products and accessing Customer Support The HCL Digital Experience 9.5 detailed system support statements are updated and published on the HCL Digital Experience Support site. You can go to the HCL Software Licensing Portal to access and download product software. For more information, see the Step-by-step guide to downloading DX products and accessing Customer Support . Parent topic: Container Update releases","title":"What's new in CF173? Containers"},{"location":"what%27s-new/new_cf173/#whats-new-in-cf173-containers","text":"This HCL Digital Experience 9.5 Container Update release include new WCM REST APIs, and Tech Previews for Content Composer and Digital Asset Management, and more.","title":"What's new in CF173? Containers"},{"location":"what%27s-new/new_cf173/#web-content-manager-mirror-syndication-disable-full-build-option","text":"An option to disable the rebuild with the WCM mirror syndication option is now available. This option can be set using the WCM Configuration service on the syndicator. See Manually syndicating items .","title":"Web Content Manager Mirror syndication - Disable full build option"},{"location":"what%27s-new/new_cf173/#new-wcm-restore-version-rest-api","text":"The Restore version API supports restoring content versions to a previous level. See How to use REST with Versions .","title":"New WCM Restore Version REST API"},{"location":"what%27s-new/new_cf173/#new-enhanced-wcm-content-template-api-element-configuration","text":"The Enhanced Content Template API Element Configuration Updates allows the configuration of template elements to be updated. See How to set default content values for content templates by using REST .","title":"New Enhanced WCM Content Template API Element Configuration"},{"location":"what%27s-new/new_cf173/#new-wcm-export-digital-asset-management-references-api","text":"The Web Content Manager Export DAM references API REST service can be used to retrieve content or components with references to externally managed resources, using the Digital Asset Manager plugin. See How to use REST with content items .","title":"New WCM Export Digital Asset Management references API"},{"location":"what%27s-new/new_cf173/#new-experience-api-samples","text":"Two new samples are provided for use with the HCL Digital Experience 9.5 Experience API, supporting Sample login and content update process flow, and Get roles with authentication functions. See the Experience API Sample Calls .","title":"New Experience API samples"},{"location":"what%27s-new/new_cf173/#new-hcl-content-composer-tech-preview-in-hcl-digital-experience-95-cf173","text":"Content Composer delivers simplified processes for creating and managing Digital Experience site content. Users can access a Tech Preview of the Content Composer features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 CF173 Content Composer \u2013 Tech Preview for details.","title":"New HCL Content Composer \u2013 Tech Preview in HCL Digital Experience 9.5 CF173"},{"location":"what%27s-new/new_cf173/#new-digital-asset-management-tech-preview-in-hcl-digital-experience-95-cf173","text":"Digital Asset Management delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access a Tech Preview of the Digital Asset Management features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 CF173 Digital Asset Management \u2013 Tech Preview for details.","title":"New Digital Asset Management \u2013 Tech Preview in HCL Digital Experience 9.5 CF173"},{"location":"what%27s-new/new_cf173/#downloading-dx-products-and-accessing-customer-support","text":"The HCL Digital Experience 9.5 detailed system support statements are updated and published on the HCL Digital Experience Support site. You can go to the HCL Software Licensing Portal to access and download product software. For more information, see the Step-by-step guide to downloading DX products and accessing Customer Support . Parent topic: Container Update releases","title":"Downloading DX products and accessing Customer Support"},{"location":"what%27s-new/new_cf18/","text":"What's new in CF18? Containers This HCL Digital Experience 9.5 Container Update release includes new WCM REST APIs, updated Tech Preview releases of Content Composer and Digital Asset Management, Rich Text Editor and Java Server Faces bridge updates, and more. Go to the HCL Software Support Site for the list of software fixes, including CF18. See What's New in CF18 for HCL Digital Experience for a list of updates in this release. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. JavaServer Faces (JSF) Bridge With the HCL Digital Experience CF18 Container and CF update, an updated JSF Portlet Bridge is introduced and installed in the HCL Digital Experience software. Customers installing HCL Digital Experience 8.5, 9, or 9.5 CF18 or Container Update release CF18, and using an IBM WebSphere Application Server Network Deployment version later than 8.5.5.17 or 9.0.5.2 can utilize the new JSF Portlet Bridge. See JavaServer Faces implementation for more information. Apply Content Template REST API The Apply Content Template API allows a developer to apply a content template to a set of specific content items or all content items of a certain template. It is equivalent to the functionality in the user interface. See How to set default content values for content templates by using REST for details. Enhanced Content Template API The Enhanced Web Content Manager Content Template API adds the ability to retrieve the configuration details of content template elements. See How to retrieve the settings of the elements for content templates by using REST for details. Rich Text Editor Textbox I/O Updates Updates are provided for the Textbox I/O Rich Text Editor, requiring use of Java 8. See Rich text editor toolbar configuration options for details. Sample Guidance to set Storage Class and Volume to deploy HCL Digital Experience 9.5 Containers to Amazon Elastic Kubernetes Service (EKS) and Red Hat OpenShift Additional \"Sample Storage Class and Volume\" guidance is available to HCL Digital Experience 9.5 container administrators. See Sample Storage Class and Volume topic for details. HCL Content Composer \u2013 Tech Preview for HCL Digital Experience 9.5 CF173 or later Content Composer delivers simplified processes for creating and managing Digital Experience site content. Users can access a Tech Preview of the Content Composer features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 Content Composer \u2013 Tech Preview for details. Digital Asset Management \u2013 Tech Preview for HCL Digital Experience 9.5 CF173 or later Digital Asset Management delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access a Tech Preview of the Digital Asset Management features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 Digital Asset Management \u2013 Tech Preview for details. Updated HCL Digital Experience 9.5 platform support statements See the updates to the HCL Digital Experience 9.5 detailed system support statements published to the HCL Digital Experience Support site. Parent topic: Container Update releases","title":"What's new in CF18? Containers"},{"location":"what%27s-new/new_cf18/#whats-new-in-cf18-containers","text":"This HCL Digital Experience 9.5 Container Update release includes new WCM REST APIs, updated Tech Preview releases of Content Composer and Digital Asset Management, Rich Text Editor and Java Server Faces bridge updates, and more. Go to the HCL Software Support Site for the list of software fixes, including CF18. See What's New in CF18 for HCL Digital Experience for a list of updates in this release. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information.","title":"What's new in CF18? Containers"},{"location":"what%27s-new/new_cf18/#javaserver-faces-jsf-bridge","text":"With the HCL Digital Experience CF18 Container and CF update, an updated JSF Portlet Bridge is introduced and installed in the HCL Digital Experience software. Customers installing HCL Digital Experience 8.5, 9, or 9.5 CF18 or Container Update release CF18, and using an IBM WebSphere Application Server Network Deployment version later than 8.5.5.17 or 9.0.5.2 can utilize the new JSF Portlet Bridge. See JavaServer Faces implementation for more information.","title":"JavaServer Faces (JSF) Bridge"},{"location":"what%27s-new/new_cf18/#apply-content-template-rest-api","text":"The Apply Content Template API allows a developer to apply a content template to a set of specific content items or all content items of a certain template. It is equivalent to the functionality in the user interface. See How to set default content values for content templates by using REST for details.","title":"Apply Content Template REST API"},{"location":"what%27s-new/new_cf18/#enhanced-content-template-api","text":"The Enhanced Web Content Manager Content Template API adds the ability to retrieve the configuration details of content template elements. See How to retrieve the settings of the elements for content templates by using REST for details.","title":"Enhanced Content Template API"},{"location":"what%27s-new/new_cf18/#rich-text-editor-textbox-io-updates","text":"Updates are provided for the Textbox I/O Rich Text Editor, requiring use of Java 8. See Rich text editor toolbar configuration options for details.","title":"Rich Text Editor Textbox I/O Updates"},{"location":"what%27s-new/new_cf18/#sample-guidance-to-set-storage-class-and-volume-to-deploy-hcl-digital-experience-95-containers-to-amazon-elastic-kubernetes-service-eks-and-red-hat-openshift","text":"Additional \"Sample Storage Class and Volume\" guidance is available to HCL Digital Experience 9.5 container administrators. See Sample Storage Class and Volume topic for details.","title":"Sample Guidance to set Storage Class and Volume to deploy HCL Digital Experience 9.5 Containers to Amazon Elastic Kubernetes Service (EKS) and Red Hat OpenShift"},{"location":"what%27s-new/new_cf18/#hcl-content-composer-tech-preview-for-hcl-digital-experience-95-cf173-or-later","text":"Content Composer delivers simplified processes for creating and managing Digital Experience site content. Users can access a Tech Preview of the Content Composer features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 Content Composer \u2013 Tech Preview for details.","title":"HCL Content Composer \u2013 Tech Preview for HCL Digital Experience 9.5 CF173 or later"},{"location":"what%27s-new/new_cf18/#digital-asset-management-tech-preview-for-hcl-digital-experience-95-cf173-or-later","text":"Digital Asset Management delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access a Tech Preview of the Digital Asset Management features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 Digital Asset Management \u2013 Tech Preview for details.","title":"Digital Asset Management \u2013 Tech Preview for HCL Digital Experience 9.5 CF173 or later"},{"location":"what%27s-new/new_cf18/#updated-hcl-digital-experience-95-platform-support-statements","text":"See the updates to the HCL Digital Experience 9.5 detailed system support statements published to the HCL Digital Experience Support site. Parent topic: Container Update releases","title":"Updated HCL Digital Experience 9.5 platform support statements"},{"location":"what%27s-new/new_cf181/","text":"What's new in CF181? Containers This HCL Digital Experience 9.5 Container Update release includes new Production releases of Content Composer, Digital Asset Management, and Experience API, new WCM REST APIs, guidance to deploy OpenLDAP, Remote Search and Database transfer processes, and more. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. Content Composer Content Composer delivers simplified processes for creating and managing Digital Experience site content. Users can work with Content Composer features in HCL Digital Experience 9.5 Container Update CF181 and higher releases. See HCL Digital Experience 9.5 Content Composer for details. Digital Asset Management Digital Asset Management delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access the Digital Asset Management features in HCL Digital Experience 9.5 Container Update CF181 and higher releases. See HCL Digital Experience 9.5 Digital Asset Management for details. Experience API The HCL Experience API is a set of OpenAPI compliant REST APIs available for customers deploying HCL Digital Experience 9.5 containers on supported Kubernetes platforms. It supports the integration and management of HCL Digital Experience content and functionality to any digital channel using any front-end development framework. The HCL Experience API includes REST APIs that serve as a wrapper around previously published HCL Digital Experience HTTP based APIs. See HCL Experience API for details. OpenLDAP Container integration OpenLDAP Software is an open source implementation of the Lightweight Directory Access Protocol. The HCL Digital Experience 9.5 Container Update release CF181 and higher includes an OpenLDAP container and a customization of the operator to deploy and configure the LDAP container to the HCL Digital Experience 9.5 container deployment. See Configure the OpenLDAP container image to the HCL Digital Experience 9.5 Container Deployment for details. Transfer default Container database to IBM DB2 HCL Digital Experience 9.5 installs a copy of Derby as the default database. HCL Digital Experience administrators can apply guidance to transfer the default database configuration to IBM DB2, if preferred for use as the relational database for Digital Experience 9.5 Container deployment data. See Transfer Digital Experience 9.5 Container default database to IBM DB2 for details. Remote Search services Docker deployment To support search services when deployed to Docker, Digital Experience administrators can configure Remote search services. This will require some different setup and configuration steps than used to set up remote search on a non-Docker container platform. See Deploy Remote Search services on Docker for details. New Digital Experience WCM Workflow REST APIs Two new WCM REST APIs are added to handle Process Now and Remove Workflow from an item functionality. See How to use REST with drafts and workflows for details. New Web Content Manager Reference REST API The new WCM Content Manager Reference REST API can be used by developers to find references to a Web Content or Digital Asset Management item identified by its UUID. See How to use REST with content items for details. New Web Content Text Search REST API The Text Search REST API Content Authors search for free form text in the Web Content Manager JCR. It is equivalent to the functionality in the Web Content Manager user interface. See REST Query service for web content - Query parameters for details. New Digital Experience Core Configuration REST API The Digital Experience Core Configuration API allows developers to retrieve Digital Experience deployment configuration settings. See Generic reading by using REST services for Web Content Manager for details. Web Developer Toolkit The Web Developer toolkit for HCL Digital Experience provides the ability to sync themes, content and script portlets (also known as Script Applications). It is available on the HCL Digital Experience GitHub repository. See Web Developer Toolkit for details. Parent topic: Container Update releases","title":"What's new in CF181? Containers"},{"location":"what%27s-new/new_cf181/#whats-new-in-cf181-containers","text":"This HCL Digital Experience 9.5 Container Update release includes new Production releases of Content Composer, Digital Asset Management, and Experience API, new WCM REST APIs, guidance to deploy OpenLDAP, Remote Search and Database transfer processes, and more. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information.","title":"What's new in CF181? Containers"},{"location":"what%27s-new/new_cf181/#content-composer","text":"Content Composer delivers simplified processes for creating and managing Digital Experience site content. Users can work with Content Composer features in HCL Digital Experience 9.5 Container Update CF181 and higher releases. See HCL Digital Experience 9.5 Content Composer for details.","title":"Content Composer"},{"location":"what%27s-new/new_cf181/#digital-asset-management","text":"Digital Asset Management delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access the Digital Asset Management features in HCL Digital Experience 9.5 Container Update CF181 and higher releases. See HCL Digital Experience 9.5 Digital Asset Management for details.","title":"Digital Asset Management"},{"location":"what%27s-new/new_cf181/#experience-api","text":"The HCL Experience API is a set of OpenAPI compliant REST APIs available for customers deploying HCL Digital Experience 9.5 containers on supported Kubernetes platforms. It supports the integration and management of HCL Digital Experience content and functionality to any digital channel using any front-end development framework. The HCL Experience API includes REST APIs that serve as a wrapper around previously published HCL Digital Experience HTTP based APIs. See HCL Experience API for details.","title":"Experience API"},{"location":"what%27s-new/new_cf181/#openldap-container-integration","text":"OpenLDAP Software is an open source implementation of the Lightweight Directory Access Protocol. The HCL Digital Experience 9.5 Container Update release CF181 and higher includes an OpenLDAP container and a customization of the operator to deploy and configure the LDAP container to the HCL Digital Experience 9.5 container deployment. See Configure the OpenLDAP container image to the HCL Digital Experience 9.5 Container Deployment for details.","title":"OpenLDAP Container integration"},{"location":"what%27s-new/new_cf181/#transfer-default-container-database-to-ibm-db2","text":"HCL Digital Experience 9.5 installs a copy of Derby as the default database. HCL Digital Experience administrators can apply guidance to transfer the default database configuration to IBM DB2, if preferred for use as the relational database for Digital Experience 9.5 Container deployment data. See Transfer Digital Experience 9.5 Container default database to IBM DB2 for details.","title":"Transfer default Container database to IBM DB2"},{"location":"what%27s-new/new_cf181/#remote-search-services-docker-deployment","text":"To support search services when deployed to Docker, Digital Experience administrators can configure Remote search services. This will require some different setup and configuration steps than used to set up remote search on a non-Docker container platform. See Deploy Remote Search services on Docker for details.","title":"Remote Search services Docker deployment"},{"location":"what%27s-new/new_cf181/#new-digital-experience-wcm-workflow-rest-apis","text":"Two new WCM REST APIs are added to handle Process Now and Remove Workflow from an item functionality. See How to use REST with drafts and workflows for details.","title":"New Digital Experience WCM Workflow REST APIs"},{"location":"what%27s-new/new_cf181/#new-web-content-manager-reference-rest-api","text":"The new WCM Content Manager Reference REST API can be used by developers to find references to a Web Content or Digital Asset Management item identified by its UUID. See How to use REST with content items for details.","title":"New Web Content Manager Reference REST API"},{"location":"what%27s-new/new_cf181/#new-web-content-text-search-rest-api","text":"The Text Search REST API Content Authors search for free form text in the Web Content Manager JCR. It is equivalent to the functionality in the Web Content Manager user interface. See REST Query service for web content - Query parameters for details.","title":"New Web Content Text Search REST API"},{"location":"what%27s-new/new_cf181/#new-digital-experience-core-configuration-rest-api","text":"The Digital Experience Core Configuration API allows developers to retrieve Digital Experience deployment configuration settings. See Generic reading by using REST services for Web Content Manager for details.","title":"New Digital Experience Core Configuration REST API"},{"location":"what%27s-new/new_cf181/#web-developer-toolkit","text":"The Web Developer toolkit for HCL Digital Experience provides the ability to sync themes, content and script portlets (also known as Script Applications). It is available on the HCL Digital Experience GitHub repository. See Web Developer Toolkit for details. Parent topic: Container Update releases","title":"Web Developer Toolkit"},{"location":"what%27s-new/new_noncf18/","text":"What's new in CF18? This HCL Digital Experience 9.5 CF18 release includes new WCM REST APIs, updated releases of Content Composer and Digital Asset Management Tech Preview releases, Rich Text Editor and JavaServer Faces bridge updates, and more. Go to the HCL Software Support Site for the list of software fixes. See What's New in CF18 for HCL Digital Experience for a list of updates in this release. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. Web Content Manager (WCM) Support Tools The HCL WCM Support Tools portlet is provided to investigate and analyze information related to the WCM Java Content Repository (JCR) nodes. The WCM Support Tools Portlet is provided in the HCL Digital Experience 9.5 CF18 release, and is accessible from the standard Digital Experience administration panel. See HCL Web Content Manager Support Tools for details. JavaServer Faces (JSF) Bridge With the HCL Digital Experience CF18 Container and CF update, an updated JSF Portlet Bridge is introduced and installed in the HCL Digital Experience software. Customers installing HCL Digital Experience 8.5, 9.0, or 9.5 non-container CF18 or Container Update release CF18, and using an IBM WebSphere Application Server Network Deployment version later than 8.5.5.17 or 9.0.5.2 can utilize the new JSF Portlet Bridge. See JavaServer Faces implementation for more information. Apply Content Template REST API The Apply Content Template API allows a developer to apply a content template to a set of specific content items or all content items of a certain template. It is equivalent to the functionality in the user interface. See How to set default content values for content templates by using REST for details. Enhanced Content Template API The Enhanced Web Content Manager Content Template API adds the ability to retrieve the configuration details of content template elements. See How to retrieve the settings of the elements for content templates by using REST for details. Rich Text Editor Textbox I/O Updates Updates are provided for the Textbox I/O Rich Text Editor, requiring use of Java 8. See Rich text editor toolbar configuration options for details. Sample Guidance to set Storage Class and Volume to deploy HCL Digital Experience 9.5 Containers to Amazon Elastic Kubernetes Service (EKS) and Red Hat OpenShift Additional guidance for \"Sample Storage Class and Volume\" is available to HCL Digital Experience 9.5 container administrators. See Sample Storage Class and Volume topic for details. HCL Content Composer \u2013 Tech Preview for HCL Digital Experience 9.5 CF173 or higher release Content Composer delivers simplified processes for creating and managing Digital Experience site content. Users can access a Tech Preview of the Content Composer features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 Content Composer \u2013 Tech Preview for details. Digital Asset Management \u2013 Tech Preview for HCL Digital Experience 9.5 CF173 or higher release Digital Asset Management (DAM) delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access a Tech Preview of the Digital Asset Management features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 Digital Asset Management \u2013 Tech Preview for details. Updated HCL Digital Experience 9.5 platform support statements See the updates to the HCL Digital Experience 9.5 detailed system support statements published to the HCL Digital Experience Support site. Parent topic: Latest Combined CF and Container updates","title":"What's new in CF18?"},{"location":"what%27s-new/new_noncf18/#whats-new-in-cf18","text":"This HCL Digital Experience 9.5 CF18 release includes new WCM REST APIs, updated releases of Content Composer and Digital Asset Management Tech Preview releases, Rich Text Editor and JavaServer Faces bridge updates, and more. Go to the HCL Software Support Site for the list of software fixes. See What's New in CF18 for HCL Digital Experience for a list of updates in this release. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information.","title":"What's new in CF18?"},{"location":"what%27s-new/new_noncf18/#web-content-manager-wcm-support-tools","text":"The HCL WCM Support Tools portlet is provided to investigate and analyze information related to the WCM Java Content Repository (JCR) nodes. The WCM Support Tools Portlet is provided in the HCL Digital Experience 9.5 CF18 release, and is accessible from the standard Digital Experience administration panel. See HCL Web Content Manager Support Tools for details.","title":"Web Content Manager (WCM) Support Tools"},{"location":"what%27s-new/new_noncf18/#javaserver-faces-jsf-bridge","text":"With the HCL Digital Experience CF18 Container and CF update, an updated JSF Portlet Bridge is introduced and installed in the HCL Digital Experience software. Customers installing HCL Digital Experience 8.5, 9.0, or 9.5 non-container CF18 or Container Update release CF18, and using an IBM WebSphere Application Server Network Deployment version later than 8.5.5.17 or 9.0.5.2 can utilize the new JSF Portlet Bridge. See JavaServer Faces implementation for more information.","title":"JavaServer Faces (JSF) Bridge"},{"location":"what%27s-new/new_noncf18/#apply-content-template-rest-api","text":"The Apply Content Template API allows a developer to apply a content template to a set of specific content items or all content items of a certain template. It is equivalent to the functionality in the user interface. See How to set default content values for content templates by using REST for details.","title":"Apply Content Template REST API"},{"location":"what%27s-new/new_noncf18/#enhanced-content-template-api","text":"The Enhanced Web Content Manager Content Template API adds the ability to retrieve the configuration details of content template elements. See How to retrieve the settings of the elements for content templates by using REST for details.","title":"Enhanced Content Template API"},{"location":"what%27s-new/new_noncf18/#rich-text-editor-textbox-io-updates","text":"Updates are provided for the Textbox I/O Rich Text Editor, requiring use of Java 8. See Rich text editor toolbar configuration options for details.","title":"Rich Text Editor Textbox I/O Updates"},{"location":"what%27s-new/new_noncf18/#sample-guidance-to-set-storage-class-and-volume-to-deploy-hcl-digital-experience-95-containers-to-amazon-elastic-kubernetes-service-eks-and-red-hat-openshift","text":"Additional guidance for \"Sample Storage Class and Volume\" is available to HCL Digital Experience 9.5 container administrators. See Sample Storage Class and Volume topic for details.","title":"Sample Guidance to set Storage Class and Volume to deploy HCL Digital Experience 9.5 Containers to Amazon Elastic Kubernetes Service (EKS) and Red Hat OpenShift"},{"location":"what%27s-new/new_noncf18/#hcl-content-composer-tech-preview-for-hcl-digital-experience-95-cf173-or-higher-release","text":"Content Composer delivers simplified processes for creating and managing Digital Experience site content. Users can access a Tech Preview of the Content Composer features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 Content Composer \u2013 Tech Preview for details.","title":"HCL Content Composer \u2013 Tech Preview for HCL Digital Experience 9.5 CF173 or higher release"},{"location":"what%27s-new/new_noncf18/#digital-asset-management-tech-preview-for-hcl-digital-experience-95-cf173-or-higher-release","text":"Digital Asset Management (DAM) delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access a Tech Preview of the Digital Asset Management features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 Digital Asset Management \u2013 Tech Preview for details.","title":"Digital Asset Management \u2013 Tech Preview for HCL Digital Experience 9.5 CF173 or higher release"},{"location":"what%27s-new/new_noncf18/#updated-hcl-digital-experience-95-platform-support-statements","text":"See the updates to the HCL Digital Experience 9.5 detailed system support statements published to the HCL Digital Experience Support site. Parent topic: Latest Combined CF and Container updates","title":"Updated HCL Digital Experience 9.5 platform support statements"},{"location":"what%27s-new/new_noncf19/","text":"What's new with CF19? Combined Cumulative Fix (CF19) includes new features and software fixes for the latest version of HCL Digital Experience. This HCL Digital Experience 9.5 CF19 release includes new WCM REST APIs, Web Developer Toolkit, updated releases of Content Composer, Digital Asset Management and Experience API, Hybrid deployment support, Progressive Web Application delivery, Google Analytics integration, Mobile Preview, DXClient and DXConnect tooling supporting CICD release processes, new HCL Digital Experience \u2018How To\u2019 videos and more. Go to the HCL Software Support Site for the list of software fixes. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. Web Developer Toolkit The Web Developer toolkit for HCL Digital Experience provides the ability to sync themes, content and script portlets (also known as Script Applications). It is available on the HCL Digital Experience Github repository. See Web Developer Toolkit for details. Hybrid Deployment The HCL Digital Experience 9.5 Hybrid deployment and topics deliver capability to deploy and manage HCL Digital Experience 9.5 core Portal Server and Web Content Manager services on premises, and connect to cloud native components Digital Asset Management, Content Composer, Experience API and related services in a production environment. See the Hybrid Deployment topic for more information. Progressive Web Application support Develop support that adds native mobile application experience and performance to your web site using browser-based functionality. See the Progressive Web Application topic for more information. Google Analytics integration Learn how to set up integration of Digital Experience sites with Google Analytics and view the resulting web analytics tracking to assess the effectiveness of your DX site pages with end user audiences. See the Integrate Google Analytics with HCL Digital Experience topic for more information. Mobile Preview Use the Mobile Preview simulator to view the presentation of Digital Experience site page components on select mobile devices. See the Mobile Preview topic for more information. DXClient and DXConnect tooling supporting CICD release processes HCL Digital Experience 9.5 CF19 and higher includes a DXClient toolset, and DX Connect servlet that provides developers and administrators with an approach to deploy changes or improvements to the HCL Digital Experience platform,and automate processes in the development and delivery process. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. New Digital Experience REST APIs New HCL DX APIs are available with the HCL DX CF19 release: Web content image renditions interactions REST API Web Content Library Locale Query Workflow Comments API Using XML Access to export and import Digital Asset Management assets Web Content Manager Syndication REST APIs Process Now and Remove Workflow REST APIs Web Content Manager References REST API Web Content Text Search REST API Digital Experience Core Configuration REST API Web Content Manager Lock/Unlock API Create or update an Option Selection Element Search Component Results Display New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step by step guidance for HCL Digital Experience practitioners presented in several new videos. See the following HCL Digital Experience Help Center topics: Remote Search Service Transfer HCL Digital Experience 9.5 Container default database to IBM DB2 Create a web content library Virtual portals Backup and restore Configuration Wizard Combined Cumulative Fix Install Rich text editor toolbar configuration options Data collection and symptom analysis HCL Digital Experience Combined Cumulative Fix (CF) Installation How to manage syndicators and subscribers Deploy HCL Digital Experience 9.5 to Red Hat OpenShift Deploy Digital Asset Management, Content Composer and Experience API to Red Hat OpenShift Configure the OpenLDAP container image to the HCL DX 9.5 Container Deployment Parent topic: Latest Combined CF and Container updates","title":"What's new with CF19?"},{"location":"what%27s-new/new_noncf19/#whats-new-with-cf19","text":"Combined Cumulative Fix (CF19) includes new features and software fixes for the latest version of HCL Digital Experience. This HCL Digital Experience 9.5 CF19 release includes new WCM REST APIs, Web Developer Toolkit, updated releases of Content Composer, Digital Asset Management and Experience API, Hybrid deployment support, Progressive Web Application delivery, Google Analytics integration, Mobile Preview, DXClient and DXConnect tooling supporting CICD release processes, new HCL Digital Experience \u2018How To\u2019 videos and more. Go to the HCL Software Support Site for the list of software fixes. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information.","title":"What's new with CF19?"},{"location":"what%27s-new/new_noncf19/#web-developer-toolkit","text":"The Web Developer toolkit for HCL Digital Experience provides the ability to sync themes, content and script portlets (also known as Script Applications). It is available on the HCL Digital Experience Github repository. See Web Developer Toolkit for details.","title":"Web Developer Toolkit"},{"location":"what%27s-new/new_noncf19/#hybrid-deployment","text":"The HCL Digital Experience 9.5 Hybrid deployment and topics deliver capability to deploy and manage HCL Digital Experience 9.5 core Portal Server and Web Content Manager services on premises, and connect to cloud native components Digital Asset Management, Content Composer, Experience API and related services in a production environment. See the Hybrid Deployment topic for more information.","title":"Hybrid Deployment"},{"location":"what%27s-new/new_noncf19/#progressive-web-application-support","text":"Develop support that adds native mobile application experience and performance to your web site using browser-based functionality. See the Progressive Web Application topic for more information.","title":"Progressive Web Application support"},{"location":"what%27s-new/new_noncf19/#google-analytics-integration","text":"Learn how to set up integration of Digital Experience sites with Google Analytics and view the resulting web analytics tracking to assess the effectiveness of your DX site pages with end user audiences. See the Integrate Google Analytics with HCL Digital Experience topic for more information.","title":"Google Analytics integration"},{"location":"what%27s-new/new_noncf19/#mobile-preview","text":"Use the Mobile Preview simulator to view the presentation of Digital Experience site page components on select mobile devices. See the Mobile Preview topic for more information.","title":"Mobile Preview"},{"location":"what%27s-new/new_noncf19/#dxclient-and-dxconnect-tooling-supporting-cicd-release-processes","text":"HCL Digital Experience 9.5 CF19 and higher includes a DXClient toolset, and DX Connect servlet that provides developers and administrators with an approach to deploy changes or improvements to the HCL Digital Experience platform,and automate processes in the development and delivery process. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"DXClient and DXConnect tooling supporting CICD release processes"},{"location":"what%27s-new/new_noncf19/#new-digital-experience-rest-apis","text":"New HCL DX APIs are available with the HCL DX CF19 release: Web content image renditions interactions REST API Web Content Library Locale Query Workflow Comments API Using XML Access to export and import Digital Asset Management assets Web Content Manager Syndication REST APIs Process Now and Remove Workflow REST APIs Web Content Manager References REST API Web Content Text Search REST API Digital Experience Core Configuration REST API Web Content Manager Lock/Unlock API Create or update an Option Selection Element Search Component Results Display","title":"New Digital Experience REST APIs"},{"location":"what%27s-new/new_noncf19/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step by step guidance for HCL Digital Experience practitioners presented in several new videos. See the following HCL Digital Experience Help Center topics: Remote Search Service Transfer HCL Digital Experience 9.5 Container default database to IBM DB2 Create a web content library Virtual portals Backup and restore Configuration Wizard Combined Cumulative Fix Install Rich text editor toolbar configuration options Data collection and symptom analysis HCL Digital Experience Combined Cumulative Fix (CF) Installation How to manage syndicators and subscribers Deploy HCL Digital Experience 9.5 to Red Hat OpenShift Deploy Digital Asset Management, Content Composer and Experience API to Red Hat OpenShift Configure the OpenLDAP container image to the HCL DX 9.5 Container Deployment Parent topic: Latest Combined CF and Container updates","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"what%27s-new/new_noncf196/","text":"What's new in CF196? Combined Cumulative Fix (CF196) includes new features and software fixes for the latest version of HCL Digital Experience. Beginning with CF19 and Container Update release CF196, release updates for both on\u2013premises platforms and container deployments will be available. This HCL Digital Experience 9.5 CF196 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management and Experience API, Design Studio (Beta) for Container deployments, Theme Editor Portlet, Content Security Policy support, DXClient and DXConnect tooling supporting CICD release processes, Multilingual enhancements, HCL Unica Discover enablement, new HCL Digital Experience \u2018How To\u2019 videos and more. Go to the HCL Software Support Site for the list of software fixes. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. Note: For new capabilities that are available for HCL DX 9.5 CF196 Container Update deployments, see What's new in the CF196 Container Update release topic. Theme Editor Portlet The Theme Editor portlet is a new addition to HCL Digital Experience CF196 and higher release capabilities. The portlet allows an administrator to edit static theme resources in WebDAV without the use of a WebDAV client or tool. See the Theme Editor Portlet topic for more information. Web Content Manager Multilingual Solution Enhancements The HCL Web Content Manager Multilingual Solution is a set of tools used to manage translated versions Web Content Manager content for localized and regionalized websites. Beginning with the HCL Digital Experience 9.5 CF196 release, a new export and import capability allows you to support translation of the content of a library by exporting it into a format supported by a translation service and importing the translated content back into the content library using the DXClient tool. Support is also added to define a maximum field length, export changed contents from a library, and export to projects. A new option to switch the language in an HCL DX 9.5 Mobile view is also available. See the How to export and import WCM library content using DXClient and The Woodburn Studio demo site Help Center topics for more information. Enable Presentation of Locales in Friendly URLs Beginning with the HCL Digital Experience 9.5 Container Update CF196 release, enablement is provided to present friendly URLs with locale specific definitions when multi-lingual page versions are requested. This enablement can also improve SEO results when users search for language-specific DX page topics. See the Enabling presentation of locales in friendly URLs topic for more information. Change language presented in the HCL Digital Experience Theme Beginning with HCL DX CF196 release, you can switch the language presented in your Digital Experience theme. An example is provided in the Woodburn Studio demo site supporting presentations for French, Spanish, and English languages. See the How to switch the languages in the Digital Experience theme topic for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include a new DXClient Docker image, and new release artifact types supporting Shared Libraries, Obtain failed Syndication reports, Undeploy Themes, and Export/Import Web Content Manager Library, Deploy DX Core, Manage Syndicator, and Manage Subscriber tasks are provided in CF196. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. Web Content Manager Advanced Cache Options New options are available to flush the Web Content Manager Advanced cache, as a result of syndication operations, to help improve performance and reliability. See the Web Content Manager Cache Parameters topic for more information. Content Security Policy The Content-Security-Policy header is used by modern browsers to enhance the security of HCL Digital Experience site documents or web pages by allowing HCL Digital Experience administrators or developers declare which dynamic resources are allowed to load. With HCL Digital Experience Container Update CF192 and later releases, developers can apply platform support and guidance to update their DX sites to validate trusted sources before rendering pages to end users. See the Content Security Policy Help Center topic for more information. Enhanced Cross Origin Resource Sharing Configuration Enhanced Cross Origin Resource Sharing Configuration adds new options for HCL Digital Experience administrators to set configuration for CORS using a WP configuration service in the IBM WebSphere Application Server resource environment provider. See Enhanced Cross Origin Resource Sharing Configuration for more information. HCL Digital Experience 9.5 Integration with HCL Unica Discover Integration of HCL Digital Experience 9.5 sites with HCL Unica Discover enables DX site managers and marketers to access deep insight analytics and session replay services to assess the effectiveness of DX site pages with end user audiences. See the Integrate HCL Unica Discover with HCL Digital Experience topic for more information. Deploy HCL DX 9.5 using Docker Compose Beginning with HCL DX 9.5 CF196, administrators and developers can deploy HCL DX 9.5 using Docker Compose, for non-production use. Docker Compose scripts for HCL DX 9.5, installation and configuration instructions for non-production use are available in the HCL Software Github . See the Docker image deployment using Docker Compose topic for more information. Deploy HCL Digital Experience 9.5 on HCL Solution Factory (SoFy) The HCL Solution Factory (SoFy) platform offers the ability for organizations to quickly prototype and test assets and can enable organizations to speed cloud-native adoption. Visit HCL SoFy to access HCL Digital Experience 9.5 and other HCL software offerings to quickly assess and test cloud-native strategies, gain hands-on experience working with demonstration assets, to see what best fits adoption plans. View this online tutorial: Deploy HCL Digital Experience in Minutes with HCL SoFy HCL Digital Experience 9.5 Integration with HCL Commerce HCL Commerce integration with HCL Digital Experience allows content and digital assets managed within HCL Digital Experience to be utilized in any Commerce store. Visit the HCL Commerce - Digital Experience integration resource for more information and pre-requisites. New Digital Experience REST APIs New HCL DX APIs are available with the HCL DX CF196 release: Web Content Manager Multilingual Solution APIs Web Content Manager Comments API Web Content Manager Find Rendering and Script Portlet References Web Content Manager Access Control Filter REST API Clear Theme Cache API Work with Deleted Web Content Items New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in videos in following HCL Digital Experience Help Center topics: Configuring user access permissions to Digital Asset Management assets Deep Dive: Progressive Web Applications with HCL DX 9.5 Content Security Policy with HCL Digital 9.5 Getting Started with DXClient on Red Hat OpenShift using HCL Digital Experience CF194 Understanding the Core Persistent Volumes in HCL Digital Experience Container Update CF194 Create a WebDAV Theme copy using HCL Digital Experience 9.5 Create and apply A/B personalized scenarios with HCL Digital Experience Deploy HCL Digital Experience in Minutes using HCL Solution Factory Using the dxctl tool to Update Digital Experience 9.5 on Red Hat OpenShift How to Upload HCL Digital Experience 9.5 CF Container Images to a Private Repository HCL Digital Experience 9.5 Container Deployment Using the dxctl tool to deploy Digital Experience 9.5 on Red Hat OpenShift Create a WAR-based theme copy on HCL Digital Experience 9.5 Parent topic: Latest Combined CF and Container updates","title":"What's new in CF196?"},{"location":"what%27s-new/new_noncf196/#whats-new-in-cf196","text":"Combined Cumulative Fix (CF196) includes new features and software fixes for the latest version of HCL Digital Experience. Beginning with CF19 and Container Update release CF196, release updates for both on\u2013premises platforms and container deployments will be available. This HCL Digital Experience 9.5 CF196 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management and Experience API, Design Studio (Beta) for Container deployments, Theme Editor Portlet, Content Security Policy support, DXClient and DXConnect tooling supporting CICD release processes, Multilingual enhancements, HCL Unica Discover enablement, new HCL Digital Experience \u2018How To\u2019 videos and more. Go to the HCL Software Support Site for the list of software fixes. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. Note: For new capabilities that are available for HCL DX 9.5 CF196 Container Update deployments, see What's new in the CF196 Container Update release topic.","title":"What's new in CF196?"},{"location":"what%27s-new/new_noncf196/#theme-editor-portlet","text":"The Theme Editor portlet is a new addition to HCL Digital Experience CF196 and higher release capabilities. The portlet allows an administrator to edit static theme resources in WebDAV without the use of a WebDAV client or tool. See the Theme Editor Portlet topic for more information.","title":"Theme Editor Portlet"},{"location":"what%27s-new/new_noncf196/#web-content-manager-multilingual-solution-enhancements","text":"The HCL Web Content Manager Multilingual Solution is a set of tools used to manage translated versions Web Content Manager content for localized and regionalized websites. Beginning with the HCL Digital Experience 9.5 CF196 release, a new export and import capability allows you to support translation of the content of a library by exporting it into a format supported by a translation service and importing the translated content back into the content library using the DXClient tool. Support is also added to define a maximum field length, export changed contents from a library, and export to projects. A new option to switch the language in an HCL DX 9.5 Mobile view is also available. See the How to export and import WCM library content using DXClient and The Woodburn Studio demo site Help Center topics for more information.","title":"Web Content Manager Multilingual Solution Enhancements"},{"location":"what%27s-new/new_noncf196/#enable-presentation-of-locales-in-friendly-urls","text":"Beginning with the HCL Digital Experience 9.5 Container Update CF196 release, enablement is provided to present friendly URLs with locale specific definitions when multi-lingual page versions are requested. This enablement can also improve SEO results when users search for language-specific DX page topics. See the Enabling presentation of locales in friendly URLs topic for more information.","title":"Enable Presentation of Locales in Friendly URLs"},{"location":"what%27s-new/new_noncf196/#change-language-presented-in-the-hcl-digital-experience-theme","text":"Beginning with HCL DX CF196 release, you can switch the language presented in your Digital Experience theme. An example is provided in the Woodburn Studio demo site supporting presentations for French, Spanish, and English languages. See the How to switch the languages in the Digital Experience theme topic for more information.","title":"Change language presented in the HCL Digital Experience Theme"},{"location":"what%27s-new/new_noncf196/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include a new DXClient Docker image, and new release artifact types supporting Shared Libraries, Obtain failed Syndication reports, Undeploy Themes, and Export/Import Web Content Manager Library, Deploy DX Core, Manage Syndicator, and Manage Subscriber tasks are provided in CF196. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"what%27s-new/new_noncf196/#web-content-manager-advanced-cache-options","text":"New options are available to flush the Web Content Manager Advanced cache, as a result of syndication operations, to help improve performance and reliability. See the Web Content Manager Cache Parameters topic for more information.","title":"Web Content Manager Advanced Cache Options"},{"location":"what%27s-new/new_noncf196/#content-security-policy","text":"The Content-Security-Policy header is used by modern browsers to enhance the security of HCL Digital Experience site documents or web pages by allowing HCL Digital Experience administrators or developers declare which dynamic resources are allowed to load. With HCL Digital Experience Container Update CF192 and later releases, developers can apply platform support and guidance to update their DX sites to validate trusted sources before rendering pages to end users. See the Content Security Policy Help Center topic for more information.","title":"Content Security Policy"},{"location":"what%27s-new/new_noncf196/#enhanced-cross-origin-resource-sharing-configuration","text":"Enhanced Cross Origin Resource Sharing Configuration adds new options for HCL Digital Experience administrators to set configuration for CORS using a WP configuration service in the IBM WebSphere Application Server resource environment provider. See Enhanced Cross Origin Resource Sharing Configuration for more information.","title":"Enhanced Cross Origin Resource Sharing Configuration"},{"location":"what%27s-new/new_noncf196/#hcl-digital-experience-95-integration-with-hcl-unica-discover","text":"Integration of HCL Digital Experience 9.5 sites with HCL Unica Discover enables DX site managers and marketers to access deep insight analytics and session replay services to assess the effectiveness of DX site pages with end user audiences. See the Integrate HCL Unica Discover with HCL Digital Experience topic for more information.","title":"HCL Digital Experience 9.5 Integration with HCL Unica Discover"},{"location":"what%27s-new/new_noncf196/#deploy-hcl-dx-95-using-docker-compose","text":"Beginning with HCL DX 9.5 CF196, administrators and developers can deploy HCL DX 9.5 using Docker Compose, for non-production use. Docker Compose scripts for HCL DX 9.5, installation and configuration instructions for non-production use are available in the HCL Software Github . See the Docker image deployment using Docker Compose topic for more information.","title":"Deploy HCL DX 9.5 using Docker Compose"},{"location":"what%27s-new/new_noncf196/#deploy-hcl-digital-experience-95-on-hcl-solution-factory-sofy","text":"The HCL Solution Factory (SoFy) platform offers the ability for organizations to quickly prototype and test assets and can enable organizations to speed cloud-native adoption. Visit HCL SoFy to access HCL Digital Experience 9.5 and other HCL software offerings to quickly assess and test cloud-native strategies, gain hands-on experience working with demonstration assets, to see what best fits adoption plans. View this online tutorial: Deploy HCL Digital Experience in Minutes with HCL SoFy","title":"Deploy HCL Digital Experience 9.5 on HCL Solution Factory (SoFy)"},{"location":"what%27s-new/new_noncf196/#hcl-digital-experience-95-integration-with-hcl-commerce","text":"HCL Commerce integration with HCL Digital Experience allows content and digital assets managed within HCL Digital Experience to be utilized in any Commerce store. Visit the HCL Commerce - Digital Experience integration resource for more information and pre-requisites.","title":"HCL Digital Experience 9.5 Integration with HCL Commerce"},{"location":"what%27s-new/new_noncf196/#new-digital-experience-rest-apis","text":"New HCL DX APIs are available with the HCL DX CF196 release: Web Content Manager Multilingual Solution APIs Web Content Manager Comments API Web Content Manager Find Rendering and Script Portlet References Web Content Manager Access Control Filter REST API Clear Theme Cache API Work with Deleted Web Content Items","title":"New Digital Experience REST APIs"},{"location":"what%27s-new/new_noncf196/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in videos in following HCL Digital Experience Help Center topics: Configuring user access permissions to Digital Asset Management assets Deep Dive: Progressive Web Applications with HCL DX 9.5 Content Security Policy with HCL Digital 9.5 Getting Started with DXClient on Red Hat OpenShift using HCL Digital Experience CF194 Understanding the Core Persistent Volumes in HCL Digital Experience Container Update CF194 Create a WebDAV Theme copy using HCL Digital Experience 9.5 Create and apply A/B personalized scenarios with HCL Digital Experience Deploy HCL Digital Experience in Minutes using HCL Solution Factory Using the dxctl tool to Update Digital Experience 9.5 on Red Hat OpenShift How to Upload HCL Digital Experience 9.5 CF Container Images to a Private Repository HCL Digital Experience 9.5 Container Deployment Using the dxctl tool to deploy Digital Experience 9.5 on Red Hat OpenShift Create a WAR-based theme copy on HCL Digital Experience 9.5 Parent topic: Latest Combined CF and Container updates","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"what%27s-new/new_noncf197/","text":"What's new with CF197? Combined Cumulative Fix (CF197) includes new features and software fixes for the latest version of HCL Digital Experience. Release updates for both on \u2013 premises platforms and container deployments are available. This HCL Digital Experience 9.5 Container Update and CF197 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management and Experience API, Design Studio (Beta) for Container deployments, New CICD release process artifacts, new HCL Digital Experience \u2018How To\u2019 videos and more. Go to the HCL Software Support Site for the latest list of software fixes. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include new release artifact types supporting Obtain failed Syndication reports for single or multiple items, and delete Digital Asset Management inactive schema from Persistence are provided in CF197. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in videos in following HCL Digital Experience Help Center topics: Updating the HCL DX 9.5 Portal & IBM WebSphere Application Server Administrator Secrets in OpenShift and Kubernetes Use Docker Compose to install HCL DX 9.5 with Cloud Components for Developer Use Parent topic: Latest Combined CF and Container updates","title":"What's new with CF197?"},{"location":"what%27s-new/new_noncf197/#whats-new-with-cf197","text":"Combined Cumulative Fix (CF197) includes new features and software fixes for the latest version of HCL Digital Experience. Release updates for both on \u2013 premises platforms and container deployments are available. This HCL Digital Experience 9.5 Container Update and CF197 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management and Experience API, Design Studio (Beta) for Container deployments, New CICD release process artifacts, new HCL Digital Experience \u2018How To\u2019 videos and more. Go to the HCL Software Support Site for the latest list of software fixes. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information.","title":"What's new with CF197?"},{"location":"what%27s-new/new_noncf197/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include new release artifact types supporting Obtain failed Syndication reports for single or multiple items, and delete Digital Asset Management inactive schema from Persistence are provided in CF197. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"what%27s-new/new_noncf197/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in videos in following HCL Digital Experience Help Center topics: Updating the HCL DX 9.5 Portal & IBM WebSphere Application Server Administrator Secrets in OpenShift and Kubernetes Use Docker Compose to install HCL DX 9.5 with Cloud Components for Developer Use Parent topic: Latest Combined CF and Container updates","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"what%27s-new/new_noncf198/","text":"What's new with CF198? Combined Cumulative Fix (CF198) includes new features and software fixes for the latest version of HCL Digital Experience. Release updates for both on-premises platforms and container deployments are available. This HCL Digital Experience 9.5 Container Update and CF198 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, the new Site Manager Custom Layout Editor, HCL Digital Experience Technical Articles, \u2018How To\u2019 videos, and more. Go to the HCL Software Support Site for the latest list of software fixes. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include process definitions to automate select IBM WebSphere Application Server settings during deployment, generate import and export lists of virtual portals, and import and export personalization rules. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. New HCL Digital Experience Site Manager Custom Layout Editor Beginning with HCL Digital Experience CF198, a Custom Layout Editor is available for use with HCL DX Site Manager capabilities, reducing custom development requirements for site designers implementing custom layouts in DX site pages. See the Using the Digital Experience Custom Layout Editor topic for more information. New Experience APIs New HCL Experience Web Content Manager REST APIs are available for new menu component update, collection responses conversion utilities, the ability to use the page editor to edit elements inline and update metadata, set locations for sites, set HTML tags for text elements, and more. See the HCL DX Experience API topic for more information Rationalized CF release versioning Beginning with HCL Digital Experience CF196, single versioning is used for both container and on-premise Combined Cumulative Fixes (CFs). This means that fixes for both deployments are included into one CF deliverable. And although the versioning is the same, CFs are packaged separately for on-premises and container deployments. See the CF release versioning and update path topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in articles and videos from the following HCL Digital Experience Help Center topics: Video: Creating an HCL Digital Experience 9.5 CF196 cluster Step-by-step guide: How to deploy HCL DX Container update CF197 and later to Microsoft Azure Kubernetes Service Parent topic: Latest Combined CF and Container updates","title":"What's new with CF198?"},{"location":"what%27s-new/new_noncf198/#whats-new-with-cf198","text":"Combined Cumulative Fix (CF198) includes new features and software fixes for the latest version of HCL Digital Experience. Release updates for both on-premises platforms and container deployments are available. This HCL Digital Experience 9.5 Container Update and CF198 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, the new Site Manager Custom Layout Editor, HCL Digital Experience Technical Articles, \u2018How To\u2019 videos, and more. Go to the HCL Software Support Site for the latest list of software fixes. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information.","title":"What's new with CF198?"},{"location":"what%27s-new/new_noncf198/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include process definitions to automate select IBM WebSphere Application Server settings during deployment, generate import and export lists of virtual portals, and import and export personalization rules. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"what%27s-new/new_noncf198/#new-hcl-digital-experience-site-manager-custom-layout-editor","text":"Beginning with HCL Digital Experience CF198, a Custom Layout Editor is available for use with HCL DX Site Manager capabilities, reducing custom development requirements for site designers implementing custom layouts in DX site pages. See the Using the Digital Experience Custom Layout Editor topic for more information.","title":"New HCL Digital Experience Site Manager Custom Layout Editor"},{"location":"what%27s-new/new_noncf198/#new-experience-apis","text":"New HCL Experience Web Content Manager REST APIs are available for new menu component update, collection responses conversion utilities, the ability to use the page editor to edit elements inline and update metadata, set locations for sites, set HTML tags for text elements, and more. See the HCL DX Experience API topic for more information","title":"New Experience APIs"},{"location":"what%27s-new/new_noncf198/#rationalized-cf-release-versioning","text":"Beginning with HCL Digital Experience CF196, single versioning is used for both container and on-premise Combined Cumulative Fixes (CFs). This means that fixes for both deployments are included into one CF deliverable. And although the versioning is the same, CFs are packaged separately for on-premises and container deployments. See the CF release versioning and update path topic for more information.","title":"Rationalized CF release versioning"},{"location":"what%27s-new/new_noncf198/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in articles and videos from the following HCL Digital Experience Help Center topics: Video: Creating an HCL Digital Experience 9.5 CF196 cluster Step-by-step guide: How to deploy HCL DX Container update CF197 and later to Microsoft Azure Kubernetes Service Parent topic: Latest Combined CF and Container updates","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"what%27s-new/new_noncf199/","text":"What's new with CF199? Combined Cumulative Fix (CF199) includes new features and software fixes for the latest version of HCL Digital Experience. Release updates for both on-premises platforms and container deployments are available. This HCL Digital Experience 9.5 Container Update and CF199 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, new \u201cHow To\u201d videos, and more. Go to the HCL Software Support Site for the latest list of software fixes. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include process definitions to automate Export and Import of select IBM WebSphere Application Server Resource Provider settings during deployment, and to create Syndication relationships and credential vault settings. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. End of Support for HCL Digital Experience Deprecated Features The following list of HCL Digital Experience deprecated features will reach end of support beginning with Container update and CF200 release. Refer to the Deprecated features and themes for HCL Digital Experience 9.5 topic in the Help Center. New Experience API V2 Web Content Manager REST APIs video See the HCL Experience API topic for the video. New Experience APIs New HCL Experience APIs are available for creating, updating and deleting Design Studio (Beta) sites, pages, and containers. New Web Content Manager REST V2 APIs are available for creating content templates, categories, and more. See the HCL Experience API topic for more information. New REST APIs to Configure Remote Search Beginning with HCL Digital Experience CF and Container Update CF199, additional REST services enable administrators and developers to programatically configure remote search in on-premises and container-based Digital Experience deployments. See the Configure Remote Search using REST APIs topic for more information. Parent topic: Latest Combined CF and Container updates","title":"What's new with CF199?"},{"location":"what%27s-new/new_noncf199/#whats-new-with-cf199","text":"Combined Cumulative Fix (CF199) includes new features and software fixes for the latest version of HCL Digital Experience. Release updates for both on-premises platforms and container deployments are available. This HCL Digital Experience 9.5 Container Update and CF199 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, new \u201cHow To\u201d videos, and more. Go to the HCL Software Support Site for the latest list of software fixes. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information.","title":"What's new with CF199?"},{"location":"what%27s-new/new_noncf199/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include process definitions to automate Export and Import of select IBM WebSphere Application Server Resource Provider settings during deployment, and to create Syndication relationships and credential vault settings. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"what%27s-new/new_noncf199/#end-of-support-for-hcl-digital-experience-deprecated-features","text":"The following list of HCL Digital Experience deprecated features will reach end of support beginning with Container update and CF200 release. Refer to the Deprecated features and themes for HCL Digital Experience 9.5 topic in the Help Center.","title":"End of Support for HCL Digital Experience Deprecated Features"},{"location":"what%27s-new/new_noncf199/#new-experience-api-v2-web-content-manager-rest-apis-video","text":"See the HCL Experience API topic for the video.","title":"New Experience API V2 Web Content Manager REST APIs video"},{"location":"what%27s-new/new_noncf199/#new-experience-apis","text":"New HCL Experience APIs are available for creating, updating and deleting Design Studio (Beta) sites, pages, and containers. New Web Content Manager REST V2 APIs are available for creating content templates, categories, and more. See the HCL Experience API topic for more information.","title":"New Experience APIs"},{"location":"what%27s-new/new_noncf199/#new-rest-apis-to-configure-remote-search","text":"Beginning with HCL Digital Experience CF and Container Update CF199, additional REST services enable administrators and developers to programatically configure remote search in on-premises and container-based Digital Experience deployments. See the Configure Remote Search using REST APIs topic for more information. Parent topic: Latest Combined CF and Container updates","title":"New REST APIs to Configure Remote Search"},{"location":"what%27s-new/newcf182/","text":"What's new in CF182? Containers This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. This update adds new guidance to minimize downtime when updating container-based deployments with new artifacts, configuration options for Digital Asset Management storage, and more. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. Deploy HCL DX 9.5 Container updates with minimal operations downtime This topic provides guidance to update artifacts in HCL Digital Experience 9.5 container deployments while minimizing operations downtime, and notes how processes and tools to support these efforts differ across Kubernetes container-based and non-Kubernetes HCL Digital Experience platform deployments. See Deploying HCL DX 9.5 Container artifact updates with minimal operations downtime for details. Optional Digital Asset Management Storage Configuration Settings This topic outlines optional configuration steps to tune Digital Asset Management storage services Storage Class and Volume. See Optional Digital Asset Management Storage Configuration Settings for details. Parent topic: Container Update releases","title":"What's new in CF182? Containers"},{"location":"what%27s-new/newcf182/#whats-new-in-cf182-containers","text":"This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. This update adds new guidance to minimize downtime when updating container-based deployments with new artifacts, configuration options for Digital Asset Management storage, and more. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information.","title":"What's new in CF182? Containers"},{"location":"what%27s-new/newcf182/#deploy-hcl-dx-95-container-updates-with-minimal-operations-downtime","text":"This topic provides guidance to update artifacts in HCL Digital Experience 9.5 container deployments while minimizing operations downtime, and notes how processes and tools to support these efforts differ across Kubernetes container-based and non-Kubernetes HCL Digital Experience platform deployments. See Deploying HCL DX 9.5 Container artifact updates with minimal operations downtime for details.","title":"Deploy HCL DX 9.5 Container updates with minimal operations downtime"},{"location":"what%27s-new/newcf182/#optional-digital-asset-management-storage-configuration-settings","text":"This topic outlines optional configuration steps to tune Digital Asset Management storage services Storage Class and Volume. See Optional Digital Asset Management Storage Configuration Settings for details. Parent topic: Container Update releases","title":"Optional Digital Asset Management Storage Configuration Settings"},{"location":"what%27s-new/newcf183/","text":"What's new in CF183? Containers This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include guidance to deploy to Microsoft Azure Kubernetes Service (AKS), new Web Content Manager REST APIs and Search Query examples, guidance to deploy the Remote Search image on Red Hat OpenShift, new HCL Digital Experience \u2018How To\u2019 videos, and more. Go to the HCL Software Support Site for the list of software fixes, including Container Update release CF183. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Deploy HCL DX 9.5 Container CF182 or higher to Microsoft Azure Kubernetes Service (AKS) Learn how to deploy HCL Digital Experience (DX) 9.5 CF182 and higher container releases along with Ambassador to Kubernetes, as verified in Microsoft Azure Kubernetes Service (AKS). See the HCL Digital Experience 9.5 Deployment and Deploy HCL Digital Experience 9.5 Container to Microsoft Azure Kubernetes Service (AKS) topics for more information. Web Content Manager Lock/Unlock API The Web Content Manager Lock/Unlock API lets you lock and unlock WCM content components, authoring templates, and item. It can also extend the WCM Query API. See the Web Content Manager Lock/Unlock AP I topic for more information. Content Template Create/Update Option Element Selection API The Web Content Manager Create/Update Option Element Selection API lets you create or update an Option Selection Element in a Content Template. See the Create or update an Option Selection Element topic for more information. Search Component Results Display examples A search element defines the layout of a form that is used to display search results. See the Search Component Results Display topic for examples of how to design your search results. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step by step guidance for HCL Digital Experience practitioners presented in several new videos. See the following HCL Digital Experience Help Center topics: Remote Search Service Transfer HCL Digital Experience 9.5 Container default database to IBM DB2 Create a web content library Virtual portals Backup and restore Configuration Wizard Combined Cumulative Fix Install Rich text editor toolbar configuration options Data collection and symptom analysis Parent topic: Container Update releases","title":"What's new in CF183? Containers"},{"location":"what%27s-new/newcf183/#whats-new-in-cf183-containers","text":"This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include guidance to deploy to Microsoft Azure Kubernetes Service (AKS), new Web Content Manager REST APIs and Search Query examples, guidance to deploy the Remote Search image on Red Hat OpenShift, new HCL Digital Experience \u2018How To\u2019 videos, and more. Go to the HCL Software Support Site for the list of software fixes, including Container Update release CF183. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in CF183? Containers"},{"location":"what%27s-new/newcf183/#deploy-hcl-dx-95-container-cf182-or-higher-to-microsoft-azure-kubernetes-service-aks","text":"Learn how to deploy HCL Digital Experience (DX) 9.5 CF182 and higher container releases along with Ambassador to Kubernetes, as verified in Microsoft Azure Kubernetes Service (AKS). See the HCL Digital Experience 9.5 Deployment and Deploy HCL Digital Experience 9.5 Container to Microsoft Azure Kubernetes Service (AKS) topics for more information.","title":"Deploy HCL DX 9.5 Container CF182 or higher to Microsoft Azure Kubernetes Service (AKS)"},{"location":"what%27s-new/newcf183/#web-content-manager-lockunlock-api","text":"The Web Content Manager Lock/Unlock API lets you lock and unlock WCM content components, authoring templates, and item. It can also extend the WCM Query API. See the Web Content Manager Lock/Unlock AP I topic for more information.","title":"Web Content Manager Lock/Unlock API"},{"location":"what%27s-new/newcf183/#content-template-createupdate-option-element-selection-api","text":"The Web Content Manager Create/Update Option Element Selection API lets you create or update an Option Selection Element in a Content Template. See the Create or update an Option Selection Element topic for more information.","title":"Content Template Create/Update Option Element Selection API"},{"location":"what%27s-new/newcf183/#search-component-results-display-examples","text":"A search element defines the layout of a form that is used to display search results. See the Search Component Results Display topic for examples of how to design your search results.","title":"Search Component Results Display examples"},{"location":"what%27s-new/newcf183/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step by step guidance for HCL Digital Experience practitioners presented in several new videos. See the following HCL Digital Experience Help Center topics: Remote Search Service Transfer HCL Digital Experience 9.5 Container default database to IBM DB2 Create a web content library Virtual portals Backup and restore Configuration Wizard Combined Cumulative Fix Install Rich text editor toolbar configuration options Data collection and symptom analysis Parent topic: Container Update releases","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"what%27s-new/newcf184/","text":"What's new in CF184? Containers This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include additional guidance to deploy to Microsoft Azure Kubernetes Service (AKS), new Web Content Manager REST APIs supporting Syndication options, HCL Digital Experience API published to HCL DX GitHub, new HCL Digital Experience \u2018How To\u2019 videos, and more. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Web Content Manager Syndication REST APIs The Web Content Manager Syndication REST APIs let you control syndication processes. See the Web Content Manager Syndication REST APIs topic for more information. Access the HCL Experience API in HCL DX GitHub The HCL Experience API is a set of OpenAPI-compliant REST APIs available for customers deploying HCL Digital Experience 9.5 containers on supported Kubernetes platforms. It supports the integration and management of HCL Digital Experience content and functionality to any digital channel using any front-end development framework. Developers may also now access this API published to the HCL DX GitHub repository. See the Experience API topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step by step guidance for HCL Digital Experience practitioners presented in several new videos. See the following HCL Digital Experience Help Center topics: HCL Digital Experience Combined Cumulative Fix (CF) Installation Deploy HCL Digital Experience 9.5 to Red Hat OpenShift Deploy Digital Asset Management, Content Composer and Experience API to Red Hat OpenShift Parent topic: Container Update releases","title":"What's new in CF184? Containers"},{"location":"what%27s-new/newcf184/#whats-new-in-cf184-containers","text":"This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include additional guidance to deploy to Microsoft Azure Kubernetes Service (AKS), new Web Content Manager REST APIs supporting Syndication options, HCL Digital Experience API published to HCL DX GitHub, new HCL Digital Experience \u2018How To\u2019 videos, and more. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in CF184? Containers"},{"location":"what%27s-new/newcf184/#web-content-manager-syndication-rest-apis","text":"The Web Content Manager Syndication REST APIs let you control syndication processes. See the Web Content Manager Syndication REST APIs topic for more information.","title":"Web Content Manager Syndication REST APIs"},{"location":"what%27s-new/newcf184/#access-the-hcl-experience-api-in-hcl-dx-github","text":"The HCL Experience API is a set of OpenAPI-compliant REST APIs available for customers deploying HCL Digital Experience 9.5 containers on supported Kubernetes platforms. It supports the integration and management of HCL Digital Experience content and functionality to any digital channel using any front-end development framework. Developers may also now access this API published to the HCL DX GitHub repository. See the Experience API topic for more information.","title":"Access the HCL Experience API in HCL DX GitHub"},{"location":"what%27s-new/newcf184/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step by step guidance for HCL Digital Experience practitioners presented in several new videos. See the following HCL Digital Experience Help Center topics: HCL Digital Experience Combined Cumulative Fix (CF) Installation Deploy HCL Digital Experience 9.5 to Red Hat OpenShift Deploy Digital Asset Management, Content Composer and Experience API to Red Hat OpenShift Parent topic: Container Update releases","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"what%27s-new/newcf19/","text":"What's new in CF19? Containers This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include additional guidance to deploy to Google Kubernetes Engine (GKE), Hybrid deployment support, Progressive Web Application delivery, Google Analytics integration, Mobile Preview, DXClient and DXConnect tooling supporting CICD release processes, Kaltura video support, new Web Content Manager REST APIs, new HCL Digital Experience \u2018How To\u2019 videos, and more. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal. Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Deploy HCL Digital Experience 9.5 Container to Google Kubernetes Engine (GKE) Learn how to deploy HCL Digital Experience (DX) 9.5 CF19 and higher container release along with Ambassador to Kubernetes, as verified in Google Kubernetes Engine (GKE) . See the Deploy HCL Digital Experience 9.5 Container to Google Kubernetes Engine (GKE) topic for more information. Hybrid Deployment The HCL Digital Experience 9.5 Hybrid deployment and topics deliver capability to deploy and manage HCL Digital Experience 9.5 core Portal Server and Web Content Manager services on premises, and connect to cloud native components Digital Asset Management, Content Composer, Experience API and related services in a production environment. See the Hybrid Deployment topic for more information. Progressive Web Application support Develop support that adds native mobile application experience and performance to your web site using browser-based functionality. See the Progressive Web Application topic for more information. Google Analytics integration Learn how to set up integration of Digital Experience sites with Google Analytics and view the resulting web analytics tracking to assess the effectiveness of your DX site pages with end user audiences. See the Integrate Google Analytics with HCL Digital Experience topic for more information. Mobile Preview Use the Mobile Preview simulator to view the presentation of Digital Experience site page components on select mobile devices. See the Mobile Preview topic for more information. DXClient and DXConnect tooling supporting CICD release processes HCL Digital Experience 9.5 CF19 and higher includes a DXClient toolset, and DX Connect servlet that provides developers and administrators with an approach to deploy changes or improvements to the HCL Digital Experience platform,and automate processes in the development and delivery process. Note: The DXClient tool is not supported for use with HCL DX 9.5 deployments in Red Hat OpenShift or supported Kubernetes platforms. Use of the DXClient tool with those platforms will be available in future HCL DX 9.5 update releases. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. Digital Asset Management and Kaltura Integration Learn how to configure Kaltura Video Content Management System integration to accelerate HCL Digital Asset Management rich media integration to HCL Digital Experience site pages and content. See the Configure DAM - Kaltura integration topic for more information. New Digital Experience REST APIs New HCL DX APIs are introduced with the HCL DX CF19 release: Web content image renditions interactions REST API Web Content Library Locale Query Workflow Comments API Using XML Access to export and import Digital Asset Management assets New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step by step guidance for HCL Digital Experience practitioners presented in several new videos. See the following HCL Digital Experience Help Center topics: Configure the OpenLDAP container image to the HCL DX 9.5 Container Deployment How to manage syndicators and subscribers Parent topic: Container Update releases","title":"What's new in CF19? Containers"},{"location":"what%27s-new/newcf19/#whats-new-in-cf19-containers","text":"This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include additional guidance to deploy to Google Kubernetes Engine (GKE), Hybrid deployment support, Progressive Web Application delivery, Google Analytics integration, Mobile Preview, DXClient and DXConnect tooling supporting CICD release processes, Kaltura video support, new Web Content Manager REST APIs, new HCL Digital Experience \u2018How To\u2019 videos, and more. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal. Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in CF19? Containers"},{"location":"what%27s-new/newcf19/#deploy-hcl-digital-experience-95-container-to-google-kubernetes-engine-gke","text":"Learn how to deploy HCL Digital Experience (DX) 9.5 CF19 and higher container release along with Ambassador to Kubernetes, as verified in Google Kubernetes Engine (GKE) . See the Deploy HCL Digital Experience 9.5 Container to Google Kubernetes Engine (GKE) topic for more information.","title":"Deploy HCL Digital Experience 9.5 Container to Google Kubernetes Engine (GKE)"},{"location":"what%27s-new/newcf19/#hybrid-deployment","text":"The HCL Digital Experience 9.5 Hybrid deployment and topics deliver capability to deploy and manage HCL Digital Experience 9.5 core Portal Server and Web Content Manager services on premises, and connect to cloud native components Digital Asset Management, Content Composer, Experience API and related services in a production environment. See the Hybrid Deployment topic for more information.","title":"Hybrid Deployment"},{"location":"what%27s-new/newcf19/#progressive-web-application-support","text":"Develop support that adds native mobile application experience and performance to your web site using browser-based functionality. See the Progressive Web Application topic for more information.","title":"Progressive Web Application support"},{"location":"what%27s-new/newcf19/#google-analytics-integration","text":"Learn how to set up integration of Digital Experience sites with Google Analytics and view the resulting web analytics tracking to assess the effectiveness of your DX site pages with end user audiences. See the Integrate Google Analytics with HCL Digital Experience topic for more information.","title":"Google Analytics integration"},{"location":"what%27s-new/newcf19/#mobile-preview","text":"Use the Mobile Preview simulator to view the presentation of Digital Experience site page components on select mobile devices. See the Mobile Preview topic for more information.","title":"Mobile Preview"},{"location":"what%27s-new/newcf19/#dxclient-and-dxconnect-tooling-supporting-cicd-release-processes","text":"HCL Digital Experience 9.5 CF19 and higher includes a DXClient toolset, and DX Connect servlet that provides developers and administrators with an approach to deploy changes or improvements to the HCL Digital Experience platform,and automate processes in the development and delivery process. Note: The DXClient tool is not supported for use with HCL DX 9.5 deployments in Red Hat OpenShift or supported Kubernetes platforms. Use of the DXClient tool with those platforms will be available in future HCL DX 9.5 update releases. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"DXClient and DXConnect tooling supporting CICD release processes"},{"location":"what%27s-new/newcf19/#digital-asset-management-and-kaltura-integration","text":"Learn how to configure Kaltura Video Content Management System integration to accelerate HCL Digital Asset Management rich media integration to HCL Digital Experience site pages and content. See the Configure DAM - Kaltura integration topic for more information.","title":"Digital Asset Management and Kaltura Integration"},{"location":"what%27s-new/newcf19/#new-digital-experience-rest-apis","text":"New HCL DX APIs are introduced with the HCL DX CF19 release: Web content image renditions interactions REST API Web Content Library Locale Query Workflow Comments API Using XML Access to export and import Digital Asset Management assets","title":"New Digital Experience REST APIs"},{"location":"what%27s-new/newcf19/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step by step guidance for HCL Digital Experience practitioners presented in several new videos. See the following HCL Digital Experience Help Center topics: Configure the OpenLDAP container image to the HCL DX 9.5 Container Deployment How to manage syndicators and subscribers Parent topic: Container Update releases","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"what%27s-new/newcf191/","text":"What's new in CF191? Containers This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images, that may be used for new deployments only. Upgrading from a previous DX 9.5 Container Release to CF191 is not supported. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Deploy HCL Digital Experience 9.5 on HCL Solution Factory The HCL Solution Factory platform offers the ability for organizations to quickly prototype and test assets and can enable organizations to speed cloud-native adoption. Visit HCL Solution Factory to access HCL Digital Experience 9.5 and other HCL software offerings to quickly assess and test cloud-native strategies to see what best fits adoption plans. View this online tutorial \u201c Deploy HCL Digital Experience in Minutes with HCL SoFy \u201d HCL Digital Experience 9.5 Integration with HCL Commerce HCL Commerce integration with HCL Digital Experience allows content and digital assets managed within HCL Digital Experience to be utilized in any Commerce store. Visit the HCL Commerce Help Center Digital Experience integration for more information and pre-requisites. Parent topic: Container Update releases","title":"What's new in CF191? Containers"},{"location":"what%27s-new/newcf191/#whats-new-in-cf191-containers","text":"This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images, that may be used for new deployments only. Upgrading from a previous DX 9.5 Container Release to CF191 is not supported. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in CF191? Containers"},{"location":"what%27s-new/newcf191/#deploy-hcl-digital-experience-95-on-hcl-solution-factory","text":"The HCL Solution Factory platform offers the ability for organizations to quickly prototype and test assets and can enable organizations to speed cloud-native adoption. Visit HCL Solution Factory to access HCL Digital Experience 9.5 and other HCL software offerings to quickly assess and test cloud-native strategies to see what best fits adoption plans. View this online tutorial \u201c Deploy HCL Digital Experience in Minutes with HCL SoFy \u201d","title":"Deploy HCL Digital Experience 9.5 on HCL Solution Factory"},{"location":"what%27s-new/newcf191/#hcl-digital-experience-95-integration-with-hcl-commerce","text":"HCL Commerce integration with HCL Digital Experience allows content and digital assets managed within HCL Digital Experience to be utilized in any Commerce store. Visit the HCL Commerce Help Center Digital Experience integration for more information and pre-requisites. Parent topic: Container Update releases","title":"HCL Digital Experience 9.5 Integration with HCL Commerce"},{"location":"what%27s-new/newcf192/","text":"What's new in CF192? Containers This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. HCL Digital Experience 9.5 Docker and Container Initialization Performance Beginning with the HCL Digital Experience 9.5 Container Update CF192 release, DX 9.5 Docker and container initialization performance is improved. See the HCL Digital Experience 9.5 Docker and Container Initialization Performance Help Center topic for more information. HCL Digital Experience 9.5 Container Core Transaction Logging Beginning with the HCL Digital Experience 9.5 Container Update CF192 release, transaction logging for the DX Docker Core image is updated to improve performance. See the Logging and tracing for Containers and new services Help Center topic for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform,and automate processes in the development and delivery process. New release artifact types supporting Script Application Undeploy and Restore, and Deploy Theme. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. New Content Composer Features New Content Composer features are added with HCL Digital Experience Container Update CF192, including a new Version Comparison interface and capabilities to View and Filter Workflow comments, and more. See the HCL Content Composer Help Center topic for additional information. New Digital Asset Management Features New Digital Asset Management Features are added with HCL Digital Experience Container Update CF192, including enhanced crop functionality, Kaltura video player support, thumbnail preview support, asset size filter, Renditions and Versioning support, and more. See the HCL Digital Asset Management Help Center topic for additional information. HCL Digital Experience 9.5 Integration with HCL Unica Discover Integration of HCL Digital Experience 9.5 sites with HCL Unica Discover enables DX site managers and marketers to access deep insight analytics and session replay services to assess the effectiveness of DX site pages with end user audiences. See the Integrate HCL Unica Discover with HCL Digital Experience topic for more information. Content Security Policy The Content-Security-Policy header is used by modern browsers to enhance the security of HCL Digital Experience site documents or web pages by allowing HCL Digital Experience administrators or developers declare which dynamic resources are allowed to load. With HCL Digital Experience Container Update CF192 and later releases, developers can apply platform support and guidance to update their DX sites to validate trusted sources before rendering pages to end users. See the Content Security Policy Help Center topic for more information. New Digital Experience REST APIs New HCL DX APIs are introduced with the HCL DX CF192 Container Update release: Using the WCM Add Comment API Web Content Manager Find Rendering and Script Portlet References Web Content Manager Access Control Filter REST API Clear Theme Cache API Work with Deleted Web Content Items Web Content Manager Multilingual Solution APIs New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: HCL Digital Experience 9.5 Container Deployment Using the dxctl tool to deploy Digital Experience 9.5 on Red Hat OpenShift Create a WAR-based theme copy on HCL Digital Experience 9.5 Parent topic: Container Update releases","title":"What's new in CF192? Containers"},{"location":"what%27s-new/newcf192/#whats-new-in-cf192-containers","text":"This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in CF192? Containers"},{"location":"what%27s-new/newcf192/#hcl-digital-experience-95-docker-and-container-initialization-performance","text":"Beginning with the HCL Digital Experience 9.5 Container Update CF192 release, DX 9.5 Docker and container initialization performance is improved. See the HCL Digital Experience 9.5 Docker and Container Initialization Performance Help Center topic for more information.","title":"HCL Digital Experience 9.5 Docker and Container Initialization Performance"},{"location":"what%27s-new/newcf192/#hcl-digital-experience-95-container-core-transaction-logging","text":"Beginning with the HCL Digital Experience 9.5 Container Update CF192 release, transaction logging for the DX Docker Core image is updated to improve performance. See the Logging and tracing for Containers and new services Help Center topic for more information.","title":"HCL Digital Experience 9.5 Container Core Transaction Logging"},{"location":"what%27s-new/newcf192/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform,and automate processes in the development and delivery process. New release artifact types supporting Script Application Undeploy and Restore, and Deploy Theme. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"what%27s-new/newcf192/#new-content-composer-features","text":"New Content Composer features are added with HCL Digital Experience Container Update CF192, including a new Version Comparison interface and capabilities to View and Filter Workflow comments, and more. See the HCL Content Composer Help Center topic for additional information.","title":"New Content Composer Features"},{"location":"what%27s-new/newcf192/#new-digital-asset-management-features","text":"New Digital Asset Management Features are added with HCL Digital Experience Container Update CF192, including enhanced crop functionality, Kaltura video player support, thumbnail preview support, asset size filter, Renditions and Versioning support, and more. See the HCL Digital Asset Management Help Center topic for additional information.","title":"New Digital Asset Management Features"},{"location":"what%27s-new/newcf192/#hcl-digital-experience-95-integration-with-hcl-unica-discover","text":"Integration of HCL Digital Experience 9.5 sites with HCL Unica Discover enables DX site managers and marketers to access deep insight analytics and session replay services to assess the effectiveness of DX site pages with end user audiences. See the Integrate HCL Unica Discover with HCL Digital Experience topic for more information.","title":"HCL Digital Experience 9.5 Integration with HCL Unica Discover"},{"location":"what%27s-new/newcf192/#content-security-policy","text":"The Content-Security-Policy header is used by modern browsers to enhance the security of HCL Digital Experience site documents or web pages by allowing HCL Digital Experience administrators or developers declare which dynamic resources are allowed to load. With HCL Digital Experience Container Update CF192 and later releases, developers can apply platform support and guidance to update their DX sites to validate trusted sources before rendering pages to end users. See the Content Security Policy Help Center topic for more information.","title":"Content Security Policy"},{"location":"what%27s-new/newcf192/#new-digital-experience-rest-apis","text":"New HCL DX APIs are introduced with the HCL DX CF192 Container Update release: Using the WCM Add Comment API Web Content Manager Find Rendering and Script Portlet References Web Content Manager Access Control Filter REST API Clear Theme Cache API Work with Deleted Web Content Items Web Content Manager Multilingual Solution APIs","title":"New Digital Experience REST APIs"},{"location":"what%27s-new/newcf192/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: HCL Digital Experience 9.5 Container Deployment Using the dxctl tool to deploy Digital Experience 9.5 on Red Hat OpenShift Create a WAR-based theme copy on HCL Digital Experience 9.5 Parent topic: Container Update releases","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"what%27s-new/newcf193/","text":"What's new in CF193? Containers This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New and updated feature references are detailed here. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Enable Presentation of Locales in Friendly URLs Beginning with the HCL Digital Experience 9.5 Container Update CF193 release, enablement is provided to present friendly URLs with locale specific definitions when multi-lingual page versions are requested. This enablement can also improve SEO results when users search for language-specific DX page topics. See the Enabling presentation of locales in friendly URLs topic for more information. Theme Editor Portlet The Theme Editor portlet is a new addition to HCL Digital Experience Container Update CF193 and higher release capabilities. The portlet allows an administrator to edit static theme resources in WebDAV without the use of a WebDAV client or tool. See the Theme Editor Portlet topic for more information. HCL Digital Experience 9.5 Container Custom Context Root URL Beginning with HCL DX 9.5 Container Update CF193 release, you can define the custom context root URLs when deploying your DX 9.5 software to the supported container platforms. See the Customizing the Digital Experience Portal URL when deployed to Container platforms topic for more information. New Digital Asset Management Features New Digital Asset Management Features are added with HCL Digital Experience Container Update CF193, and include the ability to filter Digital Assets by size. See the HCL Digital Asset Management Help Center topic for additional information. Change language presented in the HCL Digital Experience Theme Beginning with HCL DX 9.5 Container Update CF193 release, you can switch the language presented in your Digital Experience theme. An example is provided in the Woodburn Studio demo site supporting presentations for French, Spanish, and English languages. See the How to switch the languages in the Digital Experience theme topic for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. New release artifact types supporting Deploy DX Core, Manage Syndicator, and Manage Subscriber tasks are provided in Container Update CF193. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. New Digital Experience REST APIs New and updated HCL DX APIs are introduced with the HCL DX CF193 Container Update release: Web Content Manager Multilingual Solution APIs HCL Digital Experience 9.5 Container Platform Support Matrix View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix Help Center topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Deploy HCL Digital Experience in Minutes using HCL Solution Factory Using the dxctl tool to Update Digital Experience 9.5 on Red Hat OpenShift How to Upload HCL Digital Experience 9.5 CF Container Images to a Private Repository Parent topic: Container Update releases","title":"What's new in CF193? Containers"},{"location":"what%27s-new/newcf193/#whats-new-in-cf193-containers","text":"This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New and updated feature references are detailed here. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in CF193? Containers"},{"location":"what%27s-new/newcf193/#enable-presentation-of-locales-in-friendly-urls","text":"Beginning with the HCL Digital Experience 9.5 Container Update CF193 release, enablement is provided to present friendly URLs with locale specific definitions when multi-lingual page versions are requested. This enablement can also improve SEO results when users search for language-specific DX page topics. See the Enabling presentation of locales in friendly URLs topic for more information.","title":"Enable Presentation of Locales in Friendly URLs"},{"location":"what%27s-new/newcf193/#theme-editor-portlet","text":"The Theme Editor portlet is a new addition to HCL Digital Experience Container Update CF193 and higher release capabilities. The portlet allows an administrator to edit static theme resources in WebDAV without the use of a WebDAV client or tool. See the Theme Editor Portlet topic for more information.","title":"Theme Editor Portlet"},{"location":"what%27s-new/newcf193/#hcl-digital-experience-95-container-custom-context-root-url","text":"Beginning with HCL DX 9.5 Container Update CF193 release, you can define the custom context root URLs when deploying your DX 9.5 software to the supported container platforms. See the Customizing the Digital Experience Portal URL when deployed to Container platforms topic for more information.","title":"HCL Digital Experience 9.5 Container Custom Context Root URL"},{"location":"what%27s-new/newcf193/#new-digital-asset-management-features","text":"New Digital Asset Management Features are added with HCL Digital Experience Container Update CF193, and include the ability to filter Digital Assets by size. See the HCL Digital Asset Management Help Center topic for additional information.","title":"New Digital Asset Management Features"},{"location":"what%27s-new/newcf193/#change-language-presented-in-the-hcl-digital-experience-theme","text":"Beginning with HCL DX 9.5 Container Update CF193 release, you can switch the language presented in your Digital Experience theme. An example is provided in the Woodburn Studio demo site supporting presentations for French, Spanish, and English languages. See the How to switch the languages in the Digital Experience theme topic for more information.","title":"Change language presented in the HCL Digital Experience Theme"},{"location":"what%27s-new/newcf193/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. New release artifact types supporting Deploy DX Core, Manage Syndicator, and Manage Subscriber tasks are provided in Container Update CF193. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"what%27s-new/newcf193/#new-digital-experience-rest-apis","text":"New and updated HCL DX APIs are introduced with the HCL DX CF193 Container Update release: Web Content Manager Multilingual Solution APIs","title":"New Digital Experience REST APIs"},{"location":"what%27s-new/newcf193/#hcl-digital-experience-95-container-platform-support-matrix","text":"View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix Help Center topic for more information.","title":"HCL Digital Experience 9.5 Container Platform Support Matrix"},{"location":"what%27s-new/newcf193/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Deploy HCL Digital Experience in Minutes using HCL Solution Factory Using the dxctl tool to Update Digital Experience 9.5 on Red Hat OpenShift How to Upload HCL Digital Experience 9.5 CF Container Images to a Private Repository Parent topic: Container Update releases","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"what%27s-new/newcf194/","text":"What's new in CF194? Containers This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. No new features were added in this update. Important note: The default IBM WebSphere Application Server certificate that ships with HCL Digital Experience 9.5 Docker images expires on April 26, 2021. Access to HCL Digital Experience 9.5 container deployments is not adversely affected. However, scripts executed against the DX 9.5 deployed servers, like stopServer or some ConfigEngine tasks, will fail. To address this, HCL Digital Experience 9.5 customers deploying to container platforms can use either of the following options to update the certificate: Apply the HCL Digital Experience 9.5 Container Update CF194, available from the HCL Software Licensing Portal on April 19, 2021. Renew the certificate on your DX 9.5 Container Deployment by following the steps outlined in the following HCL DX Support Knowledge Base article: Manual Steps to Apply the Digital Experience 9.5 Container Deployment Core Certificate Update . Go to the HCL Software Support Site for the list of software fixes, including Container Update release CF194. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Parent topic: Container Update releases","title":"What's new in CF194? Containers"},{"location":"what%27s-new/newcf194/#whats-new-in-cf194-containers","text":"This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. No new features were added in this update. Important note: The default IBM WebSphere Application Server certificate that ships with HCL Digital Experience 9.5 Docker images expires on April 26, 2021. Access to HCL Digital Experience 9.5 container deployments is not adversely affected. However, scripts executed against the DX 9.5 deployed servers, like stopServer or some ConfigEngine tasks, will fail. To address this, HCL Digital Experience 9.5 customers deploying to container platforms can use either of the following options to update the certificate: Apply the HCL Digital Experience 9.5 Container Update CF194, available from the HCL Software Licensing Portal on April 19, 2021. Renew the certificate on your DX 9.5 Container Deployment by following the steps outlined in the following HCL DX Support Knowledge Base article: Manual Steps to Apply the Digital Experience 9.5 Container Deployment Core Certificate Update . Go to the HCL Software Support Site for the list of software fixes, including Container Update release CF194. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Parent topic: Container Update releases","title":"What's new in CF194? Containers"},{"location":"what%27s-new/newcf195/","text":"What's new in CF195? Containers This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New and updated feature references are detailed here. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Web Content Manager Multilingual Solution Library Export and Import The HCL Web Content Manager Multilingual Solution is a set of tools used to manage translated versions Web Content Manager content for localized and regionalized websites. Beginning with the HCL Digital Experience 9.5 Container Update CF195 release, a new export and import capability allows you to support translation of the content of a library by exporting it into a format supported by a translation service and importing the translated content back into the content library using the DXClient tool. See the How to export and import WCM library content using DXClient topic for more information. Web Content Manager Advanced Cache Options New options are available to flush the Web Content Manager Advanced cache, as a result of syndication operations, to help improve performance and reliability. See the Web Content Manager Cache Parameters topic for more information. Enhanced Cross Origin Resource Sharing Configuration Enhanced Cross Origin Resource Sharing Configuration adds new options for HCL Digital Experience administrators to set configuration for CORS using a WP configuration service in the IBM WebSphere Application Server resource environment provider. See the Enhanced Cross Origin Resource Sharing Configuration for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. New release artifact types supporting Undeploy Themes, and Export/Import Web Content Manager Library content are provided in Container Update CF195. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. Remote Search Configuration for HCL Digital Experience 9.5 deployments on Kubernetes platforms Beginning with HCL DX 9.5 Container Update CF195 release, Remote Search can be configured for deployment on supported Kubernetes platforms. See the Configure Remote Search in Red Hat OpenShift and Kubernetes topic for more information. Define No Context Root in for HCL Digital Experience 9.5 container deployments Beginning with HCL DX 9.5 Container Update CF195 release, administrators can define custom context root URLs, or no context root URL, when deploying HCL DX 9.5 software to the supported container platforms. See the Customizing the Digital Experience URL when deployed to Container platforms topic for more information. HCL Digital Experience 9.5 Container Platform Support Matrix View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix Help Center topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Getting Started with DXClient on Red Hat OpenShift using HCL Digital Experience CF194 Understanding the Core Persistent Volumes in HCL Digital Experience Container Update CF194 Create a WebDAV Theme copy using HCL Digital Experience 9.5 Create and apply A/B personalized scenarios with HCL Digital Experience Parent topic: Container Update releases","title":"What's new in CF195? Containers"},{"location":"what%27s-new/newcf195/#whats-new-in-cf195-containers","text":"This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New and updated feature references are detailed here. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in CF195? Containers"},{"location":"what%27s-new/newcf195/#web-content-manager-multilingual-solution-library-export-and-import","text":"The HCL Web Content Manager Multilingual Solution is a set of tools used to manage translated versions Web Content Manager content for localized and regionalized websites. Beginning with the HCL Digital Experience 9.5 Container Update CF195 release, a new export and import capability allows you to support translation of the content of a library by exporting it into a format supported by a translation service and importing the translated content back into the content library using the DXClient tool. See the How to export and import WCM library content using DXClient topic for more information.","title":"Web Content Manager Multilingual Solution Library Export and Import"},{"location":"what%27s-new/newcf195/#web-content-manager-advanced-cache-options","text":"New options are available to flush the Web Content Manager Advanced cache, as a result of syndication operations, to help improve performance and reliability. See the Web Content Manager Cache Parameters topic for more information.","title":"Web Content Manager Advanced Cache Options"},{"location":"what%27s-new/newcf195/#enhanced-cross-origin-resource-sharing-configuration","text":"Enhanced Cross Origin Resource Sharing Configuration adds new options for HCL Digital Experience administrators to set configuration for CORS using a WP configuration service in the IBM WebSphere Application Server resource environment provider. See the Enhanced Cross Origin Resource Sharing Configuration for more information.","title":"Enhanced Cross Origin Resource Sharing Configuration"},{"location":"what%27s-new/newcf195/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. New release artifact types supporting Undeploy Themes, and Export/Import Web Content Manager Library content are provided in Container Update CF195. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"what%27s-new/newcf195/#remote-search-configuration-for-hcl-digital-experience-95-deployments-on-kubernetes-platforms","text":"Beginning with HCL DX 9.5 Container Update CF195 release, Remote Search can be configured for deployment on supported Kubernetes platforms. See the Configure Remote Search in Red Hat OpenShift and Kubernetes topic for more information.","title":"Remote Search Configuration for HCL Digital Experience 9.5 deployments on Kubernetes platforms"},{"location":"what%27s-new/newcf195/#define-no-context-root-in-for-hcl-digital-experience-95-container-deployments","text":"Beginning with HCL DX 9.5 Container Update CF195 release, administrators can define custom context root URLs, or no context root URL, when deploying HCL DX 9.5 software to the supported container platforms. See the Customizing the Digital Experience URL when deployed to Container platforms topic for more information.","title":"Define No Context Root in for HCL Digital Experience 9.5 container deployments"},{"location":"what%27s-new/newcf195/#hcl-digital-experience-95-container-platform-support-matrix","text":"View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix Help Center topic for more information.","title":"HCL Digital Experience 9.5 Container Platform Support Matrix"},{"location":"what%27s-new/newcf195/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Getting Started with DXClient on Red Hat OpenShift using HCL Digital Experience CF194 Understanding the Core Persistent Volumes in HCL Digital Experience Container Update CF194 Create a WebDAV Theme copy using HCL Digital Experience 9.5 Create and apply A/B personalized scenarios with HCL Digital Experience Parent topic: Container Update releases","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"what%27s-new/newcf196/","text":"What's new in CF196? Containers This HCL Digital Experience 9.5 Container Update release includes new releases of HCL Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images, and a beta preview release of Design Studio. New and updated feature references are detailed here. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Note: For new capabilities that are available for HCL DX on-premise deployments, see What's new in the CF196 topic. Design Studio (Beta) Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. Note: Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF196. It is not yet supported for use in production deployments . See the Design Studio (Beta) topic for more information. Deploy HCL DX CF196 to container platforms using Helm Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators a larger degree of transparency and control in deployment operations, and is available for use with the Google Kubernetes Engine (GKE) platform with Container Update CF196. See the HCL DX 9.5 Helm deployment topic for more information. Deploy HCL DX 9.5 using Docker Compose Beginning with HCL DX 9.5 Container Update CF196, administrators and developers can deploy HCL DX 9.5 using Docker Compose, for non-production use. Docker Compose scripts for HCL DX 9.5, installation, and configuration instructions for non-production use are available in the HCL Software Github page. See the Docker image deployment using Docker Compose topic for more information. Web Content Manager Multilingual Solution Enhancements The HCL Web Content Manager Multilingual Solution is a set of tools used to manage translated versions Web Content Manager content for localized and regionalized websites. Beginning with the HCL Digital Experience 9.5 Container Update CF196 release, support is added to import and export multiple libraries to a format supported by a translation service, support a maximum field length, export changed contents from a library, and export to projects. A new option to switch the language in an HCL DX 9.5 mobile view is also available. See the How to export and import WCM library content using DXClient and The Woodburn Studio demo site topics for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. Updates include a new DXClient Docker image, and new release artifact types supporting shared libraries, obtain failed syndication reports are provided in Container Update CF196. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. HCL Digital Experience 9.5 Container Platform Support Matrix View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix Help Center topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Configuring user access permissions to Digital Asset Management assets Deep dive: Progressive Web Applications with HCL DX 9.5 Content Security Policy with HCL DX 9.5 Parent topic: Container Update releases","title":"What's new in CF196? Containers"},{"location":"what%27s-new/newcf196/#whats-new-in-cf196-containers","text":"This HCL Digital Experience 9.5 Container Update release includes new releases of HCL Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images, and a beta preview release of Design Studio. New and updated feature references are detailed here. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Note: For new capabilities that are available for HCL DX on-premise deployments, see What's new in the CF196 topic.","title":"What's new in CF196? Containers"},{"location":"what%27s-new/newcf196/#design-studio-beta","text":"Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. Note: Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF196. It is not yet supported for use in production deployments . See the Design Studio (Beta) topic for more information.","title":"Design Studio (Beta)"},{"location":"what%27s-new/newcf196/#deploy-hcl-dx-cf196-to-container-platforms-using-helm","text":"Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators a larger degree of transparency and control in deployment operations, and is available for use with the Google Kubernetes Engine (GKE) platform with Container Update CF196. See the HCL DX 9.5 Helm deployment topic for more information.","title":"Deploy HCL DX CF196 to container platforms using Helm"},{"location":"what%27s-new/newcf196/#deploy-hcl-dx-95-using-docker-compose","text":"Beginning with HCL DX 9.5 Container Update CF196, administrators and developers can deploy HCL DX 9.5 using Docker Compose, for non-production use. Docker Compose scripts for HCL DX 9.5, installation, and configuration instructions for non-production use are available in the HCL Software Github page. See the Docker image deployment using Docker Compose topic for more information.","title":"Deploy HCL DX 9.5 using Docker Compose"},{"location":"what%27s-new/newcf196/#web-content-manager-multilingual-solution-enhancements","text":"The HCL Web Content Manager Multilingual Solution is a set of tools used to manage translated versions Web Content Manager content for localized and regionalized websites. Beginning with the HCL Digital Experience 9.5 Container Update CF196 release, support is added to import and export multiple libraries to a format supported by a translation service, support a maximum field length, export changed contents from a library, and export to projects. A new option to switch the language in an HCL DX 9.5 mobile view is also available. See the How to export and import WCM library content using DXClient and The Woodburn Studio demo site topics for more information.","title":"Web Content Manager Multilingual Solution Enhancements"},{"location":"what%27s-new/newcf196/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. Updates include a new DXClient Docker image, and new release artifact types supporting shared libraries, obtain failed syndication reports are provided in Container Update CF196. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"what%27s-new/newcf196/#hcl-digital-experience-95-container-platform-support-matrix","text":"View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix Help Center topic for more information.","title":"HCL Digital Experience 9.5 Container Platform Support Matrix"},{"location":"what%27s-new/newcf196/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Configuring user access permissions to Digital Asset Management assets Deep dive: Progressive Web Applications with HCL DX 9.5 Content Security Policy with HCL DX 9.5 Parent topic: Container Update releases","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"what%27s-new/newcf197/","text":"What's new in CF197? Containers This HCL Digital Experience 9.5 Container Update and CF197 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management and Experience API images, and an updated beta preview release of Design Studio. New and updated feature references are detailed here. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Design Studio (Beta) Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New services available with the Container Update CF197 release include ability to render DX site pages and updates using the sample site, Ability to use the page editor to edit elements inline and update metadata, set locations for sites, set html tags for text elements, and more. Note: Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF197, and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) topic for more information. Deploy HCL DX CF197 to container platforms using Helm Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators a larger degree of transparency and control in deployment operations. Support for new HCL DX 9.5 CF197 deployments to Red Hat OpenShift, Microsoft Azure Elastic Kubernetes Service (AKS) and Amazon Elastic Kubernetes Service (EKS),and ability to update from HCL DX 9.5 version CF196 to CF197 is supported with the Google Kubernetes Engine (GKE) platform. See the HCL DX 9.5 Helm deployment topic for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. Updates include new release artifact types supporting, Obtain failed Syndication reports for single or multiple items, and delete Digital Asset Management inactive schema from Persistence are provided in CF197. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. HCL Digital Experience 9.5 Container Platform Support Matrix View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Updating the HCL DX 9.5 Portal & IBM WebSphere Application Server Administrator Secrets in OpenShift and Kubernetes Use Docker Compose to install HCL DX 9.5 with Cloud Components for Developer Use Parent topic: Container Update releases","title":"What's new in CF197? Containers"},{"location":"what%27s-new/newcf197/#whats-new-in-cf197-containers","text":"This HCL Digital Experience 9.5 Container Update and CF197 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management and Experience API images, and an updated beta preview release of Design Studio. New and updated feature references are detailed here. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in CF197? Containers"},{"location":"what%27s-new/newcf197/#design-studio-beta","text":"Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New services available with the Container Update CF197 release include ability to render DX site pages and updates using the sample site, Ability to use the page editor to edit elements inline and update metadata, set locations for sites, set html tags for text elements, and more. Note: Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF197, and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) topic for more information.","title":"Design Studio (Beta)"},{"location":"what%27s-new/newcf197/#deploy-hcl-dx-cf197-to-container-platforms-using-helm","text":"Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators a larger degree of transparency and control in deployment operations. Support for new HCL DX 9.5 CF197 deployments to Red Hat OpenShift, Microsoft Azure Elastic Kubernetes Service (AKS) and Amazon Elastic Kubernetes Service (EKS),and ability to update from HCL DX 9.5 version CF196 to CF197 is supported with the Google Kubernetes Engine (GKE) platform. See the HCL DX 9.5 Helm deployment topic for more information.","title":"Deploy HCL DX CF197 to container platforms using Helm"},{"location":"what%27s-new/newcf197/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. Updates include new release artifact types supporting, Obtain failed Syndication reports for single or multiple items, and delete Digital Asset Management inactive schema from Persistence are provided in CF197. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"what%27s-new/newcf197/#hcl-digital-experience-95-container-platform-support-matrix","text":"View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix topic for more information.","title":"HCL Digital Experience 9.5 Container Platform Support Matrix"},{"location":"what%27s-new/newcf197/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Updating the HCL DX 9.5 Portal & IBM WebSphere Application Server Administrator Secrets in OpenShift and Kubernetes Use Docker Compose to install HCL DX 9.5 with Cloud Components for Developer Use Parent topic: Container Update releases","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"what%27s-new/newcf198/","text":"What's new in CF198? Containers This HCL Digital Experience 9.5 Container Update and CF198 release includes new releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, the new Site Manager Custom Layout Editor, HCL Digital Experience Technical Articles, \u2018How To\u2019 videos, and more. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Design Studio (Beta) Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New features available with the Container Update CF198 release include the ability to create new DX sites, reading and updating site metadata, accessing site and page UUID and URLs, and client-side logging services. Note: Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF198, and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) topic for more information. Deploy HCL DX CF198 to container platforms using Helm Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators more transparency and control in deployment operations. Support for hybrid deployments is provided, enabling to update from HCL DX 9.5 CF197 to CF198 in the Red Hat OpenShift, Microsoft Azure Elastic Kubernetes Service (AKS) and Amazon Elastic Kubernetes Service (EKS) platforms. See the HCL DX 9.5 Helm deployment topic for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. Updates delivered in CF198 include process definitions to automate select IBM WebSphere Application Server settings during deployment, generate import and export lists of virtual portals, and import and export personalization rules. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. New HCL Digital Experience Site Manager Custom Layout Editor Beginning with HCL Digital Experience CF198, a Custom Layout Editor is available for use with HCL DX Site Manager capabilities, reducing custom development requirements for site designers implementing custom layouts in DX site pages. See the Using the Digital Experience Custom Layout Editor topic for more information. New Experience APIs New HCL Experience Web Content Manager REST APIs are available for new menu component update, collection responses conversion utilities, the ability to use the page editor to edit elements inline and update metadata, set locations for sites, set HTML tags for text elements, and more. See the HCL DX Experience API topic for more information Rationalized CF release versioning Beginning with HCL Digital Experience CF196, single versioning is used for both container and on-premise Combined Cumulative Fixes (CFs). This means that fixes for both deployments are included into one CF deliverable. And although the versioning is the same, CFs are packaged separately for on-premises and container deployments. See the CF release versioning and update path topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in articles and videos from the following HCL Digital Experience Help Center topics: Video: Creating an HCL Digital Experience 9.5 CF196 cluster Step-by-step guide: How to deploy HCL DX Container update CF197 and later to Microsoft Azure Kubernetes Service HCL Digital Experience 9.5 Container Platform Support Matrix View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix topic for more information. Parent topic: Container Update releases","title":"What's new in CF198? Containers"},{"location":"what%27s-new/newcf198/#whats-new-in-cf198-containers","text":"This HCL Digital Experience 9.5 Container Update and CF198 release includes new releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, the new Site Manager Custom Layout Editor, HCL Digital Experience Technical Articles, \u2018How To\u2019 videos, and more. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in CF198? Containers"},{"location":"what%27s-new/newcf198/#design-studio-beta","text":"Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New features available with the Container Update CF198 release include the ability to create new DX sites, reading and updating site metadata, accessing site and page UUID and URLs, and client-side logging services. Note: Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF198, and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) topic for more information.","title":"Design Studio (Beta)"},{"location":"what%27s-new/newcf198/#deploy-hcl-dx-cf198-to-container-platforms-using-helm","text":"Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators more transparency and control in deployment operations. Support for hybrid deployments is provided, enabling to update from HCL DX 9.5 CF197 to CF198 in the Red Hat OpenShift, Microsoft Azure Elastic Kubernetes Service (AKS) and Amazon Elastic Kubernetes Service (EKS) platforms. See the HCL DX 9.5 Helm deployment topic for more information.","title":"Deploy HCL DX CF198 to container platforms using Helm"},{"location":"what%27s-new/newcf198/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. Updates delivered in CF198 include process definitions to automate select IBM WebSphere Application Server settings during deployment, generate import and export lists of virtual portals, and import and export personalization rules. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"what%27s-new/newcf198/#new-hcl-digital-experience-site-manager-custom-layout-editor","text":"Beginning with HCL Digital Experience CF198, a Custom Layout Editor is available for use with HCL DX Site Manager capabilities, reducing custom development requirements for site designers implementing custom layouts in DX site pages. See the Using the Digital Experience Custom Layout Editor topic for more information.","title":"New HCL Digital Experience Site Manager Custom Layout Editor"},{"location":"what%27s-new/newcf198/#new-experience-apis","text":"New HCL Experience Web Content Manager REST APIs are available for new menu component update, collection responses conversion utilities, the ability to use the page editor to edit elements inline and update metadata, set locations for sites, set HTML tags for text elements, and more. See the HCL DX Experience API topic for more information","title":"New Experience APIs"},{"location":"what%27s-new/newcf198/#rationalized-cf-release-versioning","text":"Beginning with HCL Digital Experience CF196, single versioning is used for both container and on-premise Combined Cumulative Fixes (CFs). This means that fixes for both deployments are included into one CF deliverable. And although the versioning is the same, CFs are packaged separately for on-premises and container deployments. See the CF release versioning and update path topic for more information.","title":"Rationalized CF release versioning"},{"location":"what%27s-new/newcf198/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in articles and videos from the following HCL Digital Experience Help Center topics: Video: Creating an HCL Digital Experience 9.5 CF196 cluster Step-by-step guide: How to deploy HCL DX Container update CF197 and later to Microsoft Azure Kubernetes Service","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"what%27s-new/newcf198/#hcl-digital-experience-95-container-platform-support-matrix","text":"View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix topic for more information. Parent topic: Container Update releases","title":"HCL Digital Experience 9.5 Container Platform Support Matrix"},{"location":"what%27s-new/newcf199/","text":"What's new in CF199? Containers This HCL Digital Experience 9.5 Container Update and CF199 release includes new releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, \u201cHow To\u201d videos, and more. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Design Studio (Beta) Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New features available with the Container Update CF199 release include the ability to select Web Content Manager library assets when creating sites, UI globalization, support for alternate and no context root when defining sites, and more. Note: Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF196, and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) topic for more information. Migrate from HCL DX 9.5 Operator to Helm Deployments Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators more transparency and control in deployment operations. Beginning with Container Update CF199, support for migration from Operator-based (dxctl) to Helm-based deployments is provided. See the HCL DX 9.5 Helm deployment topic for more information. Digital Asset Management Staging New Digital Asset Management (DAM) staging support enables administrators to stage and synchronize DAM assets from an authoring environment (source environment/publisher) to multiple rendering environments (target environment/subscriber), using DXClient. See the DAM staging topic for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include process definitions to automate Export and Import of select IBM WebSphere Application Server Resource Provider settings during deployment, and to create Syndication relationships and credential vault settings. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. End of Support for HCL Digital Experience Deprecated Features The following list of HCL Digital Experience deprecated features will reach end of support beginning with Container update and CF200 release. Refer to the Deprecated features and themes for HCL Digital Experience 9.5 topic in the Help Center. New Experience APIs New HCL Experience APIs are available for creating, updating and deleting Design Studio (Beta) sites, pages, and containers. New Web Content Manager REST V2 APIs are available for creating content templates, categories, and more. See the HCL Experience API topic for more information. New REST APIs to Configure Remote Search Beginning with HCL Digital Experience CF and Container Update CF199, additional REST services enable administrators and developers to programatically configure remote search in on-premises and container-based Digital Experience deployments. See the Configure Remote Search using REST APIs topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in articles and videos from the following HCL Digital Experience Help Center topics: Video: Deploy HCL DX 9.5 Container Update using Helm Video: Experience API V2 Web Content Manager REST APIs Parent topic: Container Update releases","title":"What's new in CF199? Containers"},{"location":"what%27s-new/newcf199/#whats-new-in-cf199-containers","text":"This HCL Digital Experience 9.5 Container Update and CF199 release includes new releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, \u201cHow To\u201d videos, and more. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in CF199? Containers"},{"location":"what%27s-new/newcf199/#design-studio-beta","text":"Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New features available with the Container Update CF199 release include the ability to select Web Content Manager library assets when creating sites, UI globalization, support for alternate and no context root when defining sites, and more. Note: Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF196, and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) topic for more information.","title":"Design Studio (Beta)"},{"location":"what%27s-new/newcf199/#migrate-from-hcl-dx-95-operator-to-helm-deployments","text":"Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators more transparency and control in deployment operations. Beginning with Container Update CF199, support for migration from Operator-based (dxctl) to Helm-based deployments is provided. See the HCL DX 9.5 Helm deployment topic for more information.","title":"Migrate from HCL DX 9.5 Operator to Helm Deployments"},{"location":"what%27s-new/newcf199/#digital-asset-management-staging","text":"New Digital Asset Management (DAM) staging support enables administrators to stage and synchronize DAM assets from an authoring environment (source environment/publisher) to multiple rendering environments (target environment/subscriber), using DXClient. See the DAM staging topic for more information.","title":"Digital Asset Management Staging"},{"location":"what%27s-new/newcf199/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include process definitions to automate Export and Import of select IBM WebSphere Application Server Resource Provider settings during deployment, and to create Syndication relationships and credential vault settings. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"what%27s-new/newcf199/#end-of-support-for-hcl-digital-experience-deprecated-features","text":"The following list of HCL Digital Experience deprecated features will reach end of support beginning with Container update and CF200 release. Refer to the Deprecated features and themes for HCL Digital Experience 9.5 topic in the Help Center.","title":"End of Support for HCL Digital Experience Deprecated Features"},{"location":"what%27s-new/newcf199/#new-experience-apis","text":"New HCL Experience APIs are available for creating, updating and deleting Design Studio (Beta) sites, pages, and containers. New Web Content Manager REST V2 APIs are available for creating content templates, categories, and more. See the HCL Experience API topic for more information.","title":"New Experience APIs"},{"location":"what%27s-new/newcf199/#new-rest-apis-to-configure-remote-search","text":"Beginning with HCL Digital Experience CF and Container Update CF199, additional REST services enable administrators and developers to programatically configure remote search in on-premises and container-based Digital Experience deployments. See the Configure Remote Search using REST APIs topic for more information.","title":"New REST APIs to Configure Remote Search"},{"location":"what%27s-new/newcf199/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in articles and videos from the following HCL Digital Experience Help Center topics: Video: Deploy HCL DX 9.5 Container Update using Helm Video: Experience API V2 Web Content Manager REST APIs Parent topic: Container Update releases","title":"New HCL Digital Experience \u2018How To\u2019 Videos"}]}